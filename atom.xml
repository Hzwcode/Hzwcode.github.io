<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhiwei&#39;s Blog</title>
  <subtitle>Stay Hungry Stay Foolish | 常葆求知若饥 常存虚怀若愚</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://Hzwcode.github.io/"/>
  <updated>2017-03-24T16:16:57.318Z</updated>
  <id>https://Hzwcode.github.io/</id>
  
  <author>
    <name>zhwhong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>那些深度学习与计算机视觉之路上的大佬们</title>
    <link href="https://Hzwcode.github.io/2017/03/24/AI-DL-CV-org-and-person/"/>
    <id>https://Hzwcode.github.io/2017/03/24/AI-DL-CV-org-and-person/</id>
    <published>2017-03-24T07:48:24.000Z</published>
    <updated>2017-03-24T16:16:57.318Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>本文整理、归纳了自己学习Deep Learning，Computer Vision方向看到的相关研究机构以及各位大佬们的信息。打算从事这个行业或者刚入门的朋友可以多关注、多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。</p>
<p>有句话说得好，“<code>Sharing changes the world!</code>”，知识只有分享才能产生更大的价值，希望能对朋友们有所帮助。</p>
<a id="more"></a>
<h2 id="研究机构"><a href="#研究机构" class="headerlink" title="研究机构"></a>研究机构</h2><ul>
<li><a href="https://research.google.com/index.html" target="_blank" rel="external">Google Research</a></li>
<li><a href="https://plus.google.com/+ResearchatGoogle" target="_blank" rel="external">Research at Google - Google+</a></li>
</ul>
<p><img src="google_research.png" alt="Google Research"></p>
<ul>
<li><a href="https://research.facebook.com/ai" target="_blank" rel="external">Facebook AI Research</a></li>
</ul>
<p><img src="FAIR.png" alt="Facebook AI Research"></p>
<ul>
<li><a href="https://www.microsoft.com/en-us/research/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fdefault.aspx" target="_blank" rel="external">Microsoft Research – Emerging Technology, Computer, and Software Research</a></li>
<li><a href="https://www.microsoft.com/en-us/research/group/visual-computing/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fgroups%2Fvc%2F" target="_blank" rel="external">Visual Computing - Microsoft Research</a></li>
<li><a href="http://www.msra.cn/zh-cn/default.aspx" target="_blank" rel="external">微软亚洲研究院 - 梦想绽放，共创未来</a></li>
</ul>
<p><img src="MSAI.png" alt="MSAI"></p>
<ul>
<li><a href="http://www.cvpapers.com/" target="_blank" rel="external">CVPapers - Computer Vision Resource</a></li>
</ul>
<p><img src="CVpapers.png" alt="CVpapers"></p>
<ul>
<li><a href="http://vision.stanford.edu/research.html" target="_blank" rel="external">Stanford Vision Lab; Prof. Fei-Fei Li</a></li>
</ul>
<p><img src="Stanford_CV.png" alt="Stanford Vision Lab"></p>
<ul>
<li><a href="http://www.cs.cmu.edu/afs/cs/project/cil/ftp/html/vision.html" target="_blank" rel="external">CMU - The Computer Vision Homepage</a></li>
</ul>
<p><img src="CMU_CV.png" alt="CMU Computer Vision"></p>
<ul>
<li><a href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/" target="_blank" rel="external">UCB - UC Berkeley Computer Vision Group</a></li>
</ul>
<p><img src="UCB_CV.png" alt="UC Berkeley Computer Vision Group"></p>
<h2 id="大佬们"><a href="#大佬们" class="headerlink" title="大佬们"></a>大佬们</h2><ul>
<li><a href="http://yann.lecun.com/" target="_blank" rel="external">Yann LeCun - New York University | Facebook AI Research Director</a></li>
<li><a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/" target="_blank" rel="external">Yoshua Bengio - Department of Computer Science and Operations Research</a></li>
<li><a href="http://www.cs.toronto.edu/~kriz/" target="_blank" rel="external">Alex Krizhevsky - Currently working at Google</a></li>
<li><a href="http://www.cs.toronto.edu/~hinton/" target="_blank" rel="external">Geoffrey Hinton - Google Engineering Fellow &amp; University of Toronto Professor</a></li>
<li><a href="http://www.cs.toronto.edu/~ilya/" target="_blank" rel="external">Ilya Sutskever - Co-founder and Research Director of OpenAI</a></li>
<li><a href="http://www.iangoodfellow.com/" target="_blank" rel="external">Ian Goodfellow - Staff Research Scientist at Google Brain</a> | <a href="https://github.com/goodfeli" target="_blank" rel="external">goodfeli-github)</a> | <a href="http://dblp.uni-trier.de/pers/hd/g/Goodfellow:Ian_J=" target="_blank" rel="external">dblp: Ian J. Goodfellow</a></li>
<li><a href="http://www.andrewng.org/" target="_blank" rel="external">Andrew Ng - Baidu VP &amp; Chief Scientist(pre),Coursera Co-Chairman &amp; Co-Founder,Adjunct Professor at Stanford University</a></li>
<li><a href="https://research.google.com/pubs/jeff.html" target="_blank" rel="external">Jeffrey Dean - Google Senior Fellow in the Research Group</a></li>
<li><a href="http://vision.stanford.edu/feifeili/" target="_blank" rel="external">Fei-Fei Li Ph.D. - Associate Professor, Stanford University</a></li>
<li><a href="http://cs.stanford.edu/people/karpathy/" target="_blank" rel="external">Andrej Karpathy - Research Scientist at OpenAI</a> | <a href="http://karpathy.github.io/" target="_blank" rel="external">Blog</a> | <a href="https://github.com/karpathy" target="_blank" rel="external">karpathy-github</a></li>
<li><a href="http://cs.stanford.edu/people/jcjohns/" target="_blank" rel="external">Justin Johnson</a> | <a href="https://github.com/jcjohnson" target="_blank" rel="external">jcjohnson-github</a></li>
<li><a href="http://colah.github.io/about.html" target="_blank" rel="external">Christopher Olah - Research Scientist at Google Brain</a> | <a href="http://colah.github.io/" target="_blank" rel="external">colah’s blog</a> | <a href="http://colah.github.io/cv.pdf" target="_blank" rel="external">Chris Olah CV</a> | <a href="https://github.com/colah/" target="_blank" rel="external">colah-github</a></li>
<li><a href="http://kaiminghe.com/" target="_blank" rel="external">Kaiming He - Facebook AI Research, Lead Researcher at MSRA pre.</a></li>
<li><a href="http://www.rossgirshick.info/" target="_blank" rel="external">Ross Girshick (rbg) - Facebook AI Research</a></li>
<li><a href="http://pdollar.github.io/" target="_blank" rel="external">Piotr Dollár  - Facebook AI Research</a> | <a href="https://github.com/pdollar" target="_blank" rel="external">pdollar-github</a></li>
<li><a href="https://people.eecs.berkeley.edu/~gkioxari/" target="_blank" rel="external">Georgia Gkioxari - UC Berkeley</a> | <a href="https://github.com/gkioxari/" target="_blank" rel="external">gkioxari-github</a></li>
<li><a href="http://people.csail.mit.edu/bzhou/" target="_blank" rel="external">Bolei Zhou - MIT</a></li>
<li><a href="http://www.ee.cuhk.edu.hk/~xgwang/" target="_blank" rel="external">Xiaogang Wang - Associate Professor of Electronic Engineering, the Chinese University of Hong Kong</a></li>
<li><a href="http://deeplearning.csail.mit.edu/" target="_blank" rel="external">CVPR’17 Tutorial on Deep Learning for Objects and Scenes</a></li>
</ul>
<p>(持续更新中……)</p>
<h2 id="More-Reference"><a href="#More-Reference" class="headerlink" title="More Reference"></a>More Reference</h2><ul>
<li><a href="http://blog.csdn.net/adong76/article/details/42491401" target="_blank" rel="external">计算机视觉领域的一些牛人博客，超有实力的研究机构</a></li>
<li><a href="https://github.com/Hzwcode/awesome-deep-learning" target="_blank" rel="external">Awesome Deep Learning -&gt; Researchers 100人</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文整理、归纳了自己学习Deep Learning，Computer Vision方向看到的相关研究机构以及各位大佬们的信息。打算从事这个行业或者刚入门的朋友可以多关注、多了解一些CV的具体应用。搞研究的朋友也可以从中了解到很多牛人的研究动态、招生情况等。&lt;/p&gt;
&lt;p&gt;有句话说得好，“&lt;code&gt;Sharing changes the world!&lt;/code&gt;”，知识只有分享才能产生更大的价值，希望能对朋友们有所帮助。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://Hzwcode.github.io/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="https://Hzwcode.github.io/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>解决Hexo博客文章置顶问题</title>
    <link href="https://Hzwcode.github.io/2017/03/23/deal-with-hexo-article-top-problem/"/>
    <id>https://Hzwcode.github.io/2017/03/23/deal-with-hexo-article-top-problem/</id>
    <published>2017-03-23T09:07:29.000Z</published>
    <updated>2017-03-24T07:26:02.528Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>Hexo默认只提供了按发布日期的排序，只好网上找了些资料修改。</p>
<p>原理：在Hexo生成首页HTML时，将top值高的文章排在前面，达到置顶功能。</p>
<p>修改Hexo文件夹下的<code>node_modules/hexo-generator-index/lib/generator.js</code>，在生成文章之前进行文章top值排序。</p>
<a id="more"></a>
<p>需添加的代码：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">posts.data = posts.data.sort(<span class="function"><span class="keyword">function</span>(<span class="params">a, b</span>) </span>&#123;</div><div class="line">    <span class="keyword">if</span>(a.top &amp;&amp; b.top) &#123; <span class="comment">// 两篇文章top都有定义</span></div><div class="line">        <span class="keyword">if</span>(a.top == b.top) <span class="keyword">return</span> b.date - a.date; <span class="comment">// 若top值一样则按照文章日期降序排</span></div><div class="line">        <span class="keyword">else</span> <span class="keyword">return</span> b.top - a.top; <span class="comment">// 否则按照top值降序排</span></div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(a.top &amp;&amp; !b.top) &#123; <span class="comment">// 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233）</span></div><div class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(!a.top &amp;&amp; b.top) &#123;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">else</span> <span class="keyword">return</span> b.date - a.date; <span class="comment">// 都没定义按照文章日期降序排</span></div><div class="line">&#125;);</div></pre></td></tr></table></figure>
<p>其中涉及Javascript的比较函数：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cmp(<span class="keyword">var</span> a, <span class="keyword">var</span> b) &#123;</div><div class="line">    <span class="keyword">return</span>  a - b; <span class="comment">// 升序，降序的话就 b - a</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>修改完成后，只需要在front-matter中设置需要置顶文章的top值，将会根据top值大小来选择置顶顺序top值越大越靠前。需要注意的是，这个文件不是主题的一部分，也不是Git管理的，备份的时候比较容易忽略。</p>
<p>以下是最终的generator.js内容</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="meta">'use strict'</span>;</div><div class="line"></div><div class="line"><span class="keyword">var</span> pagination = <span class="built_in">require</span>(<span class="string">'hexo-pagination'</span>);</div><div class="line"></div><div class="line"><span class="built_in">module</span>.exports = <span class="function"><span class="keyword">function</span>(<span class="params">locals</span>) </span>&#123;</div><div class="line">  <span class="keyword">var</span> config = <span class="keyword">this</span>.config;</div><div class="line">  <span class="keyword">var</span> posts = locals.posts.sort(config.index_generator.order_by);</div><div class="line"></div><div class="line">  posts.data = posts.data.sort(<span class="function"><span class="keyword">function</span>(<span class="params">a, b</span>) </span>&#123;</div><div class="line">      <span class="keyword">if</span>(a.top &amp;&amp; b.top) &#123;</div><div class="line">          <span class="keyword">if</span>(a.top == b.top) <span class="keyword">return</span> b.date - a.date;</div><div class="line">          <span class="keyword">else</span> <span class="keyword">return</span> b.top - a.top;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(a.top &amp;&amp; !b.top) &#123;</div><div class="line">          <span class="keyword">return</span> <span class="number">-1</span>;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(!a.top &amp;&amp; b.top) &#123;</div><div class="line">          <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">      &#125;</div><div class="line">      <span class="keyword">else</span> <span class="keyword">return</span> b.date - a.date;</div><div class="line">  &#125;);</div><div class="line"></div><div class="line">  <span class="keyword">var</span> paginationDir = config.pagination_dir || <span class="string">'page'</span>;</div><div class="line"></div><div class="line">  <span class="keyword">return</span> pagination(<span class="string">''</span>, posts, &#123;</div><div class="line">    <span class="attr">perPage</span>: config.index_generator.per_page,</div><div class="line">    <span class="attr">layout</span>: [<span class="string">'index'</span>, <span class="string">'archive'</span>],</div><div class="line">    <span class="attr">format</span>: paginationDir + <span class="string">'/%d/'</span>,</div><div class="line">    <span class="attr">data</span>: &#123;</div><div class="line">      <span class="attr">__index</span>: <span class="literal">true</span></div><div class="line">    &#125;</div><div class="line">  &#125;);</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<ul>
<li>Reference：<a href="http://www.netcan666.com/2015/11/22/%E8%A7%A3%E5%86%B3Hexo%E7%BD%AE%E9%A1%B6%E9%97%AE%E9%A2%98/" target="_blank" rel="external">解决Hexo置顶问题</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hexo默认只提供了按发布日期的排序，只好网上找了些资料修改。&lt;/p&gt;
&lt;p&gt;原理：在Hexo生成首页HTML时，将top值高的文章排在前面，达到置顶功能。&lt;/p&gt;
&lt;p&gt;修改Hexo文件夹下的&lt;code&gt;node_modules/hexo-generator-index/lib/generator.js&lt;/code&gt;，在生成文章之前进行文章top值排序。&lt;/p&gt;
    
    </summary>
    
      <category term="Tool" scheme="https://Hzwcode.github.io/categories/Tool/"/>
    
    
      <category term="Hexo" scheme="https://Hzwcode.github.io/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>[译] Introduction to debugging neural networks</title>
    <link href="https://Hzwcode.github.io/2017/03/20/Introduction-to-debugging-neural-networks/"/>
    <id>https://Hzwcode.github.io/2017/03/20/Introduction-to-debugging-neural-networks/</id>
    <published>2017-03-19T16:08:22.000Z</published>
    <updated>2017-03-24T07:14:24.253Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><ul>
<li>本文译自：<a href="http://russellsstewart.com/" target="_blank" rel="external">Russell Stewart’s Blog</a> -&gt; <a href="http://russellsstewart.com/blog/0" target="_blank" rel="external">Introduction to debugging neural networks</a></li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-1b69d26b7f4bb783.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="neural network"></p>
<a id="more"></a>
<blockquote>
<h1 id="题目：调试神经网络简介"><a href="#题目：调试神经网络简介" class="headerlink" title="题目：调试神经网络简介"></a>题目：调试神经网络简介</h1></blockquote>
<p>以下建议主要针对神经网络的初学者，它是基于我的经验对工业界和斯坦福的神经网络新手给出的建议。神经网基本上比大多数程序更难调试，因为大多数神经网络错误不会导致<strong>类型错误</strong>或<strong>运行时间错误</strong>。他们只是导致神经网络<strong>难以收敛</strong>。特别是当你刚接触这个的时候，它可能会让你非常沮丧！但是一个有经验的神经网络训练者将能够系统地克服这些困难，尽管存在着大量似是而非的错误消息：性能错误：你的神经网络没有训练好。对于缺乏经验的人来说，这种信息是令人生畏的。但对有经验的，这是一个非常好的错误信息。这意味着样板代码已经偏离了正确道路，是时候去深挖一下原因了！</p>
<h1 id="如何应对NaN"><a href="#如何应对NaN" class="headerlink" title="如何应对NaN"></a>如何应对NaN</h1><p>到目前为止，我从学生那里得到的最常见的第一个问题是，“<strong>为什么我出现了 NaNs ？</strong>”。有时候，这个问题的答案很复杂。但大多数情况是，NaNs 在前100轮迭代中就出现了，这时候这个答案就非常简单：<strong>你的学习率设置的太高了</strong>。当学习率非常高时，在训练的前100轮迭代中就会出现NaNs。尝试不断的把学习率除以3，直到在前100轮迭代中不再出现NaNs。一旦这样做起作用了，你就会得到一个很好的初始学习率。根据我的经验，最好的有效学习率一般在你得到NaNs的学习率的1-10倍以下。</p>
<p>如果你是在超过100轮迭代之后才出现的NaNs，还有2个其他的常见原因。<strong> 1）</strong> 如果你训练的是RNN，请确保使用的是“梯度剪裁（<a href="https://www.zhihu.com/question/29873016/answer/77647103" target="_blank" rel="external"><strong>clip gradient</strong></a> ）”，这可以把全局的梯度二范数(L2)限制在一定的范围内。RNN倾向于在训练早期产生梯度，其中10%或者更少的batch会出现学习尖峰，这些尖峰上的梯度值非常大。如果没有限制幅度，这些尖峰就可能导致NaNs。 <strong>2）</strong> 如果你自己编写了任何自定义的layer，那么这个问题很可能是由这些自定义的layer中一些除零错误引发的。还有一个众所周知的产生NaNs的layer就是softmax层。 softmax的计算在分子和分母中都含有指数函数exp(x)，当inf除以inf时就可能会产生NaNs。所以要确保你使用的是一个稳定版本的softmax实现。</p>
<h1 id="当神经网络不再学习的时候怎么办"><a href="#当神经网络不再学习的时候怎么办" class="headerlink" title="当神经网络不再学习的时候怎么办"></a>当神经网络不再学习的时候怎么办</h1><p>当你不再碰到NaNs的时候，很可能就会遇到这样一种情况，你的网络顺利地训练了几千轮，但是训练的loss值却在前几百个回合后不再减小。如果你是初次构建代码库的话，基本上不会说需要等待超过2000轮迭代。这不是因为所有网络都能在2000次迭代内开始学习，而是因为你在编码中引入bug的几率很高，与其等待长时间的迭代，不如早早的进入调试模式。现在你应该不断缩小问题的范围，直到你的网络可以在2000次迭代内开始学习。幸运的是，有2个不错的维度来降低复杂度：</p>
<p><strong>1）把训练集的样本量减小到10。</strong> 任何一个可用的网络通常都能在几百次迭代后过拟合十个样本。但是很多编码bug则会阻止这种情况发生。如果你的网络仍然不能过度拟合训练集的10个样本，请再次确认数据和标签是否是正确对应的。尝试将batch size设为1来检查batch计算中的错误。在代码中加入一些log输出以确保是以你期望的方式运行的。一般来说，通过暴力排查总会找到这些错误。一旦网络可以拟合10个样本了，继续尝试拟合100个。如果现在可以正常训练了但不如预期，则可以进入下一步了。</p>
<p><strong>2）解决你感兴趣的问题的最简单版本。</strong> 如果你正在做句子翻译，尝试首先为目标语言构建一个语言模型。当上一步成功了，只给出三个源语言的单词，尝试着去预测翻译的第一个词。如果你打算从图像中检测物体，训练回归网络之前试着去分类图像中有多少个物体。在获得一个确保网络可以解决的好的子问题，以及花费最少的时间来使用代码挂接数据之间存在着平衡点。创造力可以起到帮助作用。</p>
<p>为一个新的想法扩展网络的小技巧就是慢慢地缩小上述两步中所做的简化。这是坐标上升法的一种形式，而且十分有用。一开始，你可以证明这个网络可以记住少量的样本，然后可以证明它在一个简化版的子问题中可以在验证集上具有泛化能力。慢慢提升难度，稳步前进。这并不像第一次Karpathy的风格那么有趣，但至少它是有用的。有些时候你会发现有些问题本身十分困难，难以在2000次迭代内完成学习。这很棒！但是它很少需要以前那种难度级别问题迭代次数的十倍以上。如果真需要这么多次迭代，可以尝试寻找一个中间的复杂度。</p>
<h1 id="调整超参数"><a href="#调整超参数" class="headerlink" title="调整超参数"></a>调整超参数</h1><p>既然你的网络现在开始学习东西了，你可能觉得很好。但你可能发现它不能解决这个问题中最困难的版本。超参数的调整就是其中的关键。也许有人仅仅下载了一个CNN包然后在上面跑自己的数据集，并告诉你超参数的调整并不会带来改变。你要认识到他们在用已有的框架解决已有的问题。如果你在使用新架构解决新问题，则必须调试超参数来获得一个良好的配置。你最好是为你的特定问题阅读一个超参数教程，但为了完整性我会在这里列出一些基本的想法：</p>
<ul>
<li><strong>可视化是关键</strong>。不要害怕花时间在整个训练过程中去写一些好用的可视化工具。如果你的可视化方法还是简单观察终端中的loss值变化，那你该考虑一下升级了。</li>
<li><strong>权值初始化很重要</strong>。一般来说，大一点幅度的初始权值会好一些，但太大了就会导致NaNs。因此初始权值需要和学习率一起调整。</li>
<li><strong>确保权值看起来是“健康的”</strong>。要了解这是什么意思，我推荐用ipython notebook打开现有网络的权值。花一些时间来熟悉在标准数据集（如ImageNet或Penn Tree Bank）上训练的成熟网络中的组件的权值直方图应该是什么样子。</li>
<li><strong>神经网络不是输入尺度不变的</strong>，尤其当它使用SGD训练而不是其他的二阶方法训练时，因为SGD不是一个尺度不变的方法。在确定缩放尺度之前，花点时间来尝试多次缩放输入数据和输出标签。</li>
<li><strong>在训练结束之前减小学习率总能带来提升</strong>。最佳的decay策略是：在k个epoch后，每n个epoch之后将学习率除以1.5，其中k &gt; n。</li>
<li><strong>使用超参数配置文件</strong>。虽然在你开始尝试不同的值之前把超参数放在代码中也是ok的。我通过命令行参数加载的方式使用json文件，就像 <a href="https://github.com/Russell91/tensorbox" target="_blank" rel="external">Russell91/TensorBox</a> 中一样，但是具体的形式并不重要。避免总是要去重构你的代码，因为那将是超参数加载的糟糕问题。重构引入了bugs，花费你的训练周期，这种情况能够被避免直到你有一个你觉得不错的网络。</li>
<li><strong>随机的搜索超参数</strong>，如果可以的话。随机搜索可以产生你想不到的超参数组合， 并且能减少很大工作量一旦你已经训练形成了对于给定超参数会带来什么样的影响的直觉。</li>
</ul>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>调试神经网络可能比调试传统程序更费精力，因为几乎所有错误都被投射到整个网络表现的单一维度。尽管如此，二分查找仍然起作用。通过交替<strong>1）调整问题的难度</strong>，和<strong>2）使用少量的训练样本</strong>，你可以快速解决最初的问题。然后超参数调整和长时间的等待就可以解决你剩下的问题了。</p>
<hr>
<p>(转载请联系作者并注明出处，谢谢！)</p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;本文译自：&lt;a href=&quot;http://russellsstewart.com/&quot;&gt;Russell Stewart’s Blog&lt;/a&gt; -&amp;gt; &lt;a href=&quot;http://russellsstewart.com/blog/0&quot;&gt;Introduction to debugging neural networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/145616-1b69d26b7f4bb783.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;neural network&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://Hzwcode.github.io/tags/Deep-Learning/"/>
    
      <category term="Neural Network" scheme="https://Hzwcode.github.io/tags/Neural-Network/"/>
    
      <category term="Debug" scheme="https://Hzwcode.github.io/tags/Debug/"/>
    
  </entry>
  
  <entry>
    <title>GPU和CPU服务器测试mnist手写数字集</title>
    <link href="https://Hzwcode.github.io/2017/03/13/mnist-gpu-cpu/"/>
    <id>https://Hzwcode.github.io/2017/03/13/mnist-gpu-cpu/</id>
    <published>2017-03-13T10:22:15.000Z</published>
    <updated>2017-03-24T07:19:51.927Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="一、GPU服务器"><a href="#一、GPU服务器" class="headerlink" title="一、GPU服务器"></a>一、GPU服务器</h1><p>服务器 IP ：<code>172.xx.xx.98</code> （4块NVIDIA <strong>TITAN X</strong> GPU，<strong>32</strong> CPU核心）</p>
<a id="more"></a>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">zhwhong@news-ai:~$ nvidia-smi</div><div class="line">Mon Mar 13 14:30:39 2017       </div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |</div><div class="line">|-------------------------------+----------------------+----------------------+</div><div class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</div><div class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</div><div class="line">|===============================+======================+======================|</div><div class="line">|   0  GeForce GTX TIT...  Off  | 0000:01:00.0     Off |                  N/A |</div><div class="line">| 22%   53C    P0    69W / 250W |      0MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   1  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |</div><div class="line">| 22%   57C    P0    72W / 250W |      0MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   2  GeForce GTX TIT...  Off  | 0000:82:00.0     Off |                  N/A |</div><div class="line">| 22%   57C    P0    73W / 250W |      0MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   3  GeForce GTX TIT...  Off  | 0000:83:00.0     Off |                  N/A |</div><div class="line">|  0%   53C    P0    60W / 250W |      0MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line"></div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| Processes:                                                       GPU Memory |</div><div class="line">|  GPU       PID  Type  Process name                               Usage      |</div><div class="line">|=============================================================================|</div><div class="line">|  No running processes found                                                 |</div><div class="line">+-----------------------------------------------------------------------------+</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">zhwhong@news-ai:~$ lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                32</div><div class="line">On-line CPU(s) list:   0-31</div><div class="line">Thread(s) per core:    2</div><div class="line">Core(s) per socket:    8</div><div class="line">Socket(s):             2</div><div class="line">NUMA node(s):          2</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 63</div><div class="line">Stepping:              2</div><div class="line">CPU MHz:               1201.218</div><div class="line">BogoMIPS:              4800.94</div><div class="line">Virtualization:        VT-x</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              256K</div><div class="line">L3 cache:              20480K</div><div class="line">NUMA node0 CPU(s):     0-7,16-23</div><div class="line">NUMA node1 CPU(s):     8-15,24-31</div></pre></td></tr></table></figure>
<p>使用 <code>cat /proc/cpuinfo</code>  命令可以查看每一个cpu核详细信息.</p>
<h1 id="二、CPU服务器"><a href="#二、CPU服务器" class="headerlink" title="二、CPU服务器"></a>二、CPU服务器</h1><p>服务器 IP ：<code>113.xx.xxx.196</code> （纯CPU服务器，<strong>128核</strong>）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">mye@ubuntu:~$ lscpu</div><div class="line">Architecture:          x86_64</div><div class="line">CPU op-mode(s):        32-bit, 64-bit</div><div class="line">Byte Order:            Little Endian</div><div class="line">CPU(s):                128</div><div class="line">On-line CPU(s) list:   0-127</div><div class="line">Thread(s) per core:    1</div><div class="line">Core(s) per socket:    16</div><div class="line">Socket(s):             8</div><div class="line">NUMA node(s):          8</div><div class="line">Vendor ID:             GenuineIntel</div><div class="line">CPU family:            6</div><div class="line">Model:                 63</div><div class="line">Stepping:              4</div><div class="line">CPU MHz:               1200.031</div><div class="line">BogoMIPS:              4396.82</div><div class="line">Virtualization:        VT-x</div><div class="line">L1d cache:             32K</div><div class="line">L1i cache:             32K</div><div class="line">L2 cache:              256K</div><div class="line">L3 cache:              40960K</div><div class="line">NUMA node0 CPU(s):     0-15</div><div class="line">NUMA node1 CPU(s):     16-31</div><div class="line">NUMA node2 CPU(s):     32-47</div><div class="line">NUMA node3 CPU(s):     48-63</div><div class="line">NUMA node4 CPU(s):     64-79</div><div class="line">NUMA node5 CPU(s):     80-95</div><div class="line">NUMA node6 CPU(s):     96-111</div><div class="line">NUMA node7 CPU(s):     112-127</div></pre></td></tr></table></figure>
<p>使用 <code>cat /proc/cpuinfo</code>  命令可以查看每一个cpu核详细信息.</p>
<h1 id="三、mnist测试"><a href="#三、mnist测试" class="headerlink" title="三、mnist测试"></a>三、mnist测试</h1><ul>
<li>测试代码： <a href="https://github.com/Hzwcode/awesome-deep-learning/tree/master/TensorFlow-Tutorials" target="_blank" rel="external"><strong>zhwhong/awesome-deep-learning/TensorFlow-Tutorials</strong></a></li>
</ul>
<h2 id="1-逻辑回归logistic测试"><a href="#1-逻辑回归logistic测试" class="headerlink" title="(1)逻辑回归logistic测试"></a>(1)逻辑回归logistic测试</h2><p>Example: <a href="https://github.com/Hzwcode/awesome-deep-learning/blob/master/TensorFlow-Tutorials/02_logistic_regression.py" target="_blank" rel="external"><strong>02_logistic_regression.py</strong></a></p>
<p>测试结果：</p>
<h3 id="a-batch-size-128"><a href="#a-batch-size-128" class="headerlink" title="a.batch_size : 128"></a>a.batch_size : 128</h3><table>
<thead>
<tr>
<th style="text-align:center">—</th>
<th style="text-align:center">GPU</th>
<th style="text-align:center">CPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">top信息</td>
<td style="text-align:center">%CPU：244.2</td>
<td style="text-align:center">%CPU：472</td>
</tr>
<tr>
<td style="text-align:center">nvidia-smi信息</td>
<td style="text-align:center">20%左右</td>
<td style="text-align:center">无</td>
</tr>
<tr>
<td style="text-align:center">mnist运行结果</td>
<td style="text-align:center">(99, 0.9234, <br> datetime.timedelta(0, 68, 913616)) <br> <strong>统计：68s/100轮</strong></td>
<td style="text-align:center">(99, 0.92330000000000001, <br> datetime.timedelta(0, 101, 424780)) <br> <strong>统计：101s/100轮</strong></td>
</tr>
</tbody>
</table>
<h3 id="b-batch-size-256"><a href="#b-batch-size-256" class="headerlink" title="b.batch_size : 256"></a>b.batch_size : 256</h3><table>
<thead>
<tr>
<th style="text-align:center">—</th>
<th style="text-align:center">GPU</th>
<th style="text-align:center">CPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">top信息</td>
<td style="text-align:center">%CPU：214.1</td>
<td style="text-align:center">%CPU：781.1</td>
</tr>
<tr>
<td style="text-align:center">nvidia-smi信息</td>
<td style="text-align:center">24%左右</td>
<td style="text-align:center">无</td>
</tr>
<tr>
<td style="text-align:center">mnist运行结果</td>
<td style="text-align:center">(99, 0.92290000000000005, datetime.timedelta(0, 45, 724627)) <br> <strong>统计：45s/100轮</strong></td>
<td style="text-align:center">(99, 0.92300000000000004, <br> datetime.timedelta(0, 79, 207202)) <br> <strong>统计：79s/100轮</strong></td>
</tr>
</tbody>
</table>
<h3 id="c-batch-size-512"><a href="#c-batch-size-512" class="headerlink" title="c.batch_size : 512"></a>c.batch_size : 512</h3><table>
<thead>
<tr>
<th style="text-align:center">—</th>
<th style="text-align:center">GPU</th>
<th style="text-align:center">CPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">top信息</td>
<td style="text-align:center">%CPU：203.2</td>
<td style="text-align:center">%CPU：1031</td>
</tr>
<tr>
<td style="text-align:center">nvidia-smi信息</td>
<td style="text-align:center">29%左右</td>
<td style="text-align:center">无</td>
</tr>
<tr>
<td style="text-align:center">mnist运行结果</td>
<td style="text-align:center">(99, 0.92000000000000004, datetime.timedelta(0, 30, 479467)) <br> <strong>统计：30s/100轮</strong></td>
<td style="text-align:center">(99, 0.92010000000000003,  <br> datetime.timedelta(0, 66, 738092)) <br> <strong>统计：66秒/100轮</strong></td>
</tr>
</tbody>
</table>
<p><strong>GPU运行结果：</strong></p>
<p><img src="logistic_gpu_1.png" alt=""></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">zhwhong@news-ai:~/MNIST_test$ nvidia-smi</div><div class="line">Mon Mar 13 15:13:32 2017       </div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |</div><div class="line">|-------------------------------+----------------------+----------------------+</div><div class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</div><div class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</div><div class="line">|===============================+======================+======================|</div><div class="line">|   0  GeForce GTX TIT...  Off  | 0000:01:00.0     Off |                  N/A |</div><div class="line">| 22%   57C    P2    70W / 250W |  11664MiB / 12206MiB |     29%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   1  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |</div><div class="line">| 22%   58C    P2    71W / 250W |  11603MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   2  GeForce GTX TIT...  Off  | 0000:82:00.0     Off |                  N/A |</div><div class="line">| 22%   57C    P2    71W / 250W |  11603MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   3  GeForce GTX TIT...  Off  | 0000:83:00.0     Off |                  N/A |</div><div class="line">| 22%   55C    P2    75W / 250W |  11601MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line"></div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| Processes:                                                       GPU Memory |</div><div class="line">|  GPU       PID  Type  Process name                               Usage      |</div><div class="line">|=============================================================================|</div><div class="line">|    0     28564    C   python                                       11660MiB |</div><div class="line">|    1     28564    C   python                                       11599MiB |</div><div class="line">|    2     28564    C   python                                       11599MiB |</div><div class="line">|    3     28564    C   python                                       11597MiB |</div><div class="line">+-----------------------------------------------------------------------------+</div></pre></td></tr></table></figure>
<p><img src="logistic_gpu_2.png" alt=""></p>
<p><strong>CPU运行结果：</strong></p>
<p><img src="logistic_cpu_1.png" alt=""></p>
<p><img src="logistic_cpu_2.png" alt=""></p>
<h2 id="2-卷积神经网络conv测试"><a href="#2-卷积神经网络conv测试" class="headerlink" title="(2)卷积神经网络conv测试"></a>(2)卷积神经网络conv测试</h2><p>Example : <a href="https://github.com/Hzwcode/awesome-deep-learning/blob/master/TensorFlow-Tutorials/05_convolutional_net.py" target="_blank" rel="external"><strong>05_convolutional_net.py</strong></a></p>
<p>测试结果：</p>
<h3 id="a-batch-size-128-1"><a href="#a-batch-size-128-1" class="headerlink" title="a.batch_size : 128"></a>a.batch_size : 128</h3><table>
<thead>
<tr>
<th style="text-align:center">—</th>
<th style="text-align:left">GPU</th>
<th style="text-align:left">CPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">top信息</td>
<td style="text-align:left">%CPU：141.9</td>
<td style="text-align:left">%CPU：5224.3</td>
</tr>
<tr>
<td style="text-align:center">nvidia-smi信息</td>
<td style="text-align:left">75%左右</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:center">mnist运行结果</td>
<td style="text-align:left">(0, 0.93359375, 4, 230888) <br> (1, 0.984375, 7, 929353) <br> (2, 0.97265625, 11, 635471) <br> (3, 0.98828125, 15, 310449) <br> (4, 0.9921875, 19, 3371) <br> (5, 0.98828125, 22, 720680) <br> (6, 1.0, 26, 384165) <br> (7, 0.99609375, 30, 88245) <br> …… <br> (99, 0.9921875, 370, 693523) <br> <strong>平均：3.7s/轮</strong></td>
<td style="text-align:left">(0, 0.95703125,  54, 907580) <br> (1, 0.98046875, 111, 935452) <br> (2, 0.98828125, 169, 417860) <br> (3, 0.98046875, 227, 60819) <br> (4, 0.9921875, 284, 513000) <br> (5, 0.98828125, 342, 273721) <br> (6, 0.9921875, 399, 981951) <br> (7, 0.984375, 458, 23667) <br> (8, 0.99609375, 516, 282659) <br> …… <br> <strong>平均：57s/轮</strong></td>
</tr>
</tbody>
</table>
<h3 id="b-batch-size-256-1"><a href="#b-batch-size-256-1" class="headerlink" title="b.batch_size : 256"></a>b.batch_size : 256</h3><table>
<thead>
<tr>
<th style="text-align:center">—</th>
<th style="text-align:left">GPU</th>
<th style="text-align:left">CPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">top信息</td>
<td style="text-align:left">%CPU：114.4</td>
<td style="text-align:left">%CPU：5746</td>
</tr>
<tr>
<td style="text-align:center">nvidia-smi信息</td>
<td style="text-align:left">82%左右</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:center">mnist运行结果</td>
<td style="text-align:left">(0, 0.6796875, 3, 563670) <br> (1, 0.9609375, 6, 565172) <br> (2, 0.96875, 9, 520787) <br> (3, 0.98828125, 12, 552352) <br> (4, 0.9921875, 15, 509898) <br> (5, 0.984375, 18, 508712) <br> (6, 0.9921875, 21, 465722) <br> …… <br> (99, 1.0, 301, 239776) <br> <strong>平均：3s/轮</strong></td>
<td style="text-align:left">(0, 0.69921875, 37, 712726) <br> (1, 0.97265625, 75, 387519) <br> (2, 0.984375, 113, 36748) <br> (3, 0.98828125, 150, 694555) <br> (4, 0.98828125, 188, 393595) <br> (5, 0.984375, 225, 962947) <br> (6, 0.98046875, 263, 551988) <br> (7, 0.9921875, 301, 107670) <br> …… <br> <strong>平均：37s/轮</strong></td>
</tr>
</tbody>
</table>
<h3 id="c-batch-size-512-1"><a href="#c-batch-size-512-1" class="headerlink" title="c.batch_size : 512"></a>c.batch_size : 512</h3><table>
<thead>
<tr>
<th style="text-align:center">—</th>
<th style="text-align:left">GPU</th>
<th style="text-align:left">CPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">top信息</td>
<td style="text-align:left">%CPU：98.5</td>
<td style="text-align:left">%CPU：5994</td>
</tr>
<tr>
<td style="text-align:center">nvidia-smi信息</td>
<td style="text-align:left">90%左右</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:center">mnist运行结果</td>
<td style="text-align:left">(0, 0.09375, 3, 358815) <br> (1, 0.52734375, 5, 918648) <br> (2, 0.91796875, 8, 488475) <br> (3, 0.9296875, 11, 35129) <br> (4, 0.98046875, 13, 605235) <br> (5, 0.96875, 16, 148614) <br> (6, 0.984375, 18, 715051) <br> (7, 0.9765625, 21, 281468) <br> (8, 0.9921875, 23, 854374) <br> …… <br> (99, 1.0, 263, 28433) <br> <strong>平均：2.63s/轮</strong></td>
<td style="text-align:left">(0, 0.08203125, 31, 125486) <br> (1, 0.796875, 62, 543181) <br> (2, 0.91015625, 94, 522874) <br> (3, 0.9609375, 126, 946088) <br> (4, 0.96484375, 159, 929706) <br> (5, 0.95703125, 193, 230872) <br> (6, 0.9921875, 226, 695604) <br> (7, 0.98828125, 260, 43828) <br> (8, 0.9921875, 293, 214191) <br> (9, 0.99609375, 326, 797200) <br> …… <br> <strong>平均：32.6s/轮</strong></td>
</tr>
</tbody>
</table>
<p><strong>GPU运行结果：</strong></p>
<p><img src="conv_gpu_1.png" alt=""></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">zhwhong@news-ai:~/MNIST_test$ nvidia-smi</div><div class="line">Mon Mar 13 15:44:49 2017       </div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |</div><div class="line">|-------------------------------+----------------------+----------------------+</div><div class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</div><div class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</div><div class="line">|===============================+======================+======================|</div><div class="line">|   0  GeForce GTX TIT...  Off  | 0000:01:00.0     Off |                  N/A |</div><div class="line">| 27%   70C    P2   192W / 250W |  11713MiB / 12206MiB |     90%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   1  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |</div><div class="line">| 22%   53C    P2    70W / 250W |  11603MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   2  GeForce GTX TIT...  Off  | 0000:82:00.0     Off |                  N/A |</div><div class="line">| 22%   45C    P2    69W / 250W |  11627MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   3  GeForce GTX TIT...  Off  | 0000:83:00.0     Off |                  N/A |</div><div class="line">| 22%   52C    P5    22W / 250W |  11601MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line"></div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| Processes:                                                       GPU Memory |</div><div class="line">|  GPU       PID  Type  Process name                               Usage      |</div><div class="line">|=============================================================================|</div><div class="line">|    0      9587    C   python                                       11709MiB |</div><div class="line">|    1      9587    C   python                                       11599MiB |</div><div class="line">|    2      1552    C   python                                         506MiB |</div><div class="line">|    2      9587    C   python                                       11117MiB |</div><div class="line">|    3      9587    C   python                                       11597MiB |</div><div class="line">+-----------------------------------------------------------------------------+</div></pre></td></tr></table></figure>
<p><img src="conv_gpu_2.png" alt=""></p>
<p><strong>CPU运行结果：</strong></p>
<p><img src="conv_cpu_1.png" alt=""></p>
<p><img src="conv_cpu_2.png" alt=""></p>
<h2 id="3-循环神经网络lstm测试"><a href="#3-循环神经网络lstm测试" class="headerlink" title="(3)循环神经网络lstm测试"></a>(3)循环神经网络lstm测试</h2><p>Example : <a href="https://github.com/Hzwcode/awesome-deep-learning/blob/master/TensorFlow-Tutorials/07_lstm.py" target="_blank" rel="external"><strong>07_lstm.py</strong></a></p>
<p>测试结果：</p>
<h3 id="batch-size-512"><a href="#batch-size-512" class="headerlink" title="batch_size : 512"></a>batch_size : 512</h3><table>
<thead>
<tr>
<th style="text-align:center">—</th>
<th style="text-align:left">GPU</th>
<th style="text-align:left">CPU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">top信息</td>
<td style="text-align:left">%CPU：123.4</td>
<td style="text-align:left">%CPU：818.4</td>
</tr>
<tr>
<td style="text-align:center">nvidia-smi信息</td>
<td style="text-align:left">40%左右</td>
<td style="text-align:left">无</td>
</tr>
<tr>
<td style="text-align:center">mnist运行结果</td>
<td style="text-align:left">(0, 0.26953125, 2, 390310) <br> (1, 0.37890625, 4, 420676) <br> (2, 0.68359375, 6, 385682) <br> (3, 0.7421875, 8, 494356) <br> (4, 0.7890625, 10, 649750) <br> (5, 0.84375, 12, 547186) <br> (6, 0.83203125, 14, 657817) <br> (7, 0.8671875, 16, 743615) <br> (8, 0.87109375, 18, 737803) <br> …… <br> …… <br> (99, 0.96875, 202, 633241) <br> <strong>平均：2.02s/轮</strong></td>
<td style="text-align:left">(0, 0.2265625, 10, 367446) <br> (1, 0.3984375, 20, 716101) <br> (2, 0.61328125, 31, 403893) <br> (3, 0.734375, 42, 7851) <br> (4, 0.75, 52, 698565) <br> (5, 0.78515625, 63, 61517) <br> (6, 0.84765625, 73, 529780) <br> (7, 0.84765625, 84, 130221) <br> (8, 0.8828125, 94, 898270) <br> (9, 0.90234375, 105, 455608) <br> …… <br> (99, 0.98046875, 995, 356187) <br> <strong>平均：9.95s/轮</strong></td>
</tr>
</tbody>
</table>
<p><strong>GPU运行结果：</strong></p>
<p><img src="lstm_gpu_1.png" alt=""></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">zhwhong@news-ai:~/MNIST_test$ nvidia-smi</div><div class="line">Mon Mar 13 16:05:19 2017       </div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |</div><div class="line">|-------------------------------+----------------------+----------------------+</div><div class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</div><div class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</div><div class="line">|===============================+======================+======================|</div><div class="line">|   0  GeForce GTX TIT...  Off  | 0000:01:00.0     Off |                  N/A |</div><div class="line">| 22%   61C    P2    90W / 250W |    185MiB / 12206MiB |     40%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   1  GeForce GTX TIT...  Off  | 0000:02:00.0     Off |                  N/A |</div><div class="line">| 22%   55C    P5    20W / 250W |    109MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   2  GeForce GTX TIT...  Off  | 0000:82:00.0     Off |                  N/A |</div><div class="line">| 22%   55C    P5    56W / 250W |    109MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line">|   3  GeForce GTX TIT...  Off  | 0000:83:00.0     Off |                  N/A |</div><div class="line">| 22%   54C    P5    21W / 250W |    109MiB / 12206MiB |      0%      Default |</div><div class="line">+-------------------------------+----------------------+----------------------+</div><div class="line"></div><div class="line">+-----------------------------------------------------------------------------+</div><div class="line">| Processes:                                                       GPU Memory |</div><div class="line">|  GPU       PID  Type  Process name                               Usage      |</div><div class="line">|=============================================================================|</div><div class="line">|    0     17988    C   python                                         183MiB |</div><div class="line">|    1     17988    C   python                                         107MiB |</div><div class="line">|    2     17988    C   python                                         107MiB |</div><div class="line">|    3     17988    C   python                                         107MiB |</div><div class="line">+-----------------------------------------------------------------------------+</div></pre></td></tr></table></figure>
<p><img src="lstm_gpu_2.png" alt=""></p>
<p><strong>CPU运行结果：</strong></p>
<p><img src="lstm_cpu_1.png" alt=""></p>
<p><img src="lstm_cpu_2.png" alt=""></p>
<hr>
<p>注：关于训练中每个epoch时间统计，可以使用python <code>datetime</code> 模块，使用<code>datetime.datetime.now()</code> 获取系统时间。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、GPU服务器&quot;&gt;&lt;a href=&quot;#一、GPU服务器&quot; class=&quot;headerlink&quot; title=&quot;一、GPU服务器&quot;&gt;&lt;/a&gt;一、GPU服务器&lt;/h1&gt;&lt;p&gt;服务器 IP ：&lt;code&gt;172.xx.xx.98&lt;/code&gt; （4块NVIDIA &lt;strong&gt;TITAN X&lt;/strong&gt; GPU，&lt;strong&gt;32&lt;/strong&gt; CPU核心）&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://Hzwcode.github.io/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="https://Hzwcode.github.io/tags/TensorFlow/"/>
    
      <category term="GPU" scheme="https://Hzwcode.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>GNU MAKE命令</title>
    <link href="https://Hzwcode.github.io/2017/03/11/GNU-make/"/>
    <id>https://Hzwcode.github.io/2017/03/11/GNU-make/</id>
    <published>2017-03-10T17:07:07.000Z</published>
    <updated>2017-03-24T07:19:24.519Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>代码变成可执行文件，叫做<a href="http://www.ruanyifeng.com/blog/2014/11/compiler.html" target="_blank" rel="external">编译</a>（compile）；先编译这个，还是先编译那个（即编译的安排），叫做<a href="https://en.wikipedia.org/wiki/Software_build" target="_blank" rel="external">构建</a>（build）。</p>
<p><a href="https://en.wikipedia.org/wiki/Make_%28software%29" target="_blank" rel="external">Make</a>是最常用的构建工具，诞生于1977年，主要用于C语言的项目。但是实际上 ，任何只要某个文件有变化，就要重新构建的项目，都可以用Make构建。</p>
<p>本文介绍Make命令的用法，从简单的讲起，不需要任何基础，只要会使用命令行，就能看懂。我的参考资料主要是Isaac Schlueter的<a href="https://gist.github.com/isaacs/62a2d1825d04437c6f08" target="_blank" rel="external">《Makefile文件教程》</a>和<a href="https://www.gnu.org/software/make/manual/make.html" target="_blank" rel="external">《GNU Make手册》</a>。</p>
<a id="more"></a>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-176a6bf4d5dca196.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>（题图：摄于博兹贾阿达岛，土耳其，2013年7月）</p>
<h2 id="一、Make的概念"><a href="#一、Make的概念" class="headerlink" title="一、Make的概念"></a>一、Make的概念</h2><p>Make这个词，英语的意思是”制作”。Make命令直接用了这个意思，就是要做出某个文件。比如，要做出文件a.txt，就可以执行下面的命令。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ make a.txt</div></pre></td></tr></table></figure>
<p>但是，如果你真的输入这条命令，它并不会起作用。因为Make命令本身并不知道，如何做出a.txt，需要有人告诉它，如何调用其他命令完成这个目标。</p>
<p>比如，假设文件 a.txt 依赖于 b.txt 和 c.txt ，是后面两个文件连接（cat命令）的产物。那么，make 需要知道下面的规则。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a.txt: b.txt c.txt</div><div class="line">    cat b.txt c.txt &gt; a.txt</div></pre></td></tr></table></figure>
<p>也就是说，make a.txt 这条命令的背后，实际上分成两步：第一步，确认 b.txt 和 c.txt 必须已经存在，第二步使用 cat 命令 将这个两个文件合并，输出为新文件。</p>
<p>像这样的规则，都写在一个叫做Makefile的文件中，Make命令依赖这个文件进行构建。Makefile文件也可以写为makefile， 或者用命令行参数指定为其他文件名。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ make -f rules.txt</div><div class="line"># 或者</div><div class="line">$ make --file=rules.txt</div></pre></td></tr></table></figure>
<p>上面代码指定make命令依据rules.txt文件中的规则，进行构建。</p>
<p>总之，make只是一个根据指定的Shell命令进行构建的工具。它的规则很简单，你规定要构建哪个文件、它依赖哪些源文件，当那些文件有变动时，如何重新构建它。</p>
<h2 id="二、Makefile文件的格式"><a href="#二、Makefile文件的格式" class="headerlink" title="二、Makefile文件的格式"></a>二、Makefile文件的格式</h2><p>构建规则都写在Makefile文件里面，要学会如何Make命令，就必须学会如何编写Makefile文件。</p>
<h3 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h3><p>Makefile文件由一系列规则（rules）构成。每条规则的形式如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&lt;target&gt; : &lt;prerequisites&gt;</div><div class="line">[tab]  &lt;commands&gt;</div></pre></td></tr></table></figure>
<p>上面第一行冒号前面的部分，叫做”目标”（target），冒号后面的部分叫做”前置条件”（prerequisites）；第二行必须由一个tab键起首，后面跟着”命令”（commands）。</p>
<p>“目标”是必需的，不可省略；”前置条件”和”命令”都是可选的，但是两者之中必须至少存在一个。</p>
<p>每条规则就明确两件事：构建目标的前置条件是什么，以及如何构建。下面就详细讲解，每条规则的这三个组成部分。</p>
<h3 id="2-2-目标（target）"><a href="#2-2-目标（target）" class="headerlink" title="2.2 目标（target）"></a>2.2 目标（target）</h3><p>一个目标（target）就构成一条规则。目标通常是文件名，指明Make命令所要构建的对象，比如上文的 a.txt 。目标可以是一个文件名，也可以是多个文件名，之间用空格分隔。</p>
<p>除了文件名，目标还可以是某个操作的名字，这称为”伪目标”（phony target）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">clean:</div><div class="line">      rm *.o</div></pre></td></tr></table></figure>
<p>上面代码的目标是clean，它不是文件名，而是一个操作的名字，属于”伪目标 “，作用是删除对象文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ make  clean</div></pre></td></tr></table></figure>
<p>但是，如果当前目录中，正好有一个文件叫做clean，那么这个命令不会执行。因为Make发现clean文件已经存在，就认为没有必要重新构建了，就不会执行指定的rm命令。</p>
<p>为了避免这种情况，可以明确声明clean是”伪目标”，写法如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">.PHONY: clean</div><div class="line">clean:</div><div class="line">        rm *.o temp</div></pre></td></tr></table></figure>
<p>声明clean是”伪目标”之后，make就不会去检查是否存在一个叫做clean的文件，而是每次运行都执行对应的命令。像.PHONY这样的内置目标名还有不少，可以查看手册。</p>
<p>如果Make命令运行时没有指定目标，默认会执行Makefile文件的第一个目标。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ make</div></pre></td></tr></table></figure>
<p>上面代码执行Makefile文件的第一个目标。</p>
<h3 id="2-3-前置条件（prerequisites）"><a href="#2-3-前置条件（prerequisites）" class="headerlink" title="2.3 前置条件（prerequisites）"></a>2.3 前置条件（prerequisites）</h3><p>前置条件通常是一组文件名，之间用空格分隔。它指定了”目标”是否重新构建的判断标准：只要有一个前置文件不存在，或者有过更新（前置文件的last-modification时间戳比目标的时间戳新），”目标”就需要重新构建。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">result.txt: source.txt</div><div class="line">    cp source.txt result.txt</div></pre></td></tr></table></figure>
<p>上面代码中，构建 result.txt 的前置条件是 source.txt 。如果当前目录中，source.txt 已经存在，那么make result.txt可以正常运行，否则必须再写一条规则，来生成 source.txt 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">source.txt:</div><div class="line">    echo &quot;this is the source&quot; &gt; source.txt</div></pre></td></tr></table></figure>
<p>上面代码中，source.txt后面没有前置条件，就意味着它跟其他文件都无关，只要这个文件还不存在，每次调用make source.txt，它都会生成。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ make result.txt</div><div class="line">$ make result.txt</div></pre></td></tr></table></figure>
<p>上面命令连续执行两次make result.txt。第一次执行会先新建 source.txt，然后再新建 result.txt。第二次执行，Make发现 source.txt 没有变动（时间戳晚于 result.txt），就不会执行任何操作，result.txt 也不会重新生成。</p>
<p>如果需要生成多个文件，往往采用下面的写法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source: file1 file2 file3</div></pre></td></tr></table></figure>
<p>上面代码中，source 是一个伪目标，只有三个前置文件，没有任何对应的命令。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ make source</div></pre></td></tr></table></figure>
<p>执行make source命令后，就会一次性生成 file1，file2，file3 三个文件。这比下面的写法要方便很多。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ make file1</div><div class="line">$ make file2</div><div class="line">$ make file3</div></pre></td></tr></table></figure>
<h3 id="2-4-命令（commands）"><a href="#2-4-命令（commands）" class="headerlink" title="2.4 命令（commands）"></a>2.4 命令（commands）</h3><p>命令（commands）表示如何更新目标文件，由一行或多行的Shell命令组成。它是构建”目标”的具体指令，它的运行结果通常就是生成目标文件。</p>
<p>每行命令之前必须有一个tab键。如果想用其他键，可以用内置变量.RECIPEPREFIX声明。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">.RECIPEPREFIX = &gt;</div><div class="line">all:</div><div class="line">&gt; echo Hello, world</div></pre></td></tr></table></figure>
<p>上面代码用.RECIPEPREFIX指定，大于号（&gt;）替代tab键。所以，每一行命令的起首变成了大于号，而不是tab键。</p>
<p>需要注意的是，每行命令在一个单独的shell中执行。这些Shell之间没有继承关系。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">var-lost:</div><div class="line">    export foo=bar</div><div class="line">    echo &quot;foo=[$$foo]&quot;</div></pre></td></tr></table></figure>
<p>上面代码执行后（make var-lost），取不到foo的值。因为两行命令在两个不同的进程执行。一个解决办法是将两行命令写在一行，中间用分号分隔。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">var-kept:</div><div class="line">    export foo=bar; echo &quot;foo=[$$foo]&quot;</div></pre></td></tr></table></figure>
<p>另一个解决办法是在换行符前加反斜杠转义。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">var-kept:</div><div class="line">    export foo=bar; \</div><div class="line">    echo &quot;foo=[$$foo]&quot;</div></pre></td></tr></table></figure>
<p>最后一个方法是加上.ONESHELL:命令。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">.ONESHELL:</div><div class="line">var-kept:</div><div class="line">    export foo=bar;</div><div class="line">    echo &quot;foo=[$$foo]&quot;</div></pre></td></tr></table></figure>
<h2 id="三、Makefile文件的语法"><a href="#三、Makefile文件的语法" class="headerlink" title="三、Makefile文件的语法"></a>三、Makefile文件的语法</h2><h3 id="3-1-注释"><a href="#3-1-注释" class="headerlink" title="3.1 注释"></a>3.1 注释</h3><p>井号（#）在Makefile中表示注释。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 这是注释</div><div class="line">result.txt: source.txt</div><div class="line">    # 这是注释</div><div class="line">    cp source.txt result.txt # 这也是注释</div></pre></td></tr></table></figure>
<h3 id="3-2-回声（echoing）"><a href="#3-2-回声（echoing）" class="headerlink" title="3.2 回声（echoing）"></a>3.2 回声（echoing）</h3><p>正常情况下，make会打印每条命令，然后再执行，这就叫做回声（echoing）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">test:</div><div class="line">    # 这是测试</div></pre></td></tr></table></figure>
<p>执行上面的规则，会得到下面的结果。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ make test</div><div class="line"># 这是测试</div></pre></td></tr></table></figure>
<p>在命令的前面加上@，就可以关闭回声。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">test:</div><div class="line">    @# 这是测试</div></pre></td></tr></table></figure>
<p>现在再执行make test，就不会有任何输出。</p>
<p>由于在构建过程中，需要了解当前在执行哪条命令，所以通常只在注释和纯显示的echo命令前面加上@。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">test:</div><div class="line">    @# 这是测试</div><div class="line">    @echo TODO</div></pre></td></tr></table></figure>
<h3 id="3-3-通配符"><a href="#3-3-通配符" class="headerlink" title="3.3 通配符"></a>3.3 通配符</h3><p>通配符（wildcard）用来指定一组符合条件的文件名。Makefile 的通配符与 Bash 一致，主要有星号（*）、问号（？）和 […] 。比如， *.o 表示所有后缀名为o的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">clean:</div><div class="line">        rm -f *.o</div></pre></td></tr></table></figure>
<h3 id="3-4-模式匹配"><a href="#3-4-模式匹配" class="headerlink" title="3.4 模式匹配"></a>3.4 模式匹配</h3><p>Make命令允许对文件名，进行类似正则运算的匹配，主要用到的匹配符是%。比如，假定当前目录下有 f1.c 和 f2.c 两个源码文件，需要将它们编译为对应的对象文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">%.o: %.c</div></pre></td></tr></table></figure>
<p>等同于下面的写法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">f1.o: f1.c</div><div class="line">f2.o: f2.c</div></pre></td></tr></table></figure>
<p>使用匹配符%，可以将大量同类型的文件，只用一条规则就完成构建。</p>
<h3 id="3-5-变量和赋值符"><a href="#3-5-变量和赋值符" class="headerlink" title="3.5 变量和赋值符"></a>3.5 变量和赋值符</h3><p>Makefile 允许使用等号自定义变量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">txt = Hello World</div><div class="line">test:</div><div class="line">    @echo $(txt)</div></pre></td></tr></table></figure>
<p>上面代码中，变量 txt 等于 Hello World。调用时，变量需要放在 $( ) 之中。</p>
<p>调用Shell变量，需要在美元符号前，再加一个美元符号，这是因为Make命令会对美元符号转义。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">test:</div><div class="line">    @echo $$HOME</div></pre></td></tr></table></figure>
<p>有时，变量的值可能指向另一个变量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">v1 = $(v2)</div></pre></td></tr></table></figure>
<p>上面代码中，变量 v1 的值是另一个变量 v2。这时会产生一个问题，v1 的值到底在定义时扩展（静态扩展），还是在运行时扩展（动态扩展）？如果 v2 的值是动态的，这两种扩展方式的结果可能会差异很大。</p>
<p>为了解决类似问题，Makefile一共提供了四个赋值运算符 （=、:=、？=、+=），它们的区别请看StackOverflow。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">VARIABLE = value</div><div class="line"># 在执行时扩展，允许递归扩展。</div><div class="line">VARIABLE := value</div><div class="line"># 在定义时扩展。</div><div class="line">VARIABLE ?= value</div><div class="line"># 只有在该变量为空时才设置值。</div><div class="line">VARIABLE += value</div><div class="line"># 将值追加到变量的尾端。</div></pre></td></tr></table></figure>
<h3 id="3-6-内置变量（Implicit-Variables）"><a href="#3-6-内置变量（Implicit-Variables）" class="headerlink" title="3.6 内置变量（Implicit Variables）"></a>3.6 内置变量（Implicit Variables）</h3><p>Make命令提供一系列内置变量，比如，$(CC) 指向当前使用的编译器，$(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见手册。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">output:</div><div class="line">    $(CC) -o output input.c</div></pre></td></tr></table></figure>
<h3 id="3-7-自动变量（Automatic-Variables）"><a href="#3-7-自动变量（Automatic-Variables）" class="headerlink" title="3.7 自动变量（Automatic Variables）"></a>3.7 自动变量（Automatic Variables）</h3><p>Make命令还提供一些自动变量，它们的值与当前规则有关。主要有以下几个。</p>
<p><em>（1）$@</em></p>
<pre><code>$@指代当前目标，就是Make命令当前构建的那个目标。比如，make foo的 $@ 就指代foo。
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a.txt b.txt:</div><div class="line">    touch $@</div></pre></td></tr></table></figure>
<p>等同于下面的写法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">a.txt:</div><div class="line">    touch a.txt</div><div class="line">b.txt:</div><div class="line">    touch b.txt</div></pre></td></tr></table></figure>
<p><em>（2）$&lt;</em></p>
<p>$&lt; 指代第一个前置条件。比如，规则为 t: p1 p2，那么$&lt; 就指代p1。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a.txt: b.txt c.txt</div><div class="line">    cp $&lt; $@</div></pre></td></tr></table></figure>
<p>等同于下面的写法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a.txt: b.txt c.txt</div><div class="line">    cp b.txt a.txt</div></pre></td></tr></table></figure>
<p><em>（3）$?</em></p>
<pre><code>$? 指代比目标更新的所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，其中 p2 的时间戳比 t 新，$?就指代p2。
</code></pre><p><em>（4）$^</em></p>
<pre><code>$^ 指代所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，那么 $^ 就指代 p1 p2 。
</code></pre><p><em>（5）$*</em></p>
<pre><code>$* 指代匹配符 % 匹配的部分， 比如% 匹配 f1.txt 中的f1 ，$* 就表示 f1。
</code></pre><p><em>（6）$(@D) 和 $(@F)</em></p>
<pre><code>$(@D) 和 $(@F) 分别指向 $@ 的目录名和文件名。比如，$@是 src/input.c，那么$(@D) 的值为 src ，$(@F) 的值为 input.c。
</code></pre><p><em>（7）$(&lt;D)和 $(&lt;F)</em></p>
<pre><code>$(&lt;D) 和 $(&lt;F) 分别指向 $&lt; 的目录名和文件名。
</code></pre><p>所有的自动变量清单，请看手册。下面是自动变量的一个例子。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">dest/%.txt: src/%.txt</div><div class="line">    @[ -d dest ] || mkdir dest</div><div class="line">    cp $&lt; $@</div></pre></td></tr></table></figure>
<p>上面代码将 src 目录下的 txt 文件，拷贝到 dest 目录下。首先判断 dest 目录是否存在，如果不存在就新建，然后，$&lt; 指代前置文件（src/%.txt）， $@ 指代目标文件（dest/%.txt）。</p>
<h3 id="3-8-判断和循环"><a href="#3-8-判断和循环" class="headerlink" title="3.8 判断和循环"></a>3.8 判断和循环</h3><p>Makefile使用 Bash 语法，完成判断和循环。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ifeq ($(CC),gcc)</div><div class="line">  libs=$(libs_for_gcc)</div><div class="line">else</div><div class="line">  libs=$(normal_libs)</div><div class="line">endif</div></pre></td></tr></table></figure>
<p>上面代码判断当前编译器是否 gcc ，然后指定不同的库文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">LIST = one two three</div><div class="line">all:</div><div class="line">    for i in $(LIST); do \</div><div class="line">        echo $$i; \</div><div class="line">    done</div><div class="line"># 等同于</div><div class="line">all:</div><div class="line">    for i in one two three; do \</div><div class="line">        echo $i; \</div><div class="line">    done</div></pre></td></tr></table></figure>
<p>上面代码的运行结果。</p>
<blockquote>
<p>one<br>two<br>three</p>
</blockquote>
<h3 id="3-9-函数"><a href="#3-9-函数" class="headerlink" title="3.9 函数"></a>3.9 函数</h3><p>Makefile 还可以使用函数，格式如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$(function arguments)</div><div class="line"># 或者</div><div class="line">$&#123;function arguments&#125;</div></pre></td></tr></table></figure>
<p>Makefile提供了许多<a href="http://www.gnu.org/software/make/manual/html_node/Functions.html" target="_blank" rel="external">内置函数</a>，可供调用。下面是几个常用的内置函数。</p>
<p>（1）shell 函数</p>
<p>shell 函数用来执行 shell 命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">srcfiles := $(shell echo src/&#123;00..99&#125;.txt)</div></pre></td></tr></table></figure>
<p>（2）wildcard 函数</p>
<p>wildcard 函数用来在 Makefile 中，替换 Bash 的通配符。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">srcfiles := $(wildcard src/*.txt)</div></pre></td></tr></table></figure>
<p>（3）替换函数</p>
<p>替换函数的写法是：变量名 + 冒号 + 替换规则。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">min: $(OUTPUT:.js=.min.js)</div></pre></td></tr></table></figure>
<p>上面代码的意思是，将变量OUTPUT中的 .js 全部替换成 .min.js 。</p>
<h2 id="四、Makefile-的实例"><a href="#四、Makefile-的实例" class="headerlink" title="四、Makefile 的实例"></a>四、Makefile 的实例</h2><h3 id="（1）执行多个目标"><a href="#（1）执行多个目标" class="headerlink" title="（1）执行多个目标"></a>（1）执行多个目标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">.PHONY: cleanall cleanobj cleandiff</div><div class="line">cleanall : cleanobj cleandiff</div><div class="line">        rm program</div><div class="line">cleanobj :</div><div class="line">        rm *.o</div><div class="line">cleandiff :</div><div class="line">        rm *.diff</div></pre></td></tr></table></figure>
<p>上面代码可以调用不同目标，删除不同后缀名的文件，也可以调用一个目标（cleanall），删除所有指定类型的文件。</p>
<h3 id="（2）编译C语言项目"><a href="#（2）编译C语言项目" class="headerlink" title="（2）编译C语言项目"></a>（2）编译C语言项目</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">edit : main.o kbd.o command.o display.o</div><div class="line">    cc -o edit main.o kbd.o command.o display.o</div><div class="line">main.o : main.c defs.h</div><div class="line">    cc -c main.c</div><div class="line">kbd.o : kbd.c defs.h command.h</div><div class="line">    cc -c kbd.c</div><div class="line">command.o : command.c defs.h command.h</div><div class="line">    cc -c command.c</div><div class="line">display.o : display.c defs.h</div><div class="line">    cc -c display.c</div><div class="line">clean :</div><div class="line">     rm edit main.o kbd.o command.o display.o</div><div class="line">.PHONY: edit clean</div></pre></td></tr></table></figure>
<p>今天，Make命令的介绍就到这里。</p>
<hr>
<ul>
<li>参考：<a href="http://www.ruanyifeng.com/blog/2015/02/make.html" target="_blank" rel="external">阮一峰] - MAKE命令教程</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;代码变成可执行文件，叫做&lt;a href=&quot;http://www.ruanyifeng.com/blog/2014/11/compiler.html&quot;&gt;编译&lt;/a&gt;（compile）；先编译这个，还是先编译那个（即编译的安排），叫做&lt;a href=&quot;https://en.wikipedia.org/wiki/Software_build&quot;&gt;构建&lt;/a&gt;（build）。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Make_%28software%29&quot;&gt;Make&lt;/a&gt;是最常用的构建工具，诞生于1977年，主要用于C语言的项目。但是实际上 ，任何只要某个文件有变化，就要重新构建的项目，都可以用Make构建。&lt;/p&gt;
&lt;p&gt;本文介绍Make命令的用法，从简单的讲起，不需要任何基础，只要会使用命令行，就能看懂。我的参考资料主要是Isaac Schlueter的&lt;a href=&quot;https://gist.github.com/isaacs/62a2d1825d04437c6f08&quot;&gt;《Makefile文件教程》&lt;/a&gt;和&lt;a href=&quot;https://www.gnu.org/software/make/manual/make.html&quot;&gt;《GNU Make手册》&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://Hzwcode.github.io/categories/Linux/"/>
    
    
      <category term="GNU" scheme="https://Hzwcode.github.io/tags/GNU/"/>
    
      <category term="Makefile" scheme="https://Hzwcode.github.io/tags/Makefile/"/>
    
  </entry>
  
  <entry>
    <title>Blog Music Test</title>
    <link href="https://Hzwcode.github.io/2017/02/26/music-test/"/>
    <id>https://Hzwcode.github.io/2017/02/26/music-test/</id>
    <published>2017-02-26T08:56:23.000Z</published>
    <updated>2017-03-24T07:16:55.105Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><center><br><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=38592976&auto=0&height=66"></iframe><br></center>

<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&lt;center&gt;</div><div class="line">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=38592976&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt;</div><div class="line">&lt;/center&gt;</div></pre></td></tr></table></figure>
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="450" src="http://music.163.com/outchain/player?type=0&id=3778678&auto=0&height=430"></iframe>


<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=450 src=&quot;http://music.163.com/outchain/player?type=0&amp;id=3778678&amp;auto=0&amp;height=430&quot;&gt;&lt;/iframe&gt;</div></pre></td></tr></table></figure>
<div id="aplayer0" class="aplayer" style="margin-bottom: 20px;"></div>
		<script>
			new APlayer({
				element: document.getElementById("aplayer0"),
				narrow: false,
				autoplay: false,
				showlrc: 0,
				music: {
					title: "童话镇",
					author: "陈一发儿",
					url: "http://mp3.haoduoge.com/s/2016-12-24/1482568978.mp3",
					pic: "http://p3.music.126.net/tfa811GLreJI_S0h9epqRA==/3394192426154346.jpg?param=130y130",
				}
			});
		</script>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% aplayer &quot;童话镇&quot; &quot;陈一发儿&quot; &quot;http://mp3.haoduoge.com/s/2016-12-24/1482568978.mp3&quot; &quot;http://p3.music.126.net/tfa811GLreJI_S0h9epqRA==/3394192426154346.jpg?param=130y130&quot; %&#125;</div></pre></td></tr></table></figure>
<div id="aplayer1" class="aplayer" style="margin-bottom: 20px;"><pre class="aplayer-lrc-content">[ti:告白气球]
[ar:周杰伦]
[al:周杰伦的床边故事]
[by:D.J.]
歌词千寻 - http://www.lrcgc.com
[00:00.00]周杰伦 - 告白气球
[00:08.00]词：方文山
[00:16.00]曲：周杰伦
[00:22.90]塞纳河畔 左岸的咖啡
[00:25.40]我手一杯 品尝你的美
[00:28.43]留下唇印的嘴
[00:32.95]花店玫瑰 名字写错谁
[00:36.59]告白气球 风吹到对街
[00:39.35]微笑在天上飞
[00:44.11]你说你有点难追
[00:46.35]想让我知难而退
[00:48.62]礼物不需挑最贵
[00:51.61]只要香榭的落叶
[00:54.10]喔～营造浪漫的约会
[00:57.12]不害怕搞砸一切
[00:59.59]拥有你就拥有 全世界
[01:04.85]亲爱的 爱上你 从那天起
[01:11.10]甜蜜的很轻易
[01:15.60]亲爱的 别任性 你的眼睛
[01:21.60]在说我愿意
[01:25.86]
[01:48.35]塞纳河畔 左岸的咖啡
[01:50.60]我手一杯 品尝你的美
[01:54.11]留下唇印的嘴
[01:58.25]花店玫瑰 名字写错谁
[02:01.59]告白气球 风吹到对街
[02:04.60]微笑在天上飞
[02:09.06]你说你有点难追
[02:11.60]想让我知难而退
[02:14.35]礼物不需挑最贵
[02:16.85]只要香榭的落叶
[02:19.60]喔～营造浪漫的约会
[02:22.35]不害怕搞砸一切
[02:24.61]拥有你就拥有 全世界
[02:30.11]亲爱的 爱上你 从那天起
[02:36.60]甜蜜的很轻易
[02:41.10]亲爱的 别任性 你的眼睛
[02:47.11]在说我愿意
[02:51.60]亲爱的 爱上你 恋爱日记
[02:58.11]飘香水的回忆
[03:01.57]一整瓶 的梦境 全都有你
[03:08.11]搅拌在一起
[03:12.61]亲爱的别任性 你的眼睛
[03:20.61]在说我愿意
找歌词，上歌词千寻 www.lrcgc.com。支持歌词找歌名，LRC歌词免费下载。</pre></div>
		<script>
			new APlayer({
				element: document.getElementById("aplayer1"),
				narrow: false,
				autoplay: false,
				showlrc: 2,
				music: {
					title: "告白气球",
					author: "周杰伦",
					url: "http://mp3.haoduoge.com/s/2016-06-28/1467087399.mp3",
					pic: "http://p3.music.126.net/cUTk0ewrQtYGP2YpPZoUng==/3265549553028224.jpg?param=130y130",
				}
			});
		</script>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;% aplayer &quot;告白气球&quot; &quot;周杰伦&quot; &quot;http://mp3.haoduoge.com/s/2016-06-28/1467087399.mp3&quot; &quot;http://p3.music.126.net/cUTk0ewrQtYGP2YpPZoUng==/3265549553028224.jpg?param=130y130&quot; &quot;lrc:周杰伦-告白气球.lrc&quot; %&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;br&gt;&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=38592976&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt;&lt;br&gt;&lt;/center&gt;
    
    </summary>
    
      <category term="Tool" scheme="https://Hzwcode.github.io/categories/Tool/"/>
    
    
      <category term="Hexo" scheme="https://Hzwcode.github.io/tags/Hexo/"/>
    
      <category term="Music" scheme="https://Hzwcode.github.io/tags/Music/"/>
    
  </entry>
  
  <entry>
    <title>[RNN] Simple LSTM代码实现 &amp; BPTT理论推导</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Backpropagation-through-time-BPTT/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Backpropagation-through-time-BPTT/</id>
    <published>2017-02-24T10:26:58.000Z</published>
    <updated>2017-03-24T16:19:50.974Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><ul>
<li>参考：<a href="http://nicodjimenez.github.io/2014/08/08/lstm.html" target="_blank" rel="external">Nico’s Blog - Simple LSTM</a></li>
<li>Github代码：<a href="https://github.com/Hzwcode/lstm" target="_blank" rel="external">https://github.com/Hzwcode/lstm</a></li>
</ul>
<hr>
<p>前面我们介绍过CNN中普通的<a href="https://Hzwcode.github.io/2017/02/24/Backpropagation-principle/">BP反向传播算法的推导</a>，但是在RNN（比如<a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="external">LSTM</a>）中，反向传播被称作<a href="https://en.wikipedia.org/wiki/Backpropagation_through_time" target="_blank" rel="external">BPTT</a>（Back Propagation Through Time），它是和时间序列有关的。</p>
<a id="more"></a>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-113aeedc747a3628.gif?imageMogr2/auto-orient/strip" alt="Back Propagation Through Time"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-4ae3ab8b8426cdcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>A few weeks ago I released some <a href="https://github.com/nicodjimenez/lstm" target="_blank" rel="external">code</a> on Github to help people understand how LSTM’s work at the implementation level. The forward pass is well explained elsewhere and is straightforward to understand, but I derived the backprop equations myself and the backprop code came without any explanation whatsoever. The goal of this post is to explain the so called <em>backpropagation through time</em> in the context of LSTM’s.</p>
<p>If you feel like anything is confusing, please post a comment below or submit an issue on Github.</p>
<p><strong>Note:</strong> this post assumes you understand the forward pass of an LSTM network, as this part is relatively simple. Please read this <a href="http://arxiv.org/abs/1506.00019" target="_blank" rel="external">great intro paper</a> if you are not familiar with this, as it contains a very nice intro to LSTM’s. I follow the same notation as this paper so I recommend reading having the tutorial open in a separate browser tab for easy reference while reading this post.</p>
<blockquote>
<h1 id="Introduction-Simple-LSTM"><a href="#Introduction-Simple-LSTM" class="headerlink" title="Introduction (Simple LSTM)"></a>Introduction (Simple LSTM)</h1></blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-4951e5c5352a88f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="LSTM Block"></p>
<p>The forward pass of an LSTM node is defined as follows:</p>
<p><img src="http://latex.codecogs.com/png.latex?\\g(t)%20&amp;=&amp;%20\phi(W_{gx}%20x(t)%20+%20W_{gh}%20h(t-1)%20+%20b_{g})%20\\\\%20i(t)%20&amp;=&amp;%20\sigma(W_{ix}%20x(t)%20+%20W_{ih}%20h(t-1)%20+%20b_{i})%20\\\\%20f(t)%20&amp;=&amp;%20\sigma(W_{fx}%20x(t)%20+%20W_{fh}%20h(t-1)%20+%20b_{f})%20\\\\%20o(t)%20&amp;=&amp;%20\sigma(W_{ox}%20x(t)%20+%20W_{oh}%20h(t-1)%20+%20b_{o})%20\\\\%20s(t)%20&amp;=&amp;%20g(t)%20*%20i(t)%20+%20s(t-1)%20*%20f(t)%20\\\\%20h(t)%20&amp;=&amp;%20s(t)%20*%20o(t)%20\\" alt=""></p>
<p>(<strong>注</strong>：这里最后一个式子<code>h(t)</code>的计算，普遍认为<code>s(t)</code>前面还有一个tanh激活，然后再乘以<code>o(t)</code>，不过 peephole LSTM paper中建议此处激活函数采用 <code>f(x) = x</code>，所以这里就没有用<code>tanh</code>（下同），可以参见<a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="external">Wiki - Long_short-term_memory</a>上面所说的)</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-ad0508a2df64e3ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>By concatenating the <code>x(t)</code> and <code>h(t-1)</code> vectors as follows:</p>
<p><img src="http://latex.codecogs.com/png.latex?x_c(t)%20=%20[x(t),%20h(t-1)]" alt=""></p>
<p>we can rewrite parts of the above as follows:</p>
<p><img src="http://latex.codecogs.com/png.latex?\\g(t)%20&amp;=&amp;%20\phi(W_{g}%20x_c(t)%20+%20b_{g})%20\\\\%20i(t)%20&amp;=&amp;%20\sigma(W_{i}%20x_c(t)%20+%20b_{i})%20\\\\%20f(t)%20&amp;=&amp;%20\sigma(W_{f}%20x_c(t)%20+%20b_{f})%20\\\\%20o(t)%20&amp;=&amp;%20\sigma(W_{o}%20x_c(t)%20+%20b_{o})" alt=""></p>
<p>Suppose we have a loss <code>l(t)</code> that we wish to minimize at every time step <code>t</code> that depends on the hidden layer <code>h</code> and the label <code>y</code> at the current time via a loss function <code>f</code>:</p>
<p><img src="http://latex.codecogs.com/png.latex?l(t)%20=%20f(h(t),%20y(t))" alt=""></p>
<p>where <code>f</code> can be any differentiable loss function, such as the Euclidean loss:</p>
<p><img src="http://latex.codecogs.com/png.latex?l(t)%20=%20f(h(t),%20y(t))%20=%20\|%20h(t)%20-%20y(t)%20\|^2" alt=""></p>
<p>Our ultimate goal in this case is to use gradient descent to minimize the loss <code>L</code> over an entire sequence of length <code>T</code>：</p>
<p><img src="http://latex.codecogs.com/png.latex?L%20=%20\sum_{t=1}^{T}%20l(t)" alt=""></p>
<p>Let’s work through the algebra of computing the loss gradient:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dw}" alt=""></p>
<p>where <code>w</code> is a scalar parameter of the model (for example it may be an entry in the matrix <code>W_gx</code>). Since the loss <code>l(t) = f(h(t),y(t))</code> only depends on the values of the hidden layer <code>h(t)</code> and the label <code>y(t)</code>, we have by the chain rule:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dw}%20=%20\sum_{t%20=%201}^{T}%20\sum_{i%20=%201}^{M}%20\frac{dL}{dh_i(t)}\frac{dh_i(t)}{dw}" alt=""></p>
<p>where <code>h_i(t)</code> is the scalar corresponding to the <code>i’th</code> memory cell’s hidden output and <code>M</code> is the total number of memory cells. Since the network propagates information forwards in time, changing <code>h_i(t)</code> will have no effect on the loss prior to time <code>t</code>, which allows us to write:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dh_i(t)}%20=%20\sum_{s=1}^T%20\frac{dl(s)}{dh_i(t)}%20=%20\sum_{s=t}^T%20\frac{dl(s)}{dh_i(t)}" alt=""></p>
<p>For notational convenience we introduce the variable <code>L(t)</code> that represents the cumulative loss from step tonwards:</p>
<p><img src="http://latex.codecogs.com/png.latex?L(t)%20=%20\sum_{s=t}^{s=T}%20l(s)" alt=""></p>
<p>such that <code>L(1)</code> is the loss for the entire sequence. This allows us to rewrite the above equation as:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dh_i(t)}%20=%20\sum_{s=t}^T%20\frac{dl(s)}{dh_i(t)}%20=%20\frac{dL(t)}{dh_i(t)}" alt=""></p>
<p>With this in mind, we can rewrite our gradient calculation as:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dw}%20=%20\sum_{t%20=%201}^{T}%20\sum_{i%20=%201}^{M}%20\frac{dL(t)}{dh_i(t)}\frac{dh_i(t)}{dw}" alt=""></p>
<p>Make sure you understand this last equation. The computation of <code>dh_i(t) / dw</code> follows directly follows from the forward propagation equations presented earlier. We now show how to compute <code>dL(t) / dh_i(t)</code> which is where the so called <strong><em>backpropagation through time</em></strong> comes into play.</p>
<blockquote>
<h1 id="Backpropagation-through-time-BPTT"><a href="#Backpropagation-through-time-BPTT" class="headerlink" title="Backpropagation through time (BPTT)"></a>Backpropagation through time (BPTT)</h1></blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-113aeedc747a3628.gif?imageMogr2/auto-orient/strip" alt="Back Propagation Through Time"></p>
<p>This variable <code>L(t)</code> allows us to express the following recursion:</p>
<p><img src="http://latex.codecogs.com/png.latex?L(t)%20=%20\begin{cases}%20l(t)%20+%20L(t+1)%20&amp;%20\text{if}%20\,%20t%20%3C%20T%20\\%20l(t)%20&amp;%20\text{if}%20\,%20t%20=%20T%20\end{cases}" alt=""></p>
<p>Hence, given activation <code>h(t)</code> of an LSTM node at time <code>t</code>, we have that:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL(t)}{dh(t)}%20=%20\frac{dl(t)}{dh(t)}%20+%20\frac{dL(t+1)}{dh(t)}" alt=""></p>
<p>Now, we know where the first term on the right hand side <code>dl(t) / dh(t)</code> comes from: it’s simply the elementwise derivative of the loss <code>l(t)</code> with respect to the activations <code>h(t)</code> at time <code>t</code>. The second term <code>dL(t+1) / dh(t)</code> is where the recurrent nature of LSTM’s shows up. It shows that the we need the <em>next</em> node’s derivative information in order to compute the current <em>current</em> node’s derivative information. Since we will ultimately need to compute <code>dL(t) / dh(t)</code> for all <code>t = 1, 2, ... , T</code>, we start by computing</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL(T)}{dh(T)}%20=%20\frac{dl(T)}{dh(T)}" alt=""></p>
<p>and work our way backwards through the network. Hence the term <em>backpropagation through time</em>. With these intuitions in place, we jump into the code.</p>
<blockquote>
<h1 id="Code-Talk-is-cheap-Show-me-the-code"><a href="#Code-Talk-is-cheap-Show-me-the-code" class="headerlink" title="Code (Talk is cheap, Show me the code)"></a>Code (Talk is cheap, Show me the code)</h1></blockquote>
<p>We now present the code that performs the backprop pass through a single node at time <code>1 &lt;= t &lt;= T</code>. The code takes as input:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-98208ee1ecaa495f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>And computes:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-28f4a30188b7dd3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>whose values will need to be propagated backwards in time. The code also adds derivatives to:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-8c47979fe8d86be1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>since recall that we must sum the derivatives from each time step:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dw}%20=%20\sum_{t%20=%201}^{T}%20\sum_{i%20=%201}^{M}%20\frac{dL(t)}{dh_i(t)}\frac{dh_i(t)}{dw}" alt=""></p>
<p>Also, note that we use:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-34424089b87efa6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>where we recall that <code>X_c(t) = [x(t), h(t-1)]</code>. Without any further due, the code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">top_diff_is</span><span class="params">(self, top_diff_h, top_diff_s)</span>:</span></div><div class="line">    <span class="comment"># notice that top_diff_s is carried along the constant error carousel</span></div><div class="line">    ds = self.state.o * top_diff_h + top_diff_s</div><div class="line">    do = self.state.s * top_diff_h</div><div class="line">    di = self.state.g * ds</div><div class="line">    dg = self.state.i * ds</div><div class="line">    df = self.s_prev * ds</div><div class="line"></div><div class="line">    <span class="comment"># diffs w.r.t. vector inside sigma / tanh function</span></div><div class="line">    di_input = (<span class="number">1.</span> - self.state.i) * self.state.i * di</div><div class="line">    df_input = (<span class="number">1.</span> - self.state.f) * self.state.f * df</div><div class="line">    do_input = (<span class="number">1.</span> - self.state.o) * self.state.o * do</div><div class="line">    dg_input = (<span class="number">1.</span> - self.state.g ** <span class="number">2</span>) * dg</div><div class="line"></div><div class="line">    <span class="comment"># diffs w.r.t. inputs</span></div><div class="line">    self.param.wi_diff += np.outer(di_input, self.xc)</div><div class="line">    self.param.wf_diff += np.outer(df_input, self.xc)</div><div class="line">    self.param.wo_diff += np.outer(do_input, self.xc)</div><div class="line">    self.param.wg_diff += np.outer(dg_input, self.xc)</div><div class="line">    self.param.bi_diff += di_input</div><div class="line">    self.param.bf_diff += df_input</div><div class="line">    self.param.bo_diff += do_input</div><div class="line">    self.param.bg_diff += dg_input</div><div class="line"></div><div class="line">    <span class="comment"># compute bottom diff</span></div><div class="line">    dxc = np.zeros_like(self.xc)</div><div class="line">    dxc += np.dot(self.param.wi.T, di_input)</div><div class="line">    dxc += np.dot(self.param.wf.T, df_input)</div><div class="line">    dxc += np.dot(self.param.wo.T, do_input)</div><div class="line">    dxc += np.dot(self.param.wg.T, dg_input)</div><div class="line"></div><div class="line">    <span class="comment"># save bottom diffs</span></div><div class="line">    self.state.bottom_diff_s = ds * self.state.f</div><div class="line">    self.state.bottom_diff_x = dxc[:self.param.x_dim]</div><div class="line">    self.state.bottom_diff_h = dxc[self.param.x_dim:]</div></pre></td></tr></table></figure>
<blockquote>
<h1 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h1></blockquote>
<p>The forward propagation equations show that modifying <code>s(t)</code> affects the loss <code>L(t)</code> by directly changing the values of <code>h(t)</code> as well as <code>h(t+1)</code>. However, modifying <code>s(t)</code> affects <code>L(t+1)</code> only by modifying <code>h(t+1)</code>. Therefore, by the chain rule:</p>
<p><img src="http://latex.codecogs.com/png.latex?\\\frac{dL(t)}{ds_i(t)}%20=%20\frac{dL(t)}{dh_i(t)}%20\frac{dh_i(t)}{ds_i(t)}%20+%20\frac{dL(t)}{dh_i(t+1)}%20\frac{dh_i(t+1)}{ds_i(t)}%20\\\\\\=%20\frac{dL(t)}{dh_i(t)}%20\frac{dh_i(t)}{ds_i(t)}%20+%20\frac{dL(t+1)}{dh_i(t+1)}%20\frac{dh_i(t+1)}{ds_i(t)}%20\\\\\\=%20\frac{dL(t)}{dh_i(t)}%20\frac{dh_i(t)}{ds_i(t)}%20+%20\frac{dL(t+1)}{ds_i(t)}%20\\\\\\%20=%20\frac{dL(t)}{dh_i(t)}%20\frac{dh_i(t)}{ds_i(t)}%20+%20[\texttt{top\_diff\_s}]_i%20\\" alt=""></p>
<p>Since the forward propagation equations state:</p>
<p><img src="http://latex.codecogs.com/png.latex?h(t)%20=%20s(t)%20*%20o(t)" alt=""></p>
<p>we get that:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL(t)}{dh_i(t)}%20*%20\frac{dh_i(t)}{ds_i(t)}%20=%20o_i(t)%20*%20[\texttt{top\_diff\_h}]_i" alt=""></p>
<p>Putting all this together we have:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ds = self.state.o * top_diff_h + top_diff_s</div></pre></td></tr></table></figure>
<p>The rest of the equations should be straightforward to derive, please let me know if anything is unclear.</p>
<hr>
<blockquote>
<h1 id="Test-LSTM-Network"><a href="#Test-LSTM-Network" class="headerlink" title="Test  LSTM Network"></a>Test  LSTM Network</h1></blockquote>
<p>此 <a href="https://github.com/Hzwcode/lstm" target="_blank" rel="external">代码</a> 其是通过自己实现 lstm 网络来逼近一个序列，y_list = [-0.5, 0.2, 0.1, -0.5]，测试结果如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line">cur iter:  0</div><div class="line">y_pred[0] : 0.041349</div><div class="line">y_pred[1] : 0.069304</div><div class="line">y_pred[2] : 0.116993</div><div class="line">y_pred[3] : 0.165624</div><div class="line">loss:  0.753483886253</div><div class="line">cur iter:  1</div><div class="line">y_pred[0] : -0.223297</div><div class="line">y_pred[1] : -0.323066</div><div class="line">y_pred[2] : -0.394514</div><div class="line">y_pred[3] : -0.433984</div><div class="line">loss:  0.599065083953</div><div class="line">cur iter:  2</div><div class="line">y_pred[0] : -0.140715</div><div class="line">y_pred[1] : -0.181836</div><div class="line">y_pred[2] : -0.219436</div><div class="line">y_pred[3] : -0.238904</div><div class="line">loss:  0.445095565699</div><div class="line">cur iter:  3</div><div class="line">y_pred[0] : -0.138010</div><div class="line">y_pred[1] : -0.166091</div><div class="line">y_pred[2] : -0.203394</div><div class="line">y_pred[3] : -0.233627</div><div class="line">loss:  0.428061605701</div><div class="line">cur iter:  4</div><div class="line">y_pred[0] : -0.139986</div><div class="line">y_pred[1] : -0.157368</div><div class="line">y_pred[2] : -0.195655</div><div class="line">y_pred[3] : -0.237612</div><div class="line">loss:  0.413581711096</div><div class="line">cur iter:  5</div><div class="line">y_pred[0] : -0.144410</div><div class="line">y_pred[1] : -0.151859</div><div class="line">y_pred[2] : -0.191676</div><div class="line">y_pred[3] : -0.246137</div><div class="line">loss:  0.399770442382</div><div class="line">cur iter:  6</div><div class="line">y_pred[0] : -0.150306</div><div class="line">y_pred[1] : -0.147921</div><div class="line">y_pred[2] : -0.189501</div><div class="line">y_pred[3] : -0.257119</div><div class="line">loss:  0.386136380384</div><div class="line">cur iter:  7</div><div class="line">y_pred[0] : -0.157119</div><div class="line">y_pred[1] : -0.144659</div><div class="line">y_pred[2] : -0.188067</div><div class="line">y_pred[3] : -0.269322</div><div class="line">loss:  0.372552465753</div><div class="line">cur iter:  8</div><div class="line">y_pred[0] : -0.164490</div><div class="line">y_pred[1] : -0.141537</div><div class="line">y_pred[2] : -0.186737</div><div class="line">y_pred[3] : -0.281914</div><div class="line">loss:  0.358993892096</div><div class="line">cur iter:  9</div><div class="line">y_pred[0] : -0.172187</div><div class="line">y_pred[1] : -0.138216</div><div class="line">y_pred[2] : -0.185125</div><div class="line">y_pred[3] : -0.294326</div><div class="line">loss:  0.345449256686</div><div class="line">cur iter:  10</div><div class="line">y_pred[0] : -0.180071</div><div class="line">y_pred[1] : -0.134484</div><div class="line">y_pred[2] : -0.183013</div><div class="line">y_pred[3] : -0.306198</div><div class="line">loss:  0.331888922037</div><div class="line"></div><div class="line">……</div><div class="line"></div><div class="line">cur iter:  97</div><div class="line">y_pred[0] : -0.500351</div><div class="line">y_pred[1] : 0.201185</div><div class="line">y_pred[2] : 0.099026</div><div class="line">y_pred[3] : -0.499154</div><div class="line">loss:  3.1926009167e-06</div><div class="line">cur iter:  98</div><div class="line">y_pred[0] : -0.500342</div><div class="line">y_pred[1] : 0.201122</div><div class="line">y_pred[2] : 0.099075</div><div class="line">y_pred[3] : -0.499190</div><div class="line">loss:  2.88684626031e-06</div><div class="line">cur iter:  99</div><div class="line">y_pred[0] : -0.500331</div><div class="line">y_pred[1] : 0.201063</div><div class="line">y_pred[2] : 0.099122</div><div class="line">y_pred[3] : -0.499226</div><div class="line">loss:  2.61076360677e-06</div></pre></td></tr></table></figure>
<p>可以看出迭代100轮，最后Loss在不断收敛，并且逐渐逼近了预期序列：y_list = [-0.5, 0.2, 0.1, -0.5]。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="http://www.jianshu.com/p/408ab8177a53" target="_blank" rel="external">深度学习 — 反向传播(BP)理论推导 (zhwhong)</a></li>
<li><a href="http://nicodjimenez.github.io/2014/08/08/lstm.html" target="_blank" rel="external">Nico’s Blog：Simple LSTM</a></li>
<li><a href="https://github.com/Hzwcode/lstm" target="_blank" rel="external">Github仓库：https://github.com/Hzwcode/lstm</a></li>
<li><a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank" rel="external">RECURRENT NEURAL NETWORKS TUTORIAL, PART 3 – BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS</a></li>
<li><a href="http://www.jianshu.com/p/c930d61e1f16" target="_blank" rel="external">[福利] 深入理解 RNNs &amp; LSTM 网络学习资料</a></li>
<li><a href="http://www.jianshu.com/p/c7e3f417641c" target="_blank" rel="external">关于简书中如何编辑Latex数学公式</a></li>
</ul>
<hr>
<p>(转载请联系作者并注明出处，谢谢！)</p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;参考：&lt;a href=&quot;http://nicodjimenez.github.io/2014/08/08/lstm.html&quot;&gt;Nico’s Blog - Simple LSTM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github代码：&lt;a href=&quot;https://github.com/Hzwcode/lstm&quot;&gt;https://github.com/Hzwcode/lstm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;前面我们介绍过CNN中普通的&lt;a href=&quot;https://Hzwcode.github.io/2017/02/24/Backpropagation-principle/&quot;&gt;BP反向传播算法的推导&lt;/a&gt;，但是在RNN（比如&lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot;&gt;LSTM&lt;/a&gt;）中，反向传播被称作&lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation_through_time&quot;&gt;BPTT&lt;/a&gt;（Back Propagation Through Time），它是和时间序列有关的。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Algorithm" scheme="https://Hzwcode.github.io/tags/Algorithm/"/>
    
      <category term="RNN" scheme="https://Hzwcode.github.io/tags/RNN/"/>
    
      <category term="LSTM" scheme="https://Hzwcode.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 — 反向传播(BP)理论推导</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Backpropagation-principle/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Backpropagation-principle/</id>
    <published>2017-02-24T10:23:35.000Z</published>
    <updated>2017-03-24T07:26:49.605Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><ul>
<li><a href="https://Hzwcode.github.io/2017/02/24/Backpropagation-through-time-BPTT/">[RNN] Simple LSTM代码实现 &amp; BPTT理论推导</a></li>
</ul>
<hr>
<p>【知识预备】： <a href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" target="_blank" rel="external">UFLDL教程 - 反向传导算法</a></p>
<p>首先我们不讲数学，先上图解，看完图不懂再看后面：</p>
<a id="more"></a>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-be0f5712599bf47b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-190148f7a5f6d59a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-9c0e2a3e41e50184.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-67d7988a4783c6a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-6c9b26999076e229.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-25ed873c3fd53595.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-2af819d45509d1e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-40c7e1c9c6f8cd66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-73012c1bbefe6fd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-d95cd8caa246cfd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-7b7e599bf97627ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-ef5d956b6c35c904.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-e801483bf206b984.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-74eacee144d4ac4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-7ad6f7e9368f4c91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-6cb99673d9ba0fa3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-7420efdf411bbf82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-ce90b252f0901bc6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-d830f54f90ba8f24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-84cef5edf507cd73.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<h1 id="“BP”-Math-Principle"><a href="#“BP”-Math-Principle" class="headerlink" title="“BP” Math Principle"></a>“BP” Math Principle</h1><p>======================================================================<br><strong>Example</strong>：下面看一个简单的三层神经网络模型，一层输入层，一层隐藏层，一层输出层。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-4a6d84a2e3f81c87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>注：定义输入分别为x1, x2（对应图中的i1，i2），期望输出为y1，y2，假设logistic函数采用sigmoid函数:</p>
<p><img src="http://latex.codecogs.com/png.latex?y%20=%20f(x)=sigmoid(x)%20=\frac{1}{1%20+%20e^{-x}}" alt=""></p>
<p>易知：<br><img src="http://latex.codecogs.com/png.latex?f%27(x)%20=%20f(x)%20*%20(1%20-%20f(x))" alt=""></p>
<p>下面开始正式分析(纯手打！！！)。</p>
<p>======================================================================</p>
<h1 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a><strong>前向传播</strong></h1><p>首先分析神经元h1： </p>
<p><img src="http://latex.codecogs.com/png.latex?input_{(h1)}%20=%20w1%20*%20x1%20+%20w2%20*%20x2%20+%20b1" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?output_{(h1)}%20=%20f(input_{(h1)})%20=%20\frac{1}{1%20+%20e^{-(w1*x1+w2*x2+b1)}}" alt=""></p>
<p>同理可得神经元h2：<br><img src="http://latex.codecogs.com/png.latex?input_{(h2)}%20=%20w3%20*%20x1%20+%20w4%20*%20x2%20+%20b1" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?output_{(h2)}%20=%20f(input_{(h2)})%20=%20\frac{1}{1%20+%20e^{-(w3*x1+w4*x2+b1)}}" alt=""></p>
<p>对输出层神经元重复这个过程，使用隐藏层神经元的输出作为输入。这样就能给出o1，o2的输入输出：<br><img src="http://latex.codecogs.com/png.latex?input_{(o1)}%20=%20w5%20*%20output_{(h1)}%20+%20w6%20*%20output_{(h2)}%20+%20b2" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?output_{(o1)}%20=%20f(input_{(o1)})" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?input_{(o2)}%20=%20w7%20*%20output_{(h1)}%20+%20w8%20*%20output_{(h2)}%20+%20b2" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?output_{(o2)}%20=%20f(input_{(o2)})" alt=""></p>
<p>现在开始统计所有误差，如下：<br><img src="http://latex.codecogs.com/png.latex?J_{total}%20=%20\sum%20\frac{1}{2}(output%20-%20target)^2%20=%20J_{o1}+J_{o2}" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?J_{o1}%20=%20\frac{1}{2}(output(o1)-y1)^2" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?J_{o2}%20=%20\frac{1}{2}(output(o2)-y2)^2" alt=""></p>
<p>======================================================================</p>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a><strong>反向传播</strong></h1><h2 id="【输出层】"><a href="#【输出层】" class="headerlink" title="【输出层】"></a><strong>【输出层】</strong></h2><p>对于w5，想知道其改变对总误差有多少影响，于是求Jtotal对w5的偏导数，如下：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w5}=\frac{\partial%20J_{total}}{\partial%20output_{(o1)}}*\frac{\partial%20output_{(o1)}}{\partial%20input_{(o1)}}*\frac{\partial%20input_{(o1)}}{\partial%20w5}" alt=""></p>
<p>分别求每一项：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20output_{(o1)}}=\frac{\partial%20J_{o1}}{\partial%20output_{(o1)}}=output_{(o1)}-y_1" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20output_{(o1)}}{\partial%20input_{(o1)}}%20=%20f%27(input_{(o1)})=output_{(o1)}*(1%20-%20output_{(o1)})" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20input_{(o1)}}{\partial%20w5}=\frac{\partial%20(w5%20*%20output_{(h1)}%20+%20w6%20*%20output_{(h2)}%20+%20b2)}{\partial%20w5}=output_{(h1)}" alt=""></p>
<p>于是有Jtotal对w5的偏导数：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w5}=(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*output_{(h1)}" alt=""></p>
<p>据此更新权重w5，有：<br><img src="http://latex.codecogs.com/png.latex?w5^+%20=%20w5%20-%20\eta*\frac{\partial%20J_{total}}{\partial%20w5}" alt=""></p>
<p>同理可以更新参数w6，w7，w8。<br>在有新权重导入隐藏层神经元（即，当继续下面的反向传播算法时，使用原始权重，而不是更新的权重）之后，执行神经网络中的实际更新。</p>
<h2 id="【隐藏层】"><a href="#【隐藏层】" class="headerlink" title="【隐藏层】"></a><strong>【隐藏层】</strong></h2><p><img src="http://upload-images.jianshu.io/upload_images/145616-4f4ed88c60ee15e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于w1，想知道其改变对总误差有多少影响，于是求Jtotal对w1的偏导数，如下：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w1}=\frac{\partial%20J_{total}}{\partial%20output_{(h1)}}*\frac{\partial%20output_{(h1)}}{\partial%20input_{(h1)}}*\frac{\partial%20input_{(h1)}}{\partial%20w1}" alt=""></p>
<p>分别求每一项：</p>
<hr>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20output_{(h1)}}=\frac{\partial%20J_{o1}}{\partial%20output_{(h1)}}+\frac{\partial%20J_{o2}}{\partial%20output_{(h1)}}" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{o1}}{\partial%20output_{(h1)}}=\frac{\partial%20J_{o1}}{\partial%20output_{(o1)}}*\frac{\partial%20output_{(o1)}}{\partial%20input_{(o1)}}*\frac{\partial%20input_{(o1)}}{\partial%20output_{(h1)}}" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?=(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*w5" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{o2}}{\partial%20output_{(h1)}}=\frac{\partial%20J_{o2}}{\partial%20output_{(o2)}}*\frac{\partial%20output_{(o2)}}{\partial%20input_{(o2)}}*\frac{\partial%20input_{(o2)}}{\partial%20output_{(h1)}}" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?=(output_{(o2)}-y2)*[output_{(o2)}*(1%20-%20output_{(o2)})]*w7" alt=""></p>
<hr>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20output_{(h1)}}{\partial%20input_{(h1)}}%20=%20f%27(input_{(h1)})=output_{(h1)}*(1%20-%20output_{(h1)})" alt=""></p>
<hr>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20input_{(h1)}}{\partial%20w1}=\frac{\partial%20(w1%20*%20x1%20+%20w2%20*%20x2%20+%20b1)}{\partial%20w1}=x1" alt=""></p>
<p>于是有Jtotal对w1的偏导数：</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w1}=\{(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*w5" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?+%20(output_{(o2)}-y2)*[output_{(o2)}*(1%20-%20output_{(o2)})]*w7\}*" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?[output_{(h1)}*(1%20-%20output_{(h1)})]*x1" alt=""></p>
<p>据此更新w1，有：</p>
<p><img src="http://latex.codecogs.com/png.latex?w1^+%20=%20w1%20-%20\eta*\frac{\partial%20J_{total}}{\partial%20w1}" alt=""></p>
<p>同理可以更新参数w2，w3，w4。</p>
<p>======================================================================</p>
<h1 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a><strong>应用实例</strong></h1><p>假设对于上述简单三层网络模型，按如下方式初始化权重和偏置：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-c8c0d034ff7a0c4f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>根据上述推导的公式：<br>由</p>
<p><img src="http://latex.codecogs.com/png.latex?input_{(h1)}%20=%20w1%20*%20x1%20+%20w2%20*%20x2%20+%20b1" alt=""></p>
<p>得到：<br>input(h1) = 0.15 * 0.05 + 0.20 * 0.10 + 0.35 = 0.3775<br>output(h1) = f(input(h1)) = 1 / (1 + e^(-input(h1))) = 1 / (1 + e^-0.3775) = 0.593269992</p>
<p>同样得到：<br>input(h2) = 0.25 * 0.05 + 0.30 * 0.10 + 0.35 = 0.3925<br>output(h2) = f(input(h2)) = 1 / (1 + e^(-input(h2))) = 1 / (1 + e^-0.3925) = 0.596884378</p>
<p>对输出层神经元重复这个过程，使用隐藏层神经元的输出作为输入。这样就能给出o1的输出：<br>input(o1) = w5 * output(h1) + w6 * (output(h2)) + b2 = 0.40 * 0.593269992 + 0.45 * 0.596884378 + 0.60 = 1.105905967<br>output(o1) = f(input(o1)) = 1 / (1 + e^-1.105905967) = 0.75136507</p>
<p>同理output(o2) = 0.772928465</p>
<p>开始统计所有误差，求代价函数：<br>Jo1 = 1/2 * (0.75136507 - 0.01)^2 = 0.298371109<br>Jo2 = 1/2 * (0.772928465 - 0.99)^2 = 0.023560026</p>
<p><strong>综合所述</strong>，可以得到总误差为：Jtotal = Jo1 + Jo2 = 0.321931135</p>
<p>然后反向传播，根据公式<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w5}=(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*output_{(h1)}" alt=""></p>
<p>求出 Jtotal对w5的偏导数为:<br>a = (0.75136507 - 0.01)*0.75136507*(1-0.75136507)*0.593269992 = 0.082167041</p>
<p>为了减少误差，然后从当前的权重减去这个值（可选择乘以一个学习率，比如设置为0.5），得：<br>w5+ = w5 - eta * a = 0.40 - 0.5 * 0.082167041 = 0.35891648</p>
<p>同理可以求出：<br>w6+ = 0.408666186<br>w7+ = 0.511301270<br>w8+ = 0.561370121</p>
<p>对于隐藏层，更新w1，求Jtotal对w1的偏导数：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w1}=\{(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*w5" alt=""><br><img src="http://latex.codecogs.com/png.latex?+%20(output_{(o2)}-y2)*[output_{(o2)}*(1%20-%20output_{(o2)})]*w7\}*" alt=""><br><img src="http://latex.codecogs.com/png.latex?[output_{(h1)}*(1%20-%20output_{(h1)})]*x1" alt=""></p>
<p>偏导数为：<br>b = (tmp1 + tmp2) * tmp3</p>
<p>tmp1 = (0.75136507 - 0.01) * [0.75136507 * (1 - 0.75136507)] * 0.40 = 0.74136507 * 0.186815602 * 0.40 = 0.055399425<br>tmp2 = -0.019049119<br>tmp3 = 0.593269992 * (1 - 0.593269992) * 0.05 = 0.012065035</p>
<p>于是b = 0.000438568</p>
<p>更新权重w1为：<br>w1+ = w1 - eta * b = 0.15 - 0.5 * 0.000438568 = 0.149780716</p>
<p>同样可以求得：<br>w2+ = 0.19956143<br>w3+ = 0.24975114<br>w4+ = 0.29950229</p>
<p>最后，更新了所有的权重！ 当最初前馈传播时输入为0.05和0.1，网络上的误差是0.298371109。 在第一轮反向传播之后，总误差现在下降到0.291027924。 它可能看起来不太多，但是在重复此过程10,000次之后。例如，错误倾斜到0.000035085。<br>在这一点上，当前馈输入为0.05和0.1时，两个输出神经元产生0.015912196（相对于目标为0.01）和0.984065734（相对于目标为0.99），已经很接近了O(∩_∩)O~~</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/23270674" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/23270674</a></li>
<li><a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="external">Principles of training multi-layer neural network using backpropagation</a></li>
<li><a href="http://www.jianshu.com/p/2aca6e8ac7c8" target="_blank" rel="external">[RNN] Simple LSTM代码实现 &amp; BPTT理论推导</a></li>
<li><a href="http://www.jianshu.com/p/c7e3f417641c" target="_blank" rel="external">简书中如何编辑Latex数学公式</a></li>
</ul>
<hr>
<p>(转载请联系作者并注明出处，谢谢！)</p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://Hzwcode.github.io/2017/02/24/Backpropagation-through-time-BPTT/&quot;&gt;[RNN] Simple LSTM代码实现 &amp;amp; BPTT理论推导&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;【知识预备】： &lt;a href=&quot;http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95&quot;&gt;UFLDL教程 - 反向传导算法&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先我们不讲数学，先上图解，看完图不懂再看后面：&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://Hzwcode.github.io/tags/Deep-Learning/"/>
    
      <category term="Algorithm" scheme="https://Hzwcode.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>[Detection] 深度学习之 &quot;物体检测&quot; 方法梳理</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Detection-CNN/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Detection-CNN/</id>
    <published>2017-02-24T10:20:50.000Z</published>
    <updated>2017-03-24T16:29:16.076Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h2 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h2><ul>
<li><a href="http://arxiv.org/abs/1311.2524" target="_blank" rel="external">RCNN</a> </li>
<li><a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="external">SPP-Net</a></li>
<li><a href="http://arxiv.org/abs/1504.08083" target="_blank" rel="external">Fast RCNN</a> </li>
<li><a href="http://arxiv.org/abs/1506.01497" target="_blank" rel="external">Faster RCNN</a> </li>
<li><a href="http://arxiv.org/pdf/1605.06409v1.pdf" target="_blank" rel="external">R-FCN</a></li>
<li><a href="http://arxiv.org/abs/1506.02640" target="_blank" rel="external">YOLO</a> </li>
<li><a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="external">YOLO2</a></li>
<li><a href="http://www.cs.unc.edu/~wliu/papers/ssd.pdf" target="_blank" rel="external">SSD</a> </li>
<li><a href="http://ieeexplore.ieee.org/abstract/document/1699659/" target="_blank" rel="external">NMS</a></li>
</ul>
<a id="more"></a>
<hr>
<p>　　本文部分转载自：<a href="http://www.cnblogs.com/venus024/p/5590044.html" target="_blank" rel="external">深度学习检测方法梳理</a>，原作者venus024，但是额外补充了一些其他相关内容，仅供学习交流使用，不得用于商业途径，转载请联系作者并注明出处，谢谢。</p>
<h2 id="1-RCNN"><a href="#1-RCNN" class="headerlink" title="1. RCNN"></a>1. RCNN</h2><blockquote>
<p>论文出处：<a href="http://arxiv.org/abs/1311.2524" target="_blank" rel="external">Rich feature hierarchies for accurate object detection and semantic segmentation</a><br>论文作者：<a href="https://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1" target="_blank" rel="external">Ross Girshick</a>, <a href="https://arxiv.org/find/cs/1/au:+Donahue_J/0/1/0/all/0/1" target="_blank" rel="external">Jeff Donahue</a>, <a href="https://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1" target="_blank" rel="external">Trevor Darrell</a>, <a href="https://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1" target="_blank" rel="external">Jitendra Malik</a></p>
</blockquote>
<ul>
<li><strong>技术路线</strong>：selective search + CNN + SVMs</li>
</ul>
<p>　　早期，使用窗口扫描进行物体识别，计算量大。 RCNN去掉窗口扫描，用聚类方式，对图像进行分割分组，得到多个侯选框的层次组。</p>
<p><img src="RCNN.png" alt="RCNN"></p>
<h3 id="Step1-候选框提取-selective-search"><a href="#Step1-候选框提取-selective-search" class="headerlink" title="Step1:候选框提取(selective search)"></a>Step1:候选框提取(selective search)</h3><ul>
<li><strong>训练</strong>：给定一张图片，利用seletive search方法从中提取出2000个候选框。由于候选框大小不一，考虑到后续CNN要求输入的图片大小统一，将2000个候选框全部resize到227*227分辨率（为了避免图像扭曲严重，中间可以采取一些技巧减少图像扭曲）。</li>
<li><strong>测试</strong>：给定一张图片，利用seletive search方法从中提取出2000个候选框。由于候选框大小不一，考虑到后续CNN要求输入的图片大小统一，将2000个候选框全部resize到227*227分辨率（为了避免图像扭曲严重，中间可以采取一些技巧减少图像扭曲）。</li>
</ul>
<h3 id="Step2-特征提取-CNN"><a href="#Step2-特征提取-CNN" class="headerlink" title="Step2:特征提取(CNN)"></a>Step2:特征提取(CNN)</h3><ul>
<li><strong>训练</strong>：提取特征的CNN模型需要预先训练得到。训练CNN模型时，对训练数据标定要求比较宽松，即SS方法提取的proposal只包含部分目标区域时，我们也将该proposal标定为特定物体类别。这样做的主要原因在于，CNN训练需要大规模的数据，如果标定要求极其严格（即只有完全包含目标区域且不属于目标的区域不能超过一个小的阈值），那么用于CNN训练的样本数量会很少。因此，宽松标定条件下训练得到的CNN模型只能用于特征提取。</li>
<li><strong>测试</strong>：得到统一分辨率227<em>227的proposal后，带入训练得到的CNN模型，最后一个全连接层的输出结果—4096</em>1维度向量即用于最终测试的特征。</li>
</ul>
<h3 id="Step3-分类器-SVMs"><a href="#Step3-分类器-SVMs" class="headerlink" title="Step3:分类器(SVMs)"></a>Step3:分类器(SVMs)</h3><ul>
<li><strong>训练</strong>：对于所有proposal进行严格的标定（可以这样理解，当且仅当一个候选框完全包含ground truth区域且不属于ground truth部分不超过e.g,候选框区域的5%时认为该候选框标定结果为目标，否则位背景），然后将所有proposal经过CNN处理得到的特征和SVM新标定结果输入到SVMs分类器进行训练得到分类器预测模型。</li>
<li><strong>测试</strong>：对于一副测试图像，提取得到的2000个proposal经过CNN特征提取后输入到SVM分类器预测模型中，可以给出特定类别评分结果。</li>
<li><strong>结果生成</strong>：得到SVMs对于所有Proposal的评分结果，将一些分数较低的proposal去掉后，剩下的proposal中会出现候选框相交的情况。采用非极大值抑制技术，对于相交的两个框或若干个框，找到最能代表最终检测结果的候选框。</li>
</ul>
<hr>
<p>　　R-CNN需要对SS提取得到的每个proposal进行一次前向CNN实现特征提取，因此计算量很大，无法实时。此外，由于全连接层的存在，需要严格保证输入的proposal最终resize到相同尺度大小，这在一定程度造成图像畸变，影响最终结果。</p>
<ul>
<li>拓展阅读：<a href="http://blog.csdn.net/hjimce/article/details/50187029" target="_blank" rel="external">基于R-CNN的物体检测-CVPR 2014</a></li>
</ul>
<h2 id="2-SPP-Net"><a href="#2-SPP-Net" class="headerlink" title="2. SPP-Net"></a>2. SPP-Net</h2><blockquote>
<p>论文出处：<a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="external">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a><br>论文作者：<a href="https://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1" target="_blank" rel="external">Kaiming He</a>, <a href="https://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1" target="_blank" rel="external">Xiangyu Zhang</a>, <a href="https://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1" target="_blank" rel="external">Shaoqing Ren</a>, <a href="https://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1" target="_blank" rel="external">Jian Sun</a><br>GitHub参考： <a href="https://github.com/ShaoqingRen/SPP_net" target="_blank" rel="external">https://github.com/ShaoqingRen/SPP_net</a></p>
</blockquote>
<p>传统CNN和SPP-Net流程对比如下图(图片来自<a href="http://www.image-net.org/challenges/LSVRC/2014/slides/sppnet_ilsvrc2014.pdf" target="_blank" rel="external">这里</a>)所示：</p>
<p><img src="SPP_net_1.png" alt="传统CNN和SPP-Net流程对比"></p>
<p><strong>SPP-net具有以下特点：</strong></p>
<ol>
<li>传统CNN网络中，卷积层对输入图像大小不作特别要求，但全连接层要求输入图像具有统一尺寸大小。因此，在R-CNN中，对于selective search方法提出的不同大小的proposal需要先通过Crop操作或Wrap操作将proposal区域裁剪为统一大小，然后用CNN提取proposal特征。相比之下，SPP-net在最后一个卷积层与其后的全连接层之间添加了一个SPP (spatial pyramid pooling) layer，从而避免对propsal进行Crop或Warp操作。总而言之，SPP-layer适用于不同尺寸的输入图像，通过SPP-layer对最后一个卷积层特征进行pool操作并产生固定大小feature map,进而匹配后续的全连接层。</li>
<li>由于SPP-net支持不同尺寸输入图像，因此SPP-net提取得到的图像特征具有更好的尺度不变性，降低了训练过程中的过拟合可能性。</li>
<li>R-CNN在训练和测试是需要对每一个图像中每一个proposal进行一遍CNN前向特征提取，如果是2000个propsal,需要2000次前向CNN特征提取。但SPP-net只需要进行一次前向CNN特征提取，即对整图进行CNN特征提取，得到最后一个卷积层的feature map，然后采用SPP-layer根据空间对应关系得到相应proposal的特征。SPP-net速度可以比R-CNN速度快24~102倍，且准确率比R-CNN更高（下图引自SPP-net原作论文，可以看到SPP-net中spp-layer前有5个卷积层，第5个卷积层的输出特征在位置上可以对应到原来的图像，例如第一个图中左下角车轮在其conv5的图中显示为“^”的激活区域，因此基于此特性，SPP-net只需要对整图进行一遍前向卷积，在得到的conv5特征后，然后用SPP-net分别提取相应proposal的特征）。</li>
</ol>
<p><img src="SPP_net_2.png" alt=""></p>
<p><strong>SPP-Layer原理：</strong></p>
<p>　　在RNN中，conv5后是pool5;在SPP-net中，用SPP-layer替代原来的pool5，其目标是为了使不同大小输入图像在经过SPP-Layer后得到的特征向量长度相同。其原理如图如下所示：</p>
<p><img src="SPP_net_3.png" alt=""></p>
<p>　　SPP与金字塔pooling类似，即我们先确定最终pooling得到的featuremap大小，例如4*4 bins，3*3 bins，2*2 bins，1*1 bins。那么我们已知conv5输出的featuremap大小（例如，256个13*13的feature map）.那么，对于一个13*13的feature map,我们可以通过spatial pyramid pooling （SPP）的方式得到输出结果：当window=ceil(13/4)=4, stride=floor(13/4)=3,可以得到的4*4 bins；当window=ceil(13/3)=5, stride=floor(13/3)=4,可以得到的3*3 bins；当window=ceil(13/2)=7, stride=floor(13/2)=6,可以得到的2*2 bins；当window=ceil(13/1)=13, stride=floor(13/1)=13,可以得到的1*1 bins.因此SPP-layer后的输出是256*（4*4+3*3+2*2+1*1）=256*30长度的向量。不难看出，SPP的关键实现在于通过conv5输出的feature map宽高和SPP目标输出bin的宽高计算spatial pyramid pooling中不同分辨率Bins对应的pooling window和pool stride尺寸。</p>
<p>　　原作者在训练时采用两种不同的方式，即1.采用相同尺寸的图像训练SPP-net 2.采用不同尺寸的图像训练SPP-net。实验结果表明：使用不同尺寸输入图像训练得到的SPP-Net效果更好。</p>
<p><strong>SPP-Net +SVM训练：</strong></p>
<p>　　采用selective search可以提取到一系列proposals，由于已经训练完成SPP-Net,那么我们先将整图代入到SPP-Net中，得到的conv5的输出。接下来，区别于R-CNN，新方法不需要对不同尺寸的proposals进行Crop或Wrap，直接根据proposal在图中的相对位置关系计算得到proposal在整图conv5输出中的映射输出结果。这样，对于2000个proposal，我们事实上从conv1—&gt;conv5只做了一遍前向，然后进行2000次conv5 featuremap的集合映射，再通过SPP-Layer，就可以得到的2000组长度相同的SPP-Layer输出向量，进而通过全连接层生成最终2000个proposal的卷积神经网络特征。接下来就和R-CNN类似，训练SVMs时对于所有proposal进行严格的标定（可以这样理解，当且仅当一个候选框完全包含ground truth区域且不属于ground truth部分不超过e.g,候选框区域的5%时认为该候选框标定结果为目标，否则位背景），然后将所有proposal经过CNN处理得到的特征和SVM新标定结果输入到SVMs分类器进行训练得到分类器预测模型。</p>
<p>　　当然，如果觉得SVM训练很麻烦，可以直接在SPP-Net后再加一个softmax层，用好的标定结果去训练最后的softmax层参数。</p>
<ul>
<li>拓展阅读：<a href="http://blog.csdn.net/qq_26898461/article/details/50462940" target="_blank" rel="external">RCNN SPP_net</a></li>
</ul>
<h2 id="3-Fast-RCNN"><a href="#3-Fast-RCNN" class="headerlink" title="3. Fast RCNN"></a>3. Fast RCNN</h2><blockquote>
<p>论文出处：<a href="http://arxiv.org/abs/1504.08083" target="_blank" rel="external">Fast R-CNN</a><br>论文作者：<a href="https://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1" target="_blank" rel="external">Ross Girshick</a></p>
</blockquote>
<p>　　基于R-CNN和SPP-Net思想，RBG提出了Fast-R-CNN算法。如果选用VGG16网络进行特征提取，在训练阶段，Fast-R-CNN的速度相比RCNN和SPP-Net可以分别提升9倍和3倍；在测试阶段，Fast-R-CNN的速度相比RCNN和SPP-Net可以分别提升213倍和10倍。</p>
<p><strong>R-CNN和SPP-Net缺点：</strong></p>
<ol>
<li>R-CNN和SPP-Net的训练过程类似，分多个阶段进行，实现过程较复杂。这两种方法首先选用Selective Search方法提取proposals,然后用CNN实现特征提取，最后基于SVMs算法训练分类器，在此基础上还可以进一步学习检测目标的boulding box。</li>
<li>R-CNN和SPP-Net的时间成本和空间代价较高。SPP-Net在特征提取阶段只需要对整图做一遍前向CNN计算，然后通过空间映射方式计算得到每一个proposal相应的CNN特征；区别于前者，RCNN在特征提取阶段对每一个proposal均需要做一遍前向CNN计算，考虑到proposal数量较多（~2000个），因此RCNN特征提取的时间成本很高。R-CNN和SPP-Net用于训练SVMs分类器的特征需要提前保存在磁盘，考虑到2000个proposal的CNN特征总量还是比较大，因此造成空间代价较高。</li>
<li>R-CNN检测速度很慢。RCNN在特征提取阶段对每一个proposal均需要做一遍前向CNN计算，如果用VGG进行特征提取，处理一幅图像的所有proposal需要47s。</li>
<li>特征提取CNN的训练和SVMs分类器的训练在时间上是先后顺序，两者的训练方式独立，因此SVMs的训练Loss无法更新SPP-Layer之前的卷积层参数，因此即使采用更深的CNN网络进行特征提取，也无法保证SVMs分类器的准确率一定能够提升。</li>
</ol>
<p><strong>Fast-R-CNN亮点：</strong></p>
<ol>
<li>Fast-R-CNN检测效果优于R-CNN和SPP-Net</li>
<li>训练方式简单，基于多任务Loss,不需要SVM训练分类器。</li>
<li>Fast-R-CNN可以更新所有层的网络参数（采用ROI Layer将不再需要使用SVM分类器，从而可以实现整个网络端到端训练）。</li>
<li>不需要将特征缓存到磁盘。</li>
</ol>
<p><strong>Fast-R-CNN架构：</strong></p>
<p>　　Fast-R-CNN的架构如下图所示（可以参考此<a href="https://github.com/rbgirshick/fast-rcnn/blob/master/models/VGG16/train.prototxt" target="_blank" rel="external">链接</a>理解网络模型）：输入一幅图像和Selective Search方法生成的一系列Proposals，通过一系列卷积层和Pooling层生成feature map,然后用RoI（region of ineterst）层处理最后一个卷积层得到的feature map为每一个proposal生成一个定长的特征向量roi_pool5。RoI层的输出roi_pool5接着输入到全连接层产生最终用于多任务学习的特征并用于计算多任务Loss。全连接输出包括两个分支：<code>1.SoftMax Loss</code>:计算K+1类的分类Loss函数，其中K表示K个目标类别，1表示背景；<code>2.Regression Loss</code>:即K+1的分类结果相应的Proposal的Bounding Box四个角点坐标值。最终将所有结果通过非极大抑制处理产生最终的目标检测和识别结果。</p>
<p><img src="Fast_RCNN.png" alt=""></p>
<h3 id="3-1-RoI-Pooling-Layer"><a href="#3-1-RoI-Pooling-Layer" class="headerlink" title="3.1 RoI Pooling Layer"></a>3.1 RoI Pooling Layer</h3><p>　　事实上，RoI Pooling Layer是SPP-Layer的简化形式。SPP-Layer是空间金字塔Pooling层，包括不同的尺度；RoI Layer只包含一种尺度，如论文中所述7*7。这样对于RoI Layer的输入（r,c,h,w），RoI Layer首先产生7*7个r*c*(h/7)*(w/7)的Block(块)，然后用Max-Pool方式求出每一个Block的最大值，这样RoI Layer的输出是r*c*7*7。</p>
<h3 id="3-2-预训练网络初始化"><a href="#3-2-预训练网络初始化" class="headerlink" title="3.2 预训练网络初始化"></a>3.2 预训练网络初始化</h3><p>　　RBG复用了VGG训练ImageNet得到的网络模型，即VGG16模型以初始化Fast-R-CNN中RoI Layer以前的所有层。Fast R-CNN的网络结构整体可以总结如下：13个convolution layers + 4个pooling layers+RoI layer+2个fc layer+两个parrel层（即SoftmaxLoss layer和SmoothL1Loss layer）。在Fast R-CNN中，原来VGG16中第5个pooling layer被新的ROI layer替换掉。</p>
<h3 id="3-3-Finetuning-for-detection"><a href="#3-3-Finetuning-for-detection" class="headerlink" title="3.3 Finetuning for detection"></a>3.3 Finetuning for detection</h3><p><strong>(1) fast r-cnn在网络训练阶段采用了一些trick</strong>，每个minibatch是由N幅图片（N=2）中提取得到的R个proposal（R=128）组成的。这种minibatch的构造方式比从128张不同图片中提取1个proposal的构造方式快64倍。虽然minibatch的构造速度加快，但也在一定程度上造成收敛速度减慢。此外，fast-r-cnn摒弃了之前svm训练分类器的方式，而是选用softmax classifer和bounding-box regressors联合训练的方式更新cnn网络所有层参数。注意：在每2张图中选取128个proposals时，需要严格保证至少25%的正样本类（proposals与groundtruth的IoU超过0.5），剩下的可全部视作背景类。在训练网络模型时，不需要任何其他形式的数据扩增操作。<br><strong>(2) multi-task loss</strong>：fast r-cnn包括两个同等水平的sub-layer，分别用于classification和regression。其中，softmax loss对应于classification，smoothL1Loss对应于regression. 两种Loss的权重比例为1：1<br><strong>(3) SGD hyer-parameters</strong>：用于softmax分类任务和bounding-box回归的fc层参数用标准差介于0.01~0.001之间的高斯分布初始化。</p>
<h3 id="3-4-Truncated-SVD快速检测"><a href="#3-4-Truncated-SVD快速检测" class="headerlink" title="3.4 Truncated SVD快速检测"></a>3.4 Truncated SVD快速检测</h3><p>在检测段，RBG使用truncated SVD优化较大的FC层，这样RoI数目较大时检测端速度会得到的加速。</p>
<p><strong>Fast-R-CNN实验结论：</strong></p>
<ol>
<li>multi-task loss训练方式能提高算法准确度</li>
<li>multi-scale图像训练fast r-cnn相比较single-scale图像训练相比对mAP的提升幅度很小，但是却增加了很高的时间成本。因此，综合考虑训练时间和mAP，作者建议直接用single尺度的图像训练fast-r-cnn。</li>
<li>用于训练的图像越多，训练得到的模型准确率也会越高。</li>
<li>SoftmaxLoss训练方式比SVMs训练得到的结果略好一点，因此无法证明SoftmaxLoss在效果上一定比svm强，但是简化了训练流程，无需分步骤训练模型。</li>
<li>proposal并不是提取的越多效果越好，太多proposal反而导致mAP下降。</li>
</ol>
<h2 id="4-Faster-RCNN"><a href="#4-Faster-RCNN" class="headerlink" title="4. Faster RCNN"></a>4. Faster RCNN</h2><blockquote>
<p>论文出处：<a href="http://arxiv.org/abs/1506.01497" target="_blank" rel="external">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a><br>论文作者：<a href="https://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1" target="_blank" rel="external">Shaoqing Ren</a>, <a href="https://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1" target="_blank" rel="external">Kaiming He</a>, <a href="https://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1" target="_blank" rel="external">Ross Girshick</a>, <a href="https://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1" target="_blank" rel="external">Jian Sun</a></p>
</blockquote>
<p>　　在之前介绍的Fast-R-CNN中，第一步需要先使用Selective Search方法提取图像中的proposals。基于CPU实现的Selective Search提取一幅图像的所有Proposals需要约2s的时间。在不计入proposal提取情况下，Fast-R-CNN基本可以实时进行目标检测。但是，如果从端到端的角度考虑，显然proposal提取成为影响端到端算法性能的瓶颈。目前最新的EdgeBoxes算法虽然在一定程度提高了候选框提取的准确率和效率，但是处理一幅图像仍然需要0.2s。因此，Ren Shaoqing提出新的Faster-R-CNN算法，该算法引入了RPN网络（Region Proposal Network）提取proposals。RPN网络是一个全卷积神经网络，通过共享卷积层特征可以实现proposal的提取，RPN提取一幅像的proposal只需要10ms.</p>
<p>　　Faster-R-CNN算法由两大模块组成：<code>1.PRN候选框提取模块</code>，<code>2.Fast R-CNN检测模块</code>。其中，RPN是全卷积神经网络，用于提取候选框；Fast R-CNN基于RPN提取的proposal检测并识别proposal中的目标。</p>
<p><img src="Faster_RCNN_1.png" alt=""></p>
<h3 id="4-1-Region-Proposal-Network-RPN"><a href="#4-1-Region-Proposal-Network-RPN" class="headerlink" title="4.1 Region Proposal Network (RPN)"></a>4.1 Region Proposal Network (RPN)</h3><p>　　RPN网络的输入可以是任意大小（但还是有最小分辨率要求的，例如VGG是228*228）的图片。如果用VGG16进行特征提取，那么RPN网络的组成形式可以表示为VGG16+RPN。</p>
<p><code>VGG16</code> ：参考<a href="https://github.com/rbgirshick/py-faster-rcnn/blob/master/models/pascal_voc/VGG16/faster_rcnn_end2end/train.prototxt" target="_blank" rel="external">这个</a>，可以看出VGG16中用于特征提取的部分是13个卷积层（conv1_1—-&gt;conv5.3），不包括pool5及pool5后的网络层次结构。</p>
<p><code>RPN</code> ：RPN是作者重点介绍的一种网络，如下图所示。RPN的实现方式：在conv5-3的卷积feature map上用一个n*n的滑窗（论文中作者选用了n=3，即3*3的滑窗）生成一个长度为256（对应于ZF网络）或512（对应于VGG网络）维长度的全连接特征。然后在这个256维或512维的特征后产生两个分支的全连接层：1.reg-layer,用于预测proposal的中心锚点对应的proposal的坐标x，y和宽高w，h；2.cls-layer，用于判定该proposal是前景还是背景。sliding window的处理方式保证reg-layer和cls-layer关联了conv5-3的全部特征空间。事实上，作者用全连接层实现方式介绍RPN层实现容易帮助我们理解这一过程，但在实现时作者选用了卷积层实现全连接层的功能。个人理解：全连接层本来就是特殊的卷积层，如果产生256或512维的fc特征，事实上可以用Num_out=256或512, kernel_size=3*3, stride=1的卷积层实现conv5-3到第一个全连接特征的映射。然后再用两个Num_out分别为2*9=18和4*9=36，kernel_size=1*1，stride=1的卷积层实现上一层特征到两个分支cls层和reg层的特征映射。注意：这里2*9中的2指cls层的分类结果包括前后背景两类，4*9的4表示一个Proposal的中心点坐标x，y和宽高w，h四个参数。采用卷积的方式实现全连接处理并不会减少参数的数量，但是使得输入图像的尺寸可以更加灵活。在RPN网络中，我们需要重点理解其中的anchors概念，Loss fucntions计算方式和RPN层训练数据生成的具体细节。</p>
<p><img src="Faster_RCNN_2.png" alt=""></p>
<p><code>Anchors</code> ：字面上可以理解为锚点，位于之前提到的n*n的sliding window的中心处。对于一个sliding window,我们可以同时预测多个proposal，假定有k个。k个proposal即k个reference boxes，每一个reference box又可以用一个scale，一个aspect_ratio和sliding window中的锚点唯一确定。所以，我们在后面说一个anchor,你就理解成一个anchor box 或一个reference box.作者在论文中定义k=9，即3种scales和3种aspect_ratio确定出当前sliding window位置处对应的9个reference boxes， 4*k个reg-layer的输出和2*k个cls-layer的score输出。对于一幅W*H的feature map,对应W*H*k个锚点。所有的锚点都具有尺度不变性。</p>
<p>anchor box的设置应比较好的覆盖到不同大小区域，如下图:</p>
<p><img src="Faster_RCNN_3.png" alt=""></p>
<p>一张1000×600的图片，大概可以得到20k个anchor box(60×40×9)。</p>
<p><code>Loss functions</code> ：在计算Loss值之前，作者设置了anchors的标定方法。正样本标定规则：1.如果Anchor对应的reference box与ground truth的IoU值最大，标记为正样本；2.如果Anchor对应的reference box与ground truth的IoU&gt;0.7，标记为正样本。事实上，采用第2个规则基本上可以找到足够的正样本，但是对于一些极端情况，例如所有的Anchor对应的reference box与groud truth的IoU不大于0.7,可以采用第一种规则生成。负样本标定规则：如果Anchor对应的reference box与ground truth的IoU&lt;0.3，标记为负样本。剩下的既不是正样本也不是负样本，不用于最终训练。训练RPN的Loss是有classification loss （即softmax loss）和regression loss （即L1 loss）按一定比重组成的。计算softmax loss需要的是anchors对应的groundtruth标定结果和预测结果，计算regression loss需要三组信息：1.预测框，即RPN网络预测出的proposal的中心位置坐标x,y和宽高w,h；2.锚点reference box:之前的9个锚点对应9个不同scale和aspect_ratio的reference boxes，每一个reference boxes都有一个中心点位置坐标x_a,y_a和宽高w_a,h_a。3.ground truth:标定的框也对应一个中心点位置坐标x*,y*和宽高w*,h*。因此计算regression loss和总Loss方式如下：</p>
<p><img src="Faster_RCNN_4.png" alt=""><br><img src="Faster_RCNN_5.png" alt=""></p>
<p><code>RPN训练设置</code> ：在训练RPN时，一个Mini-batch是由一幅图像中任意选取的256个proposal组成的，其中正负样本的比例为1：1。如果正样本不足128，则多用一些负样本以满足有256个Proposal可以用于训练，反之亦然。训练RPN时，与VGG共有的层参数可以直接拷贝经ImageNet训练得到的模型中的参数；剩下没有的层参数用标准差=0.01的高斯分布初始化。</p>
<h3 id="4-2-RPN与Faster-R-CNN特征共享"><a href="#4-2-RPN与Faster-R-CNN特征共享" class="headerlink" title="4.2 RPN与Faster-R-CNN特征共享"></a>4.2 RPN与Faster-R-CNN特征共享</h3><p>　　RPN在提取得到proposals后，作者选择使用Fast-R-CNN实现最终目标的检测和识别。RPN和Fast-R-CNN共用了13个VGG的卷积层，显然将这两个网络完全孤立训练不是明智的选择，作者采用交替训练阶段卷积层特征共享：</p>
<p><code>交替训练（Alternating training）</code> ：Step1:训练RPN;Step2:用RPN提取得到的proposal训练Fast R-CNN;Step3:用Faster R-CNN初始化RPN网络中共用的卷积层。迭代执行Step1,2,3，直到训练结束为止。论文中采用的就是这种训练方式，注意：第一次迭代时，用ImageNet得到的模型初始化RPN和Fast-R-CNN中卷积层的参数；从第二次迭代开始，训练RPN时，用Fast-R-CNN的共享卷积层参数初始化RPN中的共享卷积层参数，然后只Fine-tune不共享的卷积层和其他层的相应参数。训练Fast-RCNN时，保持其与RPN共享的卷积层参数不变，只Fine-tune不共享的层对应的参数。这样就可以实现两个网络卷积层特征共享训练。相应的网络模型请参考<a href="https://github.com/rbgirshick/py-faster-rcnn/tree/master/models/pascal_voc/VGG16/faster_rcnn_alt_opt" target="_blank" rel="external">这里</a>。</p>
<h3 id="4-3-深度挖掘"><a href="#4-3-深度挖掘" class="headerlink" title="4.3 深度挖掘"></a>4.3 深度挖掘</h3><ol>
<li>由于Selective Search提取得到的Proposal尺度不一，因此Fast-RCNN或SPP-Net生成的RoI也是尺度不一，最后分别用RoI Pooling Layer或SPP-Layer处理得到固定尺寸金字塔特征，在这一过程中，回归最终proposal的坐标网络的权重事实上共享了整个FeatureMap，因此其训练的网络精度也会更高。但是，RPN方式提取的ROI由k个锚点生成，具有k种不同分辨率，因此在训练过程中学习到了k种独立的回归方式。这种方式并没有共享整个FeatureMap，但其训练得到的网络精度也很高。这，我竟然无言以对。有什么问题，请找Anchors同学。</li>
<li>采用不同分辨率图像在一定程度可以提高准确率，但是也会导致训练速度下降。采用VGG16训练RPN虽然使得第13个卷积层特征尺寸至少缩小到原图尺寸的1/16（事实上，考虑到kernel_size作用，会更小一些），然并卵，最终的检测和识别效果仍然好到令我无言以对。</li>
<li>三种scale(128*128，256*256，512*512),三种宽高比（1：2，1：1，2：1）,虽然scale区间很大，总感觉这样会很奇怪，但最终结果依然表现的很出色。</li>
<li>训练时（例如600*1000的输入图像），如果reference box （即anchor box）的边界超过了图像边界，这样的anchors对训练Loss不产生影响，即忽略掉这样的Loss.一幅600*1000的图经过VGG16大约为40*60，那么anchors的数量大约为40*60*9，约等于20000个anchor boxes.去除掉与图像边界相交的anchor boxes后，剩下约6000个anchor boxes,这么多数量的anchor boxes之间会有很多重叠区域，因此使用非极值抑制方法将IoU&gt;0.7的区域全部合并，剩下2000个anchor boxes（同理，在最终检测端，可以设置规则将概率大于某阈值P且IoU大于某阈值T的预测框（注意，和前面不同，不是anchor boxes）采用非极大抑制方法合并）。在每一个epoch训练过程中，随机从一幅图最终剩余的这些anchors采样256个anchor box作为一个Mini-batch训练RPN网络。</li>
</ol>
<h3 id="4-4-实验"><a href="#4-4-实验" class="headerlink" title="4.4 实验"></a>4.4 实验</h3><p>1.PASCAL VOC 2007：使用ZF-Net训练RPN和Fast-R-CNN,那么SelectiveSearch+Fast-R-CNN, EdgeBox+Fast-R-CNN, RPN+Fast-R-CNN的准确率分别为：58.7%，58.6%，59.9%. SeletiveSeach和EdgeBox方法提取2000个proposal，RPN最多提取300个proposal,因此卷积特征共享方式提取特征的RPN显然在效率是更具有优势。<br>2.采用VGG以特征不共享方式和特征共享方式训练RPN+Fast-R-CNN,可以分别得到68.5%和69.9%的准确率（VOC2007）。此外，采用VGG训练RCNN时，需要花320ms提取2000个proposal，加入SVD优化后需要223ms，而Faster-RCNN整个前向过程（包括RPN+Fast-R-CNN）总共只要198ms.<br>3.Anchors的scales和aspect_ratio的数量虽然不会对结果产生明显影响，但是为了算法稳定性，建议两个参数都设置为合适的数值。<br>4.当Selective Search和EdgeBox提取的proposal数目由2000减少到300时，Faste-R-CNN的Recall vs. IoU overlap ratio图中recall值会明显下降；但RPN提取的proposal数目由2000减少到300时，Recall vs. IoU overlap ratio图中recall值会比较稳定。</p>
<h3 id="4-5-总结"><a href="#4-5-总结" class="headerlink" title="4.5 总结"></a>4.5 总结</h3><p>　　特征共享方式训练RPN+Fast-R-CNN能够实现极佳的检测效果，特征共享训练实现了买一送一，RPN在提取Proposal时不仅没有时间成本，还提高了proposal质量。因此Faster-R-CNN中交替训练RPN+Fast-R-CNN方式比原来的SlectiveSeach+Fast-R-CNN更上一层楼。</p>
<h2 id="5-R-FCN"><a href="#5-R-FCN" class="headerlink" title="5. R-FCN"></a>5. R-FCN</h2><blockquote>
<p>论文出处：<a href="http://arxiv.org/pdf/1605.06409v1.pdf" target="_blank" rel="external">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a><br>论文作者：<a href="https://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1" target="_blank" rel="external">Jifeng Dai</a>, <a href="https://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1" target="_blank" rel="external">Yi Li</a>, <a href="https://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1" target="_blank" rel="external">Kaiming He</a>, <a href="https://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1" target="_blank" rel="external">Jian Sun</a><br>论文翻译：<a href="http://www.jianshu.com/p/db1b74770e52" target="_blank" rel="external">[译] 基于R-FCN的物体检测 (zhwhong)</a></p>
</blockquote>
<p>　　RCNN系列(RCNN、Fast RCNN、Faster RCNN)中，网络由两个子CNN构成。在图片分类中，只需一个CNN，效率非常高。所以物体检测是不是也可以只用一个CNN？ </p>
<p>　　图片分类需要兼容形变，而物体检测需要利用形变，如何平衡？ </p>
<p>　　R-FCN利用在CNN的最后进行位置相关的特征pooling来解决以上两个问题。</p>
<p><img src="RFCN_1.png" alt=""></p>
<p>经普通CNN后，做有 k^2(C+1) 个 channel 的卷积，生成位置相关的特征(position-sensitive score maps)。</p>
<p>C 表示分类数，加 1 表示背景，k 表示后续要pooling 的大小，所以生成 k^2 倍的channel，以应对后面的空间pooling。</p>
<p><img src="RFCN_2.png" alt=""></p>
<p>　　普通CNN后，还有一个RPN(Region Proposal Network)，生成候选框。</p>
<p>　　假设一个候选框大小为 w×h，将它投影在位置相关的特征上，并采用average-pooling的方式生成一个 k×k×k^2(C+1) 的块(与Fast RCNN一样)，再采用空间相关的pooling(k×k平面上每一个点取channel上对应的部分数据)，生成 k×k×(C+1)的块，最后再做average-pooling生成 C+1 的块，最后做softmax生成分类概率。</p>
<p>　　类似的，RPN也可以采用空间pooling的结构，生成一个channel为 4k^2的特征层。空间pooling的具体操作可以参考下面：</p>
<p><img src="RFCN_3.png" alt=""></p>
<p>　　训练与SSD相似，训练时拿来做lost计算的点取一个常数，如128。 除去正点，剩下的所有使用概率最高的负点。</p>
<h2 id="6-YOLO"><a href="#6-YOLO" class="headerlink" title="6. YOLO"></a>6. YOLO</h2><blockquote>
<p>论文出处：<a href="http://arxiv.org/abs/1506.02640" target="_blank" rel="external">You Only Look Once: Unified, Real-Time Object Detection</a><br>论文作者：<a href="https://arxiv.org/find/cs/1/au:+Redmon_J/0/1/0/all/0/1" target="_blank" rel="external">Joseph Redmon</a>, <a href="https://arxiv.org/find/cs/1/au:+Divvala_S/0/1/0/all/0/1" target="_blank" rel="external">Santosh Divvala</a>, <a href="https://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1" target="_blank" rel="external">Ross Girshick</a>, <a href="https://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1" target="_blank" rel="external">Ali Farhadi</a><br>项目主页：<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="external">https://pjreddie.com/darknet/yolo/</a></p>
</blockquote>
<p>　　Faster RCNN需要对20k个anchor box进行判断是否是物体，然后再进行物体识别，分成了两步。 YOLO则把物体框的选择与识别进行了结合，一步输出，即变成”You Only Look Once”。</p>
<p>　　YOLO是一个可以一次性预测多个Box位置和类别的卷积神经网络，能够实现端到端的目标检测和识别，其最大的优势就是速度快。事实上，目标检测的本质就是回归，因此一个实现回归功能的CNN并不需要复杂的设计过程。YOLO没有选择滑窗或提取proposal的方式训练网络，而是直接选用整图训练模型。这样做的好处在于可以更好的区分目标和背景区域，相比之下，采用proposal训练方式的Fast-R-CNN常常把背景区域误检为特定目标。当然,YOLO在提升检测速度的同时牺牲了一些精度。下图所示是YOLO检测系统流程：1.将图像Resize到448*448；2.运行CNN；3.非极大抑制优化检测结果。有兴趣的童鞋可以按照<a href="http://pjreddie.com/darknet/install/" target="_blank" rel="external">这个</a>说明安装测试一下YOLO的scoring流程，非常容易上手。接下来将重点介绍YOLO的原理。</p>
<p><img src="YOLO_1.png" alt=""></p>
<h3 id="6-1-一体化检测方案"><a href="#6-1-一体化检测方案" class="headerlink" title="6.1 一体化检测方案"></a>6.1 一体化检测方案</h3><p>　　YOLO的设计理念遵循端到端训练和实时检测。YOLO将输入图像划分为S*S个网络，如果一个物体的中心落在某网格(cell)内，则相应网格负责检测该物体。在训练和测试时，每个网络预测B个bounding boxes，每个bounding box对应5个预测参数，即bounding box的中心点坐标(x,y)，宽高（w,h），和置信度评分。这里的置信度评分(Pr(Object)*IOU(pred|truth))综合反映基于当前模型bounding box内存在目标的可能性Pr(Object)和bounding box预测目标位置的准确性IOU(pred|truth)。如果bouding box内不存在物体，则Pr(Object)=0。如果存在物体，则根据预测的bounding box和真实的bounding box计算IOU，同时会预测存在物体的情况下该物体属于某一类的后验概率Pr(Class_i|Object)。假定一共有C类物体，那么每一个网格只预测一次C类物体的条件类概率Pr(Class_i|Object), i=1,2,…,C;每一个网格预测B个bounding box的位置。即这B个bounding box共享一套条件类概率Pr(Class_i|Object), i=1,2,…,C。基于计算得到的Pr(Class_i|Object)，在测试时可以计算某个bounding box类相关置信度：Pr(Class_i|Object)*Pr(Object)*IOU(pred|truth)=Pr(Class_i)*IOU(pred|truth)。如果将输入图像划分为7*7网格（S=7），每个网格预测2个bounding box (B=2)，有20类待检测的目标（C=20），则相当于最终预测一个长度为S*S*(B*5+C)=7*7*30的向量，从而完成检测+识别任务，整个流程可以通过下图理解。</p>
<p><img src="YOLO_2.png" alt=""></p>
<ul>
<li>把缩放成统一大小的图片分割成S×S的单元格</li>
<li>每个单元格输出B个矩形框(冗余设计)，包含框的位置信息(x, y, w, h)与物体的Confidence</li>
<li>每个单元格再输出C个类别的条件概率P(Class∣Object)</li>
<li>最终输出层应有S×S×(B∗5+C)个单元</li>
<li>x, y 是每个单元格的相对位置</li>
<li>w, h 是整图的相对大小</li>
</ul>
<p>Conficence定义如下:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-772c5abc28591971.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><strong>(1) 网络设计</strong></p>
<p>　　YOLO网络设计遵循了GoogleNet的思想，但与之有所区别。YOLO使用了24个级联的卷积（conv）层和2个全连接（fc）层，其中conv层包括3*3和1*1两种Kernel，最后一个fc层即YOLO网络的输出，长度为S*S*(B*5+C)=7*7*30.此外，作者还设计了一个简化版的YOLO-small网络，包括9个级联的conv层和2个fc层，由于conv层的数量少了很多，因此YOLO-small速度比YOLO快很多。如下图所示我们给出了YOLO网络的架构。</p>
<p><img src="YOLO_3.png" alt=""></p>
<p><strong>(2) 训练</strong></p>
<p>　　作者训练YOLO网络是分步骤进行的：首先，作者从上图网络中取出前20个conv层，然后自己添加了一个average pooling层和一个fc层，用1000类的ImageNet数据与训练。在ImageNet2012上用224*224d的图像训练后得到的top5准确率是88%。然后，作者在20个预训练好的conv层后添加了4个新的conv层和2个fc层，并采用随即参数初始化这些新添加的层，在fine-tune新层时，作者选用448*448图像训练。最后一个fc层可以预测物体属于不同类的概率和bounding box中心点坐标x,y和宽高w,h。Boundingbox的宽高是相对于图像宽高归一化后得到的，Bounding box的中心位置坐标是相对于某一个网格的位置坐标进行过归一化，因此x,y,w,h均介于0到1之间。</p>
<p>代价函数如下：</p>
<p><img src="YOLO_Loss.png" alt=""></p>
<p>　　在设计Loss函数时，有两个主要的问题：1.对于最后一层长度为7*7*30长度预测结果，计算预测loss通常会选用平方和误差。然而这种Loss函数的位置误差和分类误差是1：1的关系。2.整个图有7*7个网格，大多数网格实际不包含物体（当物体的中心位于网格内才算包含物体），如果只计算Pr(Class_i),很多网格的分类概率为0，网格loss呈现出稀疏矩阵的特性，使得Loss收敛效果变差，模型不稳定。为了解决上述问题，作者采用了一系列方案：</p>
<ol>
<li>增加bounding box坐标预测的loss权重，降低bounding box分类的loss权重。坐标预测和分类预测的权重分别是λcoord=5,λnoobj=0.5.</li>
<li>平方和误差对于大和小的bounding box的权重是相同的，作者为了降低不同大小bounding box宽高预测的方差，采用了平方根形式计算宽高预测loss，即sqrt(w)和sqrt(h)。</li>
</ol>
<p>　　训练Loss组成形式较为复杂，这里不作列举，如有兴趣可以参考作者原文慢慢理解体会。</p>
<p><strong>(3) 测试</strong></p>
<p>　　作者选用PASAL VOC图像测试训练得到的YOLO网络，每幅图会预测得到98个（7*7*2）个bouding box及相应的类概率。通常一个cell可以直接预测出一个物体对应的bounding box,但是对于某些尺寸较大或靠近图像边界的物体，需要多个网格预测的结果通过非极大抑制处理生成。虽然YOLO对于非极大抑制的依赖不及R-CNN和DPM，但非极大抑制确实可以将mAP提高2到3个点。</p>
<h3 id="6-2-方法对比"><a href="#6-2-方法对比" class="headerlink" title="6.2 方法对比"></a>6.2 方法对比</h3><p>作者将YOLO目标检测与识别方法与其他几种经典方案进行比较可知：</p>
<p><code>DPM(Deformable parts models)</code> : DPM是一种基于滑窗方式的目标检测方法，基本流程包括几个独立的环节：特征提取，区域划分，基于高分值区域预测bounding box。YOLO采用端到端的训练方式，将特征提取、候选框预测，非极大抑制及目标识别连接在一起，实现了更快更准的检测模型。</p>
<p><code>R-CNN</code> ：R-CNN方案分需要先用SeletiveSearch方法提取proposal,然后用CNN进行特征提取，最后用SVM训练分类器。如此方案，诚繁琐也！YOLO精髓思想与其类似，但是通过共享卷积特征的方式提取proposal和目标识别。另外，YOLO用网格对proposal进行空间约束，避免在一些区域重复提取Proposal，相较于SeletiveSearch提取2000个proposal进行R-CNN训练，YOLO只需要提取98个proposal，这样训练和测试速度怎能不快？</p>
<p><code>Fast-R-CNN、Faster-R-CNN、Fast-DPM</code> : Fast-R-CNN和Faster-R-CNN分别替换了SVMs训练和SelectiveSeach提取proposal的方式，在一定程度上加速了训练和测试速度，但其速度依然无法和YOLO相比。同理，将DPM优化在GPU上实现也无出YOLO之右。</p>
<h3 id="6-3-实验"><a href="#6-3-实验" class="headerlink" title="6.3 实验"></a>6.3 实验</h3><p><strong>(1) 实时检测识别系统对比</strong></p>
<p><img src="YOLO_4.png" alt=""></p>
<p><strong>(2) VOC2007准确率比较</strong></p>
<p><img src="YOLO_5.png" alt=""></p>
<p><strong>(3) Fast-R-CNN和YOLO错误分析</strong></p>
<p><img src="YOLO_6.png" alt=""></p>
<p>如图所示，不同区域分别表示不同的指标：</p>
<ul>
<li>Correct：正确检测和识别的比例，即分类正确且IOU&gt;0.5</li>
<li>Localization:分类正确，但0.1&lt;IOU&lt;0.5</li>
<li>Similar:类别相似，IOU&gt;0.1</li>
<li>Other:分类错误，IOU&gt;0.1</li>
<li>Background: 对于任何目标IOU&lt;0.1</li>
</ul>
<p>　　可以看出，YOLO在定位目标位置时准确度不及Fast-R-CNN。YOLO的error中，目标定位错误占据的比例最大，比Fast-R-CNN高出了10个点。但是，YOLO在定位识别背景时准确率更高，可以看出Fast-R-CNN假阳性很高（Background=13.6%，即认为某个框是目标，但是实际里面不含任何物体）。</p>
<p><strong>(4) VOC2012准确率比较</strong></p>
<p><img src="YOLO_7.png" alt=""></p>
<p>　　由于YOLO在目标检测和识别是处理背景部分优势更明显，因此作者设计了Fast-R-CNN+YOLO检测识别模式，即先用R-CNN提取得到一组bounding box，然后用YOLO处理图像也得到一组bounding box。对比这两组bounding box是否基本一致，如果一致就用YOLO计算得到的概率对目标分类，最终的bouding box的区域选取二者的相交区域。Fast-R-CNN的最高准确率可以达到71.8%,采用Fast-R-CNN+YOLO可以将准确率提升至75.0%。这种准确率的提升是基于YOLO在测试端出错的情况不同于Fast-R-CNN。虽然Fast-R-CNN_YOLO提升了准确率，但是相应的检测识别速度大大降低，因此导致其无法实时检测。</p>
<p>　　使用VOC2012测试不同算法的mean Average Precision，YOLO的mAP=57.9%，该数值与基于VGG16的RCNN检测算法准确率相当。对于不同大小图像的测试效果进行研究，作者发现：YOLO在检测小目标时准确率比R-CNN低大约8~10%，在检测大目标是准确率高于R-CNN。采用Fast-R-CNN+YOLO的方式准确率最高，比Fast-R-CNN的准确率高了2.3%。</p>
<h3 id="5-4-总结"><a href="#5-4-总结" class="headerlink" title="5.4 总结"></a>5.4 总结</h3><p>　　YOLO是一种支持端到端训练和测试的卷积神经网络，在保证一定准确率的前提下能图像中多目标的检测与识别。</p>
<h2 id="7-YOLO2"><a href="#7-YOLO2" class="headerlink" title="7. YOLO2"></a>7. YOLO2</h2><blockquote>
<p>论文出处：<a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="external">YOLO9000: Better, Faster, Stronger</a><br>论文作者：<a href="https://arxiv.org/find/cs/1/au:+Redmon_J/0/1/0/all/0/1" target="_blank" rel="external">Joseph Redmon</a>, <a href="https://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1" target="_blank" rel="external">Ali Farhadi</a><br>项目主页：<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="external">https://pjreddie.com/darknet/yolo/</a></p>
</blockquote>
<p>　　时隔一年，YOLO（You Only Look Once: Unified, Real-Time Object Detection）从v1版本进化到了v2版本，作者在darknet主页先行一步放出源代码，论文在我们等候之下终于在12月25日发布出来，本文对论文重要部分进行了翻译理解工作，不一定完全对，如有疑问，欢迎讨论。博主如果有新的理解，也会更新文章，或者新写一篇。</p>
<p>　　新的YOLO版本论文全名叫“YOLO9000: Better, Faster, Stronger”，主要有两个大方面的改进：</p>
<ul>
<li>第一，作者使用了一系列的方法对原来的YOLO多目标检测框架进行了改进，在保持原有速度的优势之下，精度上得以提升。VOC 2007数据集测试，67FPS下mAP达到76.8%，40FPS下mAP达到78.6%，基本上可以与Faster R-CNN和SSD一战。</li>
<li>第二，作者提出了一种目标分类与检测的联合训练方法，通过这种方法，YOLO9000可以同时在COCO和ImageNet数据集中进行训练，训练后的模型可以实现多达9000种物体的实时检测。</li>
</ul>
<p>更多内容请参考：</p>
<ul>
<li><a href="http://blog.csdn.net/jesse_mx/article/details/53925356" target="_blank" rel="external">YOLOv2 论文笔记</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25167153" target="_blank" rel="external">YOLO2 - 知乎专栏</a></li>
</ul>
<h2 id="8-SSD"><a href="#8-SSD" class="headerlink" title="8. SSD"></a>8. SSD</h2><blockquote>
<p>论文：<a href="http://www.cs.unc.edu/~wliu/papers/ssd.pdf" target="_blank" rel="external">SSD: Single Shot MultiBox Detector</a> </p>
</blockquote>
<p>YOLO在 7×7 的框架下识别物体，遇到大量小物体时，难以处理。<br>SSD则在不同层级的feature map下进行识别，能够覆盖更多范围。</p>
<p><img src="SSD_1.png" alt=""></p>
<p>假设在 m 层 feature map 上进行识别，则第 k 层的基本比例为</p>
<p><img src="SSD_2.png" alt=""></p>
<p>比如 s_min=0.2，s_max=0.95，表示整张图片识别物体所占比最小 0.2，最大 0.95。</p>
<p>在基本比例上，再取多个长宽比，令 a={1, 2, 3, 1/2, 1/3}，长宽分别为：</p>
<p><img src="SSD_3.png" alt=""></p>
<p>Match策略上，取ground truth与以上生成的格子重叠率大于0.5的。</p>
<h2 id="9-SSD-vs-YOLO"><a href="#9-SSD-vs-YOLO" class="headerlink" title="9. SSD vs YOLO"></a>9. SSD vs YOLO</h2><p><img src="SSD_VS_YOLO.png" alt=""></p>
<p>位置采用Smooth L1 Regression，分类采用Softmax。<br>代价函数为：</p>
<p><img src="http://latex.codecogs.com/png.latex?L%20=%20L_{conf}(x,%20c)%20+%20\alpha%20\cdot%20L_{loc}(c,%20l,%20g))" alt=""></p>
<p>x  表示类别输出，c 表示目标分类，l 表示位置输出，g 表示目标位置, α是比例常数，可取1。<br>训练过程中负点远多于正点，所以只取负点中，概率最大的几个，数量与正点成 3:1 。</p>
<h2 id="10-NMS"><a href="#10-NMS" class="headerlink" title="10. NMS"></a>10. NMS</h2><blockquote>
<p>论文出处：<a href="http://ieeexplore.ieee.org/abstract/document/1699659/" target="_blank" rel="external">Efficient Non-Maximum Suppression</a><br>发表于：<a href="http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=11159" target="_blank" rel="external">Pattern Recognition, 2006. ICPR 2006. 18th International Conference on</a></p>
</blockquote>
<p>以上方法，同一物体可能有多个预测值。<br>可用NMS(Non-maximum suppression，非极大值抑制)来去重。</p>
<p><img src="NMS.png" alt=""></p>
<p>如上图所示，一共有6个识别为人的框，每一个框有一个置信率。<br>现在需要消除多余的:</p>
<ul>
<li>按置信率排序: 0.95, 0.9, 0.9, 0.8, 0.7, 0.7</li>
<li>取最大0.95的框为一个物体框</li>
<li>剩余5个框中，去掉与0.95框重叠率大于0.6(可以另行设置)，则保留0.9, 0.8, 0.7三个框</li>
<li>重复上面的步骤，直到没有框了，0.9为一个框</li>
<li>选出来的为: 0.95, 0.9</li>
</ul>
<p>两个矩形的重叠率计算方式如下:</p>
<p><img src="NMS_2.png" alt="非最大抑制（NMS）"></p>
<p>补充阅读：</p>
<ul>
<li><a href="http://blog.csdn.net/shuzfan/article/details/52711706" target="_blank" rel="external">NMS——非极大值抑制</a></li>
<li><a href="http://blog.csdn.net/pb09013037/article/details/45621757" target="_blank" rel="external">非最大抑制（NMS）</a></li>
<li><a href="http://blog.163.com/yuyang_tech/blog/static/2160500832016026444987/" target="_blank" rel="external">非极大值抑制（Non-maximum suppression）</a></li>
<li><a href="http://blog.csdn.net/pb09013037/article/details/45477591" target="_blank" rel="external">非极大值抑制（Non-maximum suppression）在物体检测领域的应用</a></li>
</ul>
<h2 id="11-xywh-VS-xyxy"><a href="#11-xywh-VS-xyxy" class="headerlink" title="11. xywh VS xyxy"></a>11. xywh VS xyxy</h2><p>系列论文中，位置都用 (x,y,w,h)来表示，没有用左上角、右下角 (x,y,x,y) 来表示。<br>初衷是当 (w,h)正确时，(x,y) 一点错，会导致整个框就不准了。<br>在初步的实际实验中，(x,y,x,y) 效果要差一些。</p>
<p>背后的逻辑，物体位置用 (x,y,w,h) 来学习比较容易。<br>(x,y) 只需要位置相关的加权就能计算出来；<br>(w,h) 就更简单了，直接特征值相加即可。</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="http://www.cnblogs.com/venus024/p/5590044.html" target="_blank" rel="external">深度学习检测方法梳理</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/25600546" target="_blank" rel="external">RCNN-&gt; SPP net -&gt; Fast RCNN -&gt; Faster RCNN</a></li>
<li><a href="http://blog.csdn.net/j_study/article/details/51188704" target="_blank" rel="external">深度学习进行目标识别的资源列表</a></li>
<li><a href="http://blog.csdn.net/u012759136/article/details/52434826#t9" target="_blank" rel="external">图像语义分割之FCN和CRF</a></li>
<li><a href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html" target="_blank" rel="external">Object Detection</a></li>
<li><a href="http://www.cosmosshadow.com/ml/%E5%BA%94%E7%94%A8/2015/12/07/%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B.html" target="_blank" rel="external">Detection</a></li>
<li><a href="http://www.jianshu.com/p/067f6a989d31" target="_blank" rel="external">[Detection] CNN 之 “物体检测” 篇</a></li>
<li><a href="http://www.jianshu.com/p/db1b74770e52" target="_blank" rel="external">[译] 基于R-FCN的物体检测 (zhwhong)</a></li>
</ul>
<hr>
<p>(本文部分转载自：<a href="http://www.cnblogs.com/venus024/p/5590044.html" target="_blank" rel="external">深度学习检测方法梳理</a>，原作者venus024，但是额外补充了一些其他相关内容，仅供学习交流使用，不得用于商业途径，转载请联系作者并注明出处，谢谢。)</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Index&quot;&gt;&lt;a href=&quot;#Index&quot; class=&quot;headerlink&quot; title=&quot;Index&quot;&gt;&lt;/a&gt;Index&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1311.2524&quot;&gt;RCNN&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1406.4729&quot;&gt;SPP-Net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1504.08083&quot;&gt;Fast RCNN&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1506.01497&quot;&gt;Faster RCNN&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1605.06409v1.pdf&quot;&gt;R-FCN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1506.02640&quot;&gt;YOLO&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.08242&quot;&gt;YOLO2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.unc.edu/~wliu/papers/ssd.pdf&quot;&gt;SSD&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/abstract/document/1699659/&quot;&gt;NMS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://Hzwcode.github.io/tags/Deep-Learning/"/>
    
      <category term="Object Detection" scheme="https://Hzwcode.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[斯坦福CS231n课程整理] Convolutional Neural Networks for Visual Recognition(附翻译，作业)</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Convolutional-Neural-Networks-for-Visual-Recognition-CS231n/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Convolutional-Neural-Networks-for-Visual-Recognition-CS231n/</id>
    <published>2017-02-24T09:27:52.000Z</published>
    <updated>2017-03-24T13:25:11.791Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><blockquote>
<h1 id="CS231n课程：面向视觉识别的卷积神经网络"><a href="#CS231n课程：面向视觉识别的卷积神经网络" class="headerlink" title="CS231n课程：面向视觉识别的卷积神经网络"></a>CS231n课程：面向视觉识别的卷积神经网络</h1></blockquote>
<ul>
<li>课程官网：<a href="http://cs231n.stanford.edu/" target="_blank" rel="external">CS231n: Convolutional Neural Networks for Visual Recognition</a></li>
<li>Github：<a href="https://github.com/cs231n/cs231n.github.io" target="_blank" rel="external">https://github.com/cs231n/cs231n.github.io</a> | <a href="http://cs231n.github.io/" target="_blank" rel="external">http://cs231n.github.io/</a></li>
<li>教学安排及大纲：<a href="http://vision.stanford.edu/teaching/cs231n/syllabus.html" target="_blank" rel="external">Schedule and Syllabus</a></li>
<li>课程视频：Youtube上查看<a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw" target="_blank" rel="external">Andrej Karpathy</a>创建的<a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/playlist%3Flist%3DPLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC" target="_blank" rel="external">播放列表</a>，或者<a href="http://study.163.com/course/introduction/1003223001.htm#/courseDetail" target="_blank" rel="external">网易云课堂</a></li>
<li>课程pdf及视频下载：<a href="https://pan.baidu.com/s/1eRHH4L8" target="_blank" rel="external">百度网盘下载</a>，密码是4efx</li>
</ul>
<a id="more"></a>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-a0eeadfcd667b7bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<blockquote>
<h1 id="授课-Stanford-Vision-Lab"><a href="#授课-Stanford-Vision-Lab" class="headerlink" title="授课 (Stanford Vision Lab)"></a>授课 (<a href="http://vision.stanford.edu/index.html" target="_blank" rel="external">Stanford Vision Lab</a>)</h1></blockquote>
<ul>
<li><a href="http://vision.stanford.edu/feifeili/" target="_blank" rel="external">Fei-Fei Li</a> (Associate Professor, Stanford University)</li>
<li><a href="http://cs.stanford.edu/people/karpathy/" target="_blank" rel="external">Andrej Karpathy</a> | <a href="https://github.com/karpathy" target="_blank" rel="external">Github</a> | <a href="http://karpathy.github.io/" target="_blank" rel="external">Blog</a> | <a href="https://twitter.com/karpathy" target="_blank" rel="external">Twitter</a></li>
<li><a href="http://cs.stanford.edu/people/jcjohns/" target="_blank" rel="external">Justin Johnson</a> | <a href="https://github.com/jcjohnson" target="_blank" rel="external">Github</a></li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-df9eb2f6ea9512fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Course Instructors and Teaching Assistants"></p>
<blockquote>
<h1 id="课程原文-amp-作业-amp-中文翻译笔记"><a href="#课程原文-amp-作业-amp-中文翻译笔记" class="headerlink" title="课程原文 &amp; 作业 &amp; 中文翻译笔记"></a>课程原文 &amp; 作业 &amp; 中文翻译笔记</h1><ul>
<li>知乎专栏：<a href="https://zhuanlan.zhihu.com/intelligentunit" target="_blank" rel="external"><strong>智能单元</strong></a></li>
<li>作者：<a href="https://www.zhihu.com/people/du-ke/answers" target="_blank" rel="external"><strong>杜客</strong></a> (在此对作者表示特别感谢！)</li>
</ul>
</blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-3b415a85af702e04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="翻译得到Karpathy许可"></p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit" target="_blank" rel="external">贺完结！CS231n官方笔记授权翻译总集篇发布</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20870307?refer=intelligentunit" target="_blank" rel="external">获得授权翻译斯坦福CS231n课程笔记系列</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：Python Numpy教程</a> | <a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：图像分类笔记（上）</a> | <a href="http://cs231n.github.io/classification/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20900216?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：图像分类笔记（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：线性分类笔记（上）</a> | <a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：线性分类笔记（中）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：线性分类笔记（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21354230?refer=intelligentunit" target="_blank" rel="external">知友智靖远关于CS231n课程字幕翻译的倡议 </a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：最优化笔记（上）</a> | <a href="http://cs231n.github.io/optimization-1/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：最优化笔记（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：反向传播笔记 </a> | <a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21441838?refer=intelligentunit" target="_blank" rel="external">斯坦福CS231n课程作业 # 1 简介 </a> | <a href="http://cs231n.github.io/assignments2016/assignment1/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 1（上）</a> | <a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 1（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 2 </a> | <a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 3（上）</a> | <a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 3（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21941485?refer=intelligentunit" target="_blank" rel="external">斯坦福CS231n课程作业 # 2 简介 </a> | <a href="http://cs231n.github.io/assignments2016/assignment2/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：卷积神经网络笔记 </a> | <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" target="_blank" rel="external">斯坦福CS231n课程作业 # 3 简介 </a> | <a href="http://cs231n.github.io/assignments2016/assignment3/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22282421?refer=intelligentunit" target="_blank" rel="external">Andrej Karpathy的回信和Quora活动邀请</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22232836?refer=intelligentunit" target="_blank" rel="external">知行合一码作业，深度学习真入门 </a></li>
</ul>
<p><strong>【附录 - Assignment】：</strong></p>
<ul>
<li>[简书] <a href="http://www.jianshu.com/p/004c99623104" target="_blank" rel="external">CS231n (winter 2016) : Assignment1</a></li>
<li>[简书] <a href="http://www.jianshu.com/p/9c4396653324" target="_blank" rel="external">CS231n (winter 2016) : Assignment2</a></li>
<li>[简书] <a href="http://www.jianshu.com/p/e46b1aa48886" target="_blank" rel="external">CS231n (winter 2016) : Assignment3（更新中）</a></li>
<li>[Github] CS231n作业<a href="https://github.com/MyHumbleSelf/cs231n" target="_blank" rel="external"> 参考1</a> | <a href="https://github.com/dengfy/cs231n" target="_blank" rel="external">参考2</a> ……</li>
</ul>
<hr>
<p>(再次感谢<a href="https://zhuanlan.zhihu.com/intelligentunit" target="_blank" rel="external">智能单元-知乎专栏</a>，以及知乎作者<a href="https://www.zhihu.com/people/du-ke/answers" target="_blank" rel="external">@杜客</a>和相关朋友<a href="https://www.zhihu.com/people/584f06e4ed2edc6007e4793179e7cdc1" target="_blank" rel="external">@ShiqingFan</a>，<a href="https://www.zhihu.com/people/hmonkey" target="_blank" rel="external">@猴子</a>，<a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" target="_blank" rel="external">@堃堃</a>，<a href="https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481" target="_blank" rel="external">@李艺颖</a>等为CS231n课程翻译工作做出的贡献，辛苦了！)</p>
<hr>
<p><strong>其他课程整理：</strong></p>
<ul>
<li><a href="http://www.jianshu.com/p/062d2bbbef93" target="_blank" rel="external">[斯坦福CS224d课程整理] Natural Language Processing with Deep Learning</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/0a6ef31ff77a" target="_blank" rel="external">[斯坦福CS229课程整理] Machine Learning Autumn 2016</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/c68d0df13e0b" target="_blank" rel="external">[coursera 机器学习课程] Machine Learning by Andrew Ng</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h1 id=&quot;CS231n课程：面向视觉识别的卷积神经网络&quot;&gt;&lt;a href=&quot;#CS231n课程：面向视觉识别的卷积神经网络&quot; class=&quot;headerlink&quot; title=&quot;CS231n课程：面向视觉识别的卷积神经网络&quot;&gt;&lt;/a&gt;CS231n课程：面向视觉识别的卷积神经网络&lt;/h1&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;课程官网：&lt;a href=&quot;http://cs231n.stanford.edu/&quot;&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github：&lt;a href=&quot;https://github.com/cs231n/cs231n.github.io&quot;&gt;https://github.com/cs231n/cs231n.github.io&lt;/a&gt; | &lt;a href=&quot;http://cs231n.github.io/&quot;&gt;http://cs231n.github.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;教学安排及大纲：&lt;a href=&quot;http://vision.stanford.edu/teaching/cs231n/syllabus.html&quot;&gt;Schedule and Syllabus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;课程视频：Youtube上查看&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw&quot;&gt;Andrej Karpathy&lt;/a&gt;创建的&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.youtube.com/playlist%3Flist%3DPLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&quot;&gt;播放列表&lt;/a&gt;，或者&lt;a href=&quot;http://study.163.com/course/introduction/1003223001.htm#/courseDetail&quot;&gt;网易云课堂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;课程pdf及视频下载：&lt;a href=&quot;https://pan.baidu.com/s/1eRHH4L8&quot;&gt;百度网盘下载&lt;/a&gt;，密码是4efx&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://Hzwcode.github.io/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="https://Hzwcode.github.io/tags/Computer-Vision/"/>
    
      <category term="CNN" scheme="https://Hzwcode.github.io/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>[Linux] Ubuntu下超好看扁平主题 : Flatabulous</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Linux-Ubuntu-Flatabulous/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Linux-Ubuntu-Flatabulous/</id>
    <published>2017-02-24T08:12:16.000Z</published>
    <updated>2017-03-24T16:24:30.647Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><img src="http://upload-images.jianshu.io/upload_images/145616-909d61913233d890.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Flatabulous"></p>
<a id="more"></a>
<p>使用ubuntu的小伙伴们，不知道你们对ubuntu自带主题有什么看法，反正我个人不太喜欢，个人比较喜欢扁平化的风格。<br>下面给大家推荐一个我长期使用的扁平化风格的主题－<a href="https://github.com/anmoljagetia/Flatabulous" target="_blank" rel="external">Flatabulous</a> 。<br>先看一下我的桌面(个人比较偏向单色调，不要在意这些细节啦)：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-1564d71f915f7cef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="My Desktop"></p>
<p>那么<strong>Flatabulous</strong>到底是什么呢？<br>　　“This is a Flat theme for Ubuntu and other debian based Linux Systems. This is based on the Ultra-Flat theme. Special thanks to @steftrikia and Satyajit Sahoo for the original work.”<br>哈哈，不卖关子了，它其实就是一个超级好看的扁平化Ubuntu主题。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-e639fe182b0b743b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>下面就开始说说怎么安装它吧~</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="[ 安装 ]"></a>[ 安装 ]</h2><h3 id="Step-1-安装-Unity-Tweak-Tool"><a href="#Step-1-安装-Unity-Tweak-Tool" class="headerlink" title="Step 1　安装 Unity Tweak Tool"></a>Step 1　安装 Unity Tweak Tool</h3><p>要安装这个主题，首先要安装<a href="https://launchpad.net/unity-tweak-tool" target="_blank" rel="external">Unity Tweak Tool</a>或者<a href="https://github.com/tualatrix/ubuntu-tweak" target="_blank" rel="external">Ubuntu Tweak Tool</a>。</p>
<p>安装Unity Tweak Tool可以很简单地执行如下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install unity-tweak-tool</div></pre></td></tr></table></figure>
<p>安装Ubuntu Tweak Tool可以使用如下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo add-apt-repository ppa:tualatrix/ppa  </div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install ubuntu-tweak</div></pre></td></tr></table></figure>
<p>或者跑到它们的网站下载.deb文件(推荐)，打开Ubuntu软件中心安装或者使用命令<code>dpkg -i</code>(推荐)安装。</p>
<p>注：If you are on Ubuntu 16.04 or higher, run the commands below to install Ubuntu Tweak:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ wget -q -O - http://archive.getdeb.net/getdeb-archive.key | sudo apt-key add -</div><div class="line">$ sudo sh -c <span class="string">'echo "deb http://archive.getdeb.net/ubuntu xenial-getdeb apps" &gt;&gt; /etc/apt/sources.list.d/getdeb.list'</span></div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install ubuntu-tweak</div></pre></td></tr></table></figure>
<p>安装完毕后，我们可以就搜到Ubuntu Tweak这款软件了，如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-c073230df8a73b8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h3 id="Step-2-安装Flatabulous主题"><a href="#Step-2-安装Flatabulous主题" class="headerlink" title="Step 2　安装Flatabulous主题"></a>Step 2　安装Flatabulous主题</h3><h4 id="方式1：Using-the-deb-file-for-Debian-Ubuntu-and-derivatives-Recommended"><a href="#方式1：Using-the-deb-file-for-Debian-Ubuntu-and-derivatives-Recommended" class="headerlink" title="方式1：Using the .deb file for Debian, Ubuntu and derivatives (Recommended)"></a>方式1：Using the .deb file for Debian, Ubuntu and derivatives (Recommended)</h4><p>下载.deb文件，点击<a href="https://github.com/anmoljagetia/Flatabulous/releases" target="_blank" rel="external">这里</a>，下载后，打开Ubuntu软件中心或者使用命令<code>dpkg -i</code>（推荐）安装。</p>
<h4 id="方式2：Using-the-noobslab-PPA"><a href="#方式2：Using-the-noobslab-PPA" class="headerlink" title="方式2：Using the noobslab PPA"></a>方式2：Using the noobslab PPA</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo add-apt-repository ppa:noobslab/themes</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install flatabulous-theme</div></pre></td></tr></table></figure>
<h4 id="方式3：下载Flatabulous源码"><a href="#方式3：下载Flatabulous源码" class="headerlink" title="方式3：下载Flatabulous源码"></a>方式3：下载Flatabulous源码</h4><p>下载主题源码，点击<a href="https://github.com/anmoljagetia/Flatabulous/archive/master.zip" target="_blank" rel="external">这里</a>，或者使用git克隆下来，Github仓库地址： <a href="https://github.com/anmoljagetia/Flatabulous" target="_blank" rel="external">https://github.com/anmoljagetia/Flatabulous</a><br>如果下载的是zip文件，先将其解压，然后移动到/usr/share/themes/下。如果是git clone下来的，直接执行下如下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo mv Flatabulous /usr/share/themes/</div></pre></td></tr></table></figure>
<h3 id="Step-3-Tweak配置"><a href="#Step-3-Tweak配置" class="headerlink" title="Step 3　Tweak配置"></a>Step 3　Tweak配置</h3><p>我们打开Ubuntu Tweak，选择<strong>调整-&gt;主题</strong>，如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-c937f438f034d8bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>然后，配置GTK主题和窗口主题，选择Flatabulous，如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-1e90b53fc9d14f68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>你们可以模仿我的配置，不过此时还有一个问题，就是你发现图标主题没有<code>Ultra-Flat</code>选项，这个<code>icon</code>需要额外下载，原生的<code>Tweak</code>里面并没有。</p>
<p>对于图标，我使用的是ultra-flat-icons主题。有蓝色（推荐），橙色和薄荷绿颜色可用。要安装它，你可以运行下面的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo add-apt-repository ppa:noobslab/icons</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install ultra-flat-icons</div></pre></td></tr></table></figure>
<p>或者你也可以运行<code>sudo apt-get install ultra-flat-icons-orange</code>或者 <code>sudo apt-get install ultra-flat-icons-green</code>。<br>根据你自己喜欢的颜色选择，我推荐的是扁平图标，但是你也可以看看<strong>Numix</strong>和<strong>Flattr</strong>。</p>
<p>图标安装好后，再打开Ubuntu Tweak，选择 <code>调整-&gt;主题</code>，选择图标主题为<code>Ultra-Flat</code>。</p>
<p>安装完以后，只需要在theme进行相应的配置，然后换一个自己喜欢的桌面壁纸，我们就能看到超级好看的ubuntu啦。如果不行，重启计算机，应该就可以了。重启之后你的计算机看起来差不多是这样的：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-a9408b3132214304.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="扁平化图标"></p>
<h2 id="部分效果图截图"><a href="#部分效果图截图" class="headerlink" title="[ 部分效果图截图 ]"></a>[ 部分效果图截图 ]</h2><h3 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h3><p><img src="http://upload-images.jianshu.io/upload_images/145616-63d8986b44433f99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h3 id="Theme-with-Sublime-Text-3-and-JavaScript-Code"><a href="#Theme-with-Sublime-Text-3-and-JavaScript-Code" class="headerlink" title="Theme with Sublime Text 3 and JavaScript Code"></a>Theme with Sublime Text 3 and JavaScript Code</h3><p><img src="http://upload-images.jianshu.io/upload_images/145616-ac6eca94b5b50a2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h3 id="系统设置"><a href="#系统设置" class="headerlink" title="系统设置"></a>系统设置</h3><p><img src="http://upload-images.jianshu.io/upload_images/145616-b0e714419dcd6433.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h3 id="Posters"><a href="#Posters" class="headerlink" title="Posters"></a>Posters</h3><p><img src="http://upload-images.jianshu.io/upload_images/145616-9c99a2b56e70c30f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-0e6a006a57adb9d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h3 id="Terminal"><a href="#Terminal" class="headerlink" title="Terminal"></a>Terminal</h3><p><img src="http://upload-images.jianshu.io/upload_images/145616-421d8c2880c84c62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="[ Reference ]"></a>[ Reference ]</h2><ul>
<li><a href="http://www.xulukun.cn/flatabulous-ubuntu.html" target="_blank" rel="external">Flatabulous：超级好看的Ubuntu 扁平主题</a></li>
<li><a href="https://github.com/anmoljagetia/Flatabulous" target="_blank" rel="external">Github -&gt; Flatabulous</a></li>
</ul>
<p>(转载请注明原作者及出处, 谢谢！)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/145616-909d61913233d890.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;Flatabulous&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://Hzwcode.github.io/categories/Linux/"/>
    
    
      <category term="Ubuntu" scheme="https://Hzwcode.github.io/tags/Ubuntu/"/>
    
      <category term="Theme" scheme="https://Hzwcode.github.io/tags/Theme/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Materials</title>
    <link href="https://Hzwcode.github.io/2017/02/23/Machine-Learning-Materials/"/>
    <id>https://Hzwcode.github.io/2017/02/23/Machine-Learning-Materials/</id>
    <published>2017-02-23T08:57:14.000Z</published>
    <updated>2017-03-24T13:23:42.390Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><img src="machine_learning_materials.png" alt="Machine Learning"></p>
<hr>
<blockquote>
<h2 id="Awesome系列"><a href="#Awesome系列" class="headerlink" title="Awesome系列　"></a>Awesome系列　</h2></blockquote>
<ul>
<li><a href="https://github.com/josephmisiti/awesome-machine-learning" target="_blank" rel="external"><strong>Awesome Machine Learning</strong></a></li>
<li><a href="https://github.com/ChristosChristofidis/awesome-deep-learning" target="_blank" rel="external"><strong>Awesome Deep Learning</strong></a></li>
<li><a href="https://github.com/jtoy/awesome-tensorflow" target="_blank" rel="external"><strong>Awesome TensorFlow</strong></a></li>
<li><a href="https://github.com/TensorFlowKR/awesome_tensorflow_implementations" target="_blank" rel="external">Awesome TensorFlow Implementations</a></li>
<li><a href="https://github.com/carpedm20/awesome-torch" target="_blank" rel="external">Awesome Torch</a></li>
<li><a href="https://github.com/jbhuang0604/awesome-computer-vision" target="_blank" rel="external">Awesome Computer Vision</a></li>
<li><a href="https://github.com/kjw0612/awesome-deep-vision" target="_blank" rel="external">Awesome Deep Vision</a></li>
<li><a href="https://github.com/kjw0612/awesome-rnn" target="_blank" rel="external">Awesome RNN</a></li>
<li><a href="https://github.com/keonkim/awesome-nlp" target="_blank" rel="external">Awesome NLP</a></li>
<li><a href="https://github.com/owainlewis/awesome-artificial-intelligence" target="_blank" rel="external">Awesome AI</a></li>
<li><a href="https://github.com/terryum/awesome-deep-learning-papers" target="_blank" rel="external">Awesome Deep Learning Papers</a></li>
<li><a href="https://github.com/MaxwellRebo/awesome-2vec" target="_blank" rel="external">Awesome 2vec</a></li>
</ul>
<blockquote>
<h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2></blockquote>
<ul>
<li>[Book] <a href="http://neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="external"><strong>Neural Networks and Deep Learning</strong></a> 中文翻译(不完整): <a href="https://www.gitbook.com/book/hit-scir/neural-networks-and-deep-learning-zh_cn/details" target="_blank" rel="external">神经网络与深度学习</a> 第五章中文翻译: <a href="http://www.jianshu.com/p/917f71b06499" target="_blank" rel="external">[译] 第五章 深度神经网络为何很难训练</a></li>
<li>[Book] <a href="http://www.deeplearningbook.org/" target="_blank" rel="external">Deep Learning - MIT Press</a></li>
<li>[Book] <a href="http://www.springer.com/gb/book/9780387310732" target="_blank" rel="external">Pattern Recognition and Machine Learning</a> (Bishop) | <a href="https://book.douban.com/subject/2061116/" target="_blank" rel="external">豆瓣</a> | <a href="http://nbviewer.jupyter.org/github/lijin-THU/notes-machine-learning/blob/master/ReadMe.ipynb" target="_blank" rel="external">PRML &amp; DL笔记</a> | <a href="https://www.gitbook.com/book/mqshen/prml/details" target="_blank" rel="external">GitBook</a></li>
<li>[Course] <a href="https://cn.udacity.com/course/deep-learning--ud730/" target="_blank" rel="external"><strong>Deep Learning - Udacity</strong></a></li>
<li>[Course] <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="external"><strong>Machine Learning by Andrew Ng - Coursera</strong></a> | <a href="http://www.jianshu.com/p/c68d0df13e0b" target="_blank" rel="external"><strong>课程资料整理</strong></a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li>[Course] <a href="http://cs231n.stanford.edu/" target="_blank" rel="external"><strong>Convolutional Neural Networks for Visual Recognition(CS231n)</strong></a> | <a href="http://www.jianshu.com/p/182baeb82c71" target="_blank" rel="external"><strong>课程资料整理</strong></a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li>[Course] <a href="http://cs224d.stanford.edu/" target="_blank" rel="external">Deep Learning for Natural Language Processing(CS224d)</a> | <a href="http://www.jianshu.com/p/062d2bbbef93" target="_blank" rel="external">课程资料整理</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li>[View] <a href="https://github.com/aymericdamien/TopDeepLearning" target="_blank" rel="external">Top Deep Learning Projects on Github</a></li>
<li>[View] <a href="https://github.com/andrewt3000/DL4NLP/blob/master/README.md" target="_blank" rel="external">Deep Learning for NLP resources</a></li>
<li>[View] <a href="http://www.jianshu.com/p/6752a8845d01" target="_blank" rel="external">资源 | 深度学习资料大全：从基础到各种网络模型</a></li>
<li>[View] <a href="http://www.jianshu.com/nb/8413272" target="_blank" rel="external">Paper | DL相关论文中文翻译</a></li>
<li>[View] <a href="http://www.jianshu.com/p/80bd4d4c2992" target="_blank" rel="external">深度学习新星：GAN的基本原理、应用和走向</a></li>
<li>[View] <a href="http://www.jianshu.com/p/c20917a91472" target="_blank" rel="external">推荐 | 九本不容错过的深度学习和神经网络书籍</a></li>
<li>[View] <a href="https://github.com/memect/hao" target="_blank" rel="external">Github好东西传送门</a> –&gt; <a href="https://github.com/memect/hao/blob/master/awesome/deep-learning-introduction.md" target="_blank" rel="external">深度学习入门与综述资料</a></li>
</ul>
<blockquote>
<h2 id="Frameworks"><a href="#Frameworks" class="headerlink" title="Frameworks"></a>Frameworks</h2></blockquote>
<ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">TensorFlow (by google)</a></li>
<li><a href="https://github.com/dmlc/mxnet" target="_blank" rel="external">MXNet</a></li>
<li><a href="http://torch.ch/" target="_blank" rel="external">Torch (by Facebook)</a></li>
<li>[Caffe (by UC Berkley)(<a href="http://caffe.berkeleyvision.org/" target="_blank" rel="external">http://caffe.berkeleyvision.org/</a>)</li>
<li>[Deeplearning4j(<a href="http://deeplearning4j.org/" target="_blank" rel="external">http://deeplearning4j.org</a>)</li>
<li>Brainstorm</li>
<li>Theano、Chainer、Marvin、Neon、ConvNetJS</li>
</ul>
<blockquote>
<h2 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h2></blockquote>
<ul>
<li>官方文档</li>
<li><a href="https://www.tensorflow.org/tutorials" target="_blank" rel="external">TensorFlow Tutorial</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/" target="_blank" rel="external">TensorFlow 官方文档中文版</a></li>
<li><a href="http://download.tensorflow.org/paper/whitepaper2015.pdf" target="_blank" rel="external">TensorFlow Whitepaper</a></li>
<li><a href="http://www.jianshu.com/p/65dc64e4c81f" target="_blank" rel="external">[译] TensorFlow白皮书</a></li>
<li>[API] <a href="https://www.tensorflow.org/versions/r0.8/api_docs/index.html" target="_blank" rel="external">API Document</a></li>
</ul>
<blockquote>
<h2 id="入门教程"><a href="#入门教程" class="headerlink" title="入门教程"></a>入门教程</h2></blockquote>
<ul>
<li>[教程] <a href="http://learningtensorflow.com/index.html" target="_blank" rel="external">Learning TensorFlow</a></li>
<li><a href="https://github.com/nlintz/TensorFlow-Tutorials" target="_blank" rel="external">TensorFlow-Tutorials @ github</a> (推荐)</li>
<li><a href="https://github.com/jtoy/awesome-tensorflow" target="_blank" rel="external">Awesome-TensorFlow</a> (推荐)</li>
<li><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="external">TensorFlow-Examples @ github</a></li>
<li><a href="https://github.com/pkmital/tensorflow_tutorials" target="_blank" rel="external">tensorflow_tutorials @ github</a></li>
</ul>
<blockquote>
<h2 id="分布式教程"><a href="#分布式教程" class="headerlink" title="分布式教程"></a>分布式教程</h2></blockquote>
<ul>
<li><a href="https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html#distributed-tensorflow" target="_blank" rel="external">Distributed TensorFlow官方文档</a></li>
<li><a href="https://github.com/ischlag/distributed-tensorflow-example" target="_blank" rel="external">distributed-tensorflow-example @ github</a> (推荐)</li>
<li><a href="https://github.com/ashitani/DistributedTensorFlowSample" target="_blank" rel="external">DistributedTensorFlowSample @ github</a></li>
<li><a href="http://parameterserver.org/" target="_blank" rel="external">Parameter Server</a></li>
</ul>
<blockquote>
<h2 id="Paper-Model"><a href="#Paper-Model" class="headerlink" title="Paper (Model)"></a>Paper (Model)</h2></blockquote>
<h3 id="CNN-Nets"><a href="#CNN-Nets" class="headerlink" title="CNN Nets"></a>CNN Nets</h3><ul>
<li><a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="external">LeNet</a></li>
<li><a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="external">AlexNet</a></li>
<li><a href="https://arxiv.org/abs/1312.6229v4" target="_blank" rel="external">OverFeat</a></li>
<li><a href="https://arxiv.org/abs/1312.4400v3" target="_blank" rel="external">NIN</a></li>
<li><a href="http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf" target="_blank" rel="external">GoogLeNet</a></li>
<li><a href="https://arxiv.org/abs/1409.4842v1" target="_blank" rel="external">Inception-V1</a></li>
<li><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">Inception-V2</a></li>
<li><a href="http://arxiv.org/abs/1512.00567" target="_blank" rel="external">Inception-V3</a></li>
<li><a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">Inception-V4</a></li>
<li><a href="http://arxiv.org/abs/1602.07261" target="_blank" rel="external">Inception-ResNet-v2</a></li>
<li><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet 50</a></li>
<li><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet 101</a></li>
<li><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet 152</a></li>
<li><a href="http://arxiv.org/abs/1409.1556.pdf" target="_blank" rel="external">VGG 16</a></li>
<li><a href="http://arxiv.org/abs/1409.1556.pdf" target="_blank" rel="external">VGG 19</a></li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-131a561dcbe74aba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>(注：图片来自 <a href="https://github.com/tensorflow/models/tree/master/slim#Pretrained" target="_blank" rel="external">Github : TensorFlow-Slim image classification library</a>)</p>
<p>额外参考：</p>
<ul>
<li><a href="http://www.jianshu.com/p/6d441e208547" target="_blank" rel="external">[ILSVRC] 基于OverFeat的图像分类、定位、检测</a></li>
<li><a href="http://www.jianshu.com/p/7975f179ec49" target="_blank" rel="external">[卷积神经网络-进化史] 从LeNet到AlexNet</a></li>
<li><a href="http://www.jianshu.com/p/fe428f0b32c1" target="_blank" rel="external">[透析] 卷积神经网络CNN究竟是怎样一步一步工作的？</a></li>
<li><a href="http://www.jianshu.com/p/ba51f8c6e348" target="_blank" rel="external">GoogLenet中，1X1卷积核到底有什么作用呢？</a></li>
<li><a href="http://www.jianshu.com/p/408ab8177a53" target="_blank" rel="external">深度学习 — 反向传播(BP)理论推导</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22464594?refer=hsmyy" target="_blank" rel="external">无痛的机器学习第一季目录 - 知乎</a></li>
</ul>
<h3 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h3><ul>
<li><a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="external">Fast R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1506.01497v3" target="_blank" rel="external">Faster R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="external">FCN</a></li>
<li><a href="https://arxiv.org/abs/1605.06409v2" target="_blank" rel="external">R-FCN</a></li>
<li><a href="https://arxiv.org/abs/1506.02640v5" target="_blank" rel="external">YOLO</a></li>
<li><a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="external">SSD</a></li>
</ul>
<p>额外参考：</p>
<ul>
<li><a href="http://www.jianshu.com/p/067f6a989d31" target="_blank" rel="external">[Detection] CNN 之 “物体检测” 篇</a></li>
<li><a href="http://www.jianshu.com/p/7e52daaba512" target="_blank" rel="external">计算机视觉中 RNN 应用于目标检测</a></li>
<li><a href="http://www.jianshu.com/p/4ce0aba4e3c2" target="_blank" rel="external">Machine Learning 硬件投入调研</a></li>
</ul>
<h3 id="RNN-amp-LSTM"><a href="#RNN-amp-LSTM" class="headerlink" title="RNN &amp; LSTM"></a>RNN &amp; LSTM</h3><ul>
<li><a href="http://www.jianshu.com/p/c930d61e1f16" target="_blank" rel="external">[福利] 深入理解 RNNs &amp; LSTM 网络学习资料</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/2aca6e8ac7c8" target="_blank" rel="external">[RNN] Simple LSTM代码实现 &amp; BPTT理论推导</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/7e52daaba512" target="_blank" rel="external">计算机视觉中 RNN 应用于目标检测</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li>[推荐] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external"><strong>Understanding LSTM Networks</strong></a> @ <a href="http://colah.github.io/" target="_blank" rel="external">colah</a> | <a href="http://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="external"><strong>理解LSTM网络</strong></a>[简书] @ <a href="http://www.jianshu.com/u/696dc6c6f01c" target="_blank" rel="external">Not_GOD</a></li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a> @ <a href="http://cs.stanford.edu/people/karpathy/" target="_blank" rel="external">Andrej Karpathy</a></li>
<li><a href="http://deeplearning.net/tutorial/lstm.html" target="_blank" rel="external">LSTM Networks for Sentiment Analysis</a> (theano官网LSTM教程+代码)</li>
<li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">Recurrent Neural Networks Tutorial</a> @ <a href="http://www.wildml.com/" target="_blank" rel="external">WILDML</a></li>
<li><a href="http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/" target="_blank" rel="external">Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)</a> @ <a href="https://twitter.com/iamtrask" target="_blank" rel="external">iamtrask</a></li>
</ul>
<blockquote>
<h2 id="Stanford-机器学习课程整理"><a href="#Stanford-机器学习课程整理" class="headerlink" title="Stanford 机器学习课程整理"></a>Stanford 机器学习课程整理</h2></blockquote>
<ul>
<li><a href="http://www.jianshu.com/p/c68d0df13e0b" target="_blank" rel="external">[coursera 机器学习课程] Machine Learning by Andrew Ng</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/182baeb82c71" target="_blank" rel="external">[斯坦福CS231n课程整理] Convolutional Neural Networks for Visual Recognition（附翻译，下载）</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/062d2bbbef93" target="_blank" rel="external">[斯坦福CS224d课程整理] Natural Language Processing with Deep Learning</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/0a6ef31ff77a" target="_blank" rel="external">[斯坦福CS229课程整理] Machine Learning Autumn 2016</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
</ul>
<hr>
<p>( 个人整理，未经允许禁止转载，授权转载请注明作者及出处，谢谢！)</p>
]]></content>
    
    <summary type="html">
    
      本篇文章整理、归纳了自己学习Deep Learning方面的一些资料，包括GitHub Awesome，DL框架如TensorFlow，分布式教程，卷积神经网络CNN，物体检测Paper，循环神经网络RNN、LSTM等，以及斯坦福CS231n计算机视觉识别和Coursera Andrew Ng机器学习等相关课程整理。
    
    </summary>
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://Hzwcode.github.io/tags/Deep-Learning/"/>
    
      <category term="RNN" scheme="https://Hzwcode.github.io/tags/RNN/"/>
    
      <category term="LSTM" scheme="https://Hzwcode.github.io/tags/LSTM/"/>
    
      <category term="CNN" scheme="https://Hzwcode.github.io/tags/CNN/"/>
    
      <category term="Object Detection" scheme="https://Hzwcode.github.io/tags/Object-Detection/"/>
    
      <category term="TensorFlow" scheme="https://Hzwcode.github.io/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to Hexo</title>
    <link href="https://Hzwcode.github.io/2017/02/23/hello-world/"/>
    <id>https://Hzwcode.github.io/2017/02/23/hello-world/</id>
    <published>2017-02-23T06:00:00.000Z</published>
    <updated>2017-03-24T07:17:24.621Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<a id="more"></a>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#Create-a-new-post&quot; class=&quot;headerlink&quot; title=&quot;Create a new post&quot;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;$ hexo new &lt;span class=&quot;string&quot;&gt;&quot;My New Post&quot;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/writing.html&quot;&gt;Writing&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Tool" scheme="https://Hzwcode.github.io/categories/Tool/"/>
    
    
      <category term="Hexo" scheme="https://Hzwcode.github.io/tags/Hexo/"/>
    
  </entry>
  
</feed>
