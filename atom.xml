<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhiwei&#39;s Blog</title>
  <subtitle>Stay Hungry Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://Hzwcode.github.io/"/>
  <updated>2017-02-26T09:17:07.171Z</updated>
  <id>https://Hzwcode.github.io/</id>
  
  <author>
    <name>zhwhong</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>music test</title>
    <link href="https://Hzwcode.github.io/2017/02/26/music-test/"/>
    <id>https://Hzwcode.github.io/2017/02/26/music-test/</id>
    <published>2017-02-26T08:56:23.000Z</published>
    <updated>2017-02-26T09:17:07.171Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="450" src="http://music.163.com/outchain/player?type=0&id=144236857&auto=0&height=430"></iframe>

<a id="more"></a>
<center><br><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="http://music.163.com/outchain/player?type=2&id=34578162&auto=0&height=66"></iframe><br></center>

<div id="aplayer0" class="aplayer" style="margin-bottom: 20px;"></div>
		<script>
			new APlayer({
				element: document.getElementById("aplayer0"),
				narrow: false,
				autoplay: false,
				showlrc: 0,
				music: {
					title: "童话镇",
					author: "陈一发儿",
					url: "http://mp3.haoduoge.com/s/2016-12-24/1482568978.mp3",
					pic: "http://p3.music.126.net/tfa811GLreJI_S0h9epqRA==/3394192426154346.jpg?param=130y130",
				}
			});
		</script>
]]></content>
    
    <summary type="html">
    
      &lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=450 src=&quot;http://music.163.com/outchain/player?type=0&amp;id=144236857&amp;auto=0&amp;height=430&quot;&gt;&lt;/iframe&gt;
    
    </summary>
    
      <category term="tool" scheme="https://Hzwcode.github.io/categories/tool/"/>
    
    
      <category term="hexo" scheme="https://Hzwcode.github.io/tags/hexo/"/>
    
      <category term="music" scheme="https://Hzwcode.github.io/tags/music/"/>
    
  </entry>
  
  <entry>
    <title>[RNN] Simple LSTM代码实现 &amp; BPTT理论推导</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Backpropagation-through-time-BPTT/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Backpropagation-through-time-BPTT/</id>
    <published>2017-02-24T10:26:58.000Z</published>
    <updated>2017-02-24T10:29:27.824Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><ul>
<li>参考：<a href="http://nicodjimenez.github.io/2014/08/08/lstm.html" target="_blank" rel="external">Nico’s Blog - Simple LSTM</a></li>
<li>Github代码：<a href="https://github.com/Hzwcode/lstm" target="_blank" rel="external">https://github.com/Hzwcode/lstm</a></li>
</ul>
<hr>
<p>前面我们介绍过CNN中普通的<a href="http://www.jianshu.com/p/408ab8177a53" target="_blank" rel="external">BP反向传播算法的推导</a>，但是在RNN（比如<a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="external">LSTM</a>）中，反向传播被称作<a href="https://en.wikipedia.org/wiki/Backpropagation_through_time" target="_blank" rel="external">BPTT</a>（Back Propagation Through Time），它是和时间序列有关的。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-4ae3ab8b8426cdcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-113aeedc747a3628.gif?imageMogr2/auto-orient/strip" alt="Back Propagation Through Time"></p>
<a id="more"></a>
<p>A few weeks ago I released some <a href="https://github.com/nicodjimenez/lstm" target="_blank" rel="external">code</a> on Github to help people understand how LSTM’s work at the implementation level. The forward pass is well explained elsewhere and is straightforward to understand, but I derived the backprop equations myself and the backprop code came without any explanation whatsoever. The goal of this post is to explain the so called <em>backpropagation through time</em> in the context of LSTM’s.</p>
<p>If you feel like anything is confusing, please post a comment below or submit an issue on Github.</p>
<p><strong>Note:</strong> this post assumes you understand the forward pass of an LSTM network, as this part is relatively simple. Please read this <a href="http://arxiv.org/abs/1506.00019" target="_blank" rel="external">great intro paper</a> if you are not familiar with this, as it contains a very nice intro to LSTM’s. I follow the same notation as this paper so I recommend reading having the tutorial open in a separate browser tab for easy reference while reading this post.</p>
<blockquote>
<h1 id="Introduction-Simple-LSTM"><a href="#Introduction-Simple-LSTM" class="headerlink" title="Introduction (Simple LSTM)"></a>Introduction (Simple LSTM)</h1></blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-4951e5c5352a88f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="LSTM Block"></p>
<p>The forward pass of an LSTM node is defined as follows:</p>
<p><img src="http://latex.codecogs.com/png.latex?\\g(t)%20&amp;=&amp;%20\phi(W_{gx}%20x(t)%20+%20W_{gh}%20h(t-1)%20+%20b_{g})%20\\\\%20i(t)%20&amp;=&amp;%20\sigma(W_{ix}%20x(t)%20+%20W_{ih}%20h(t-1)%20+%20b_{i})%20\\\\%20f(t)%20&amp;=&amp;%20\sigma(W_{fx}%20x(t)%20+%20W_{fh}%20h(t-1)%20+%20b_{f})%20\\\\%20o(t)%20&amp;=&amp;%20\sigma(W_{ox}%20x(t)%20+%20W_{oh}%20h(t-1)%20+%20b_{o})%20\\\\%20s(t)%20&amp;=&amp;%20g(t)%20*%20i(t)%20+%20s(t-1)%20*%20f(t)%20\\\\%20h(t)%20&amp;=&amp;%20s(t)%20*%20o(t)%20\\" alt=""></p>
<p>(<strong>注</strong>：这里最后一个式子<code>h(t)</code>的计算，普遍认为<code>s(t)</code>前面还有一个tanh激活，然后再乘以<code>o(t)</code>，不过 peephole LSTM paper中建议此处激活函数采用 <code>f(x) = x</code>，所以这里就没有用<code>tanh</code>（下同），可以参见<a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="external">Wiki - Long_short-term_memory</a>上面所说的)</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-ad0508a2df64e3ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>By concatenating the <code>x(t)</code> and <code>h(t-1)</code> vectors as follows:</p>
<p><img src="http://latex.codecogs.com/png.latex?x_c(t)%20=%20[x(t),%20h(t-1)]" alt=""></p>
<p>we can rewrite parts of the above as follows:</p>
<p><img src="http://latex.codecogs.com/png.latex?\\g(t)%20&amp;=&amp;%20\phi(W_{g}%20x_c(t)%20+%20b_{g})%20\\\\%20i(t)%20&amp;=&amp;%20\sigma(W_{i}%20x_c(t)%20+%20b_{i})%20\\\\%20f(t)%20&amp;=&amp;%20\sigma(W_{f}%20x_c(t)%20+%20b_{f})%20\\\\%20o(t)%20&amp;=&amp;%20\sigma(W_{o}%20x_c(t)%20+%20b_{o})" alt=""></p>
<p>Suppose we have a loss <code>l(t)</code> that we wish to minimize at every time step <code>t</code> that depends on the hidden layer <code>h</code> and the label <code>y</code> at the current time via a loss function <code>f</code>:</p>
<p><img src="http://latex.codecogs.com/png.latex?l(t)%20=%20f(h(t),%20y(t))" alt=""></p>
<p>where <code>f</code> can be any differentiable loss function, such as the Euclidean loss:</p>
<p><img src="http://latex.codecogs.com/png.latex?l(t)%20=%20f(h(t),%20y(t))%20=%20\|%20h(t)%20-%20y(t)%20\|^2" alt=""></p>
<p>Our ultimate goal in this case is to use gradient descent to minimize the loss <code>L</code> over an entire sequence of length <code>T</code>：</p>
<p><img src="http://latex.codecogs.com/png.latex?L%20=%20\sum_{t=1}^{T}%20l(t)" alt=""></p>
<p>Let’s work through the algebra of computing the loss gradient:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dw}" alt=""></p>
<p>where <code>w</code> is a scalar parameter of the model (for example it may be an entry in the matrix <code>W_gx</code>). Since the loss <code>l(t) = f(h(t),y(t))</code> only depends on the values of the hidden layer <code>h(t)</code> and the label <code>y(t)</code>, we have by the chain rule:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dw}%20=%20\sum_{t%20=%201}^{T}%20\sum_{i%20=%201}^{M}%20\frac{dL}{dh_i(t)}\frac{dh_i(t)}{dw}" alt=""></p>
<p>where <code>h_i(t)</code> is the scalar corresponding to the <code>i’th</code> memory cell’s hidden output and <code>M</code> is the total number of memory cells. Since the network propagates information forwards in time, changing <code>h_i(t)</code> will have no effect on the loss prior to time <code>t</code>, which allows us to write:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dh_i(t)}%20=%20\sum_{s=1}^T%20\frac{dl(s)}{dh_i(t)}%20=%20\sum_{s=t}^T%20\frac{dl(s)}{dh_i(t)}" alt=""></p>
<p>For notational convenience we introduce the variable <code>L(t)</code> that represents the cumulative loss from step tonwards:</p>
<p><img src="http://latex.codecogs.com/png.latex?L(t)%20=%20\sum_{s=t}^{s=T}%20l(s)" alt=""></p>
<p>such that <code>L(1)</code> is the loss for the entire sequence. This allows us to rewrite the above equation as:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dh_i(t)}%20=%20\sum_{s=t}^T%20\frac{dl(s)}{dh_i(t)}%20=%20\frac{dL(t)}{dh_i(t)}" alt=""></p>
<p>With this in mind, we can rewrite our gradient calculation as:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dw}%20=%20\sum_{t%20=%201}^{T}%20\sum_{i%20=%201}^{M}%20\frac{dL(t)}{dh_i(t)}\frac{dh_i(t)}{dw}" alt=""></p>
<p>Make sure you understand this last equation. The computation of <code>dh_i(t) / dw</code> follows directly follows from the forward propagation equations presented earlier. We now show how to compute <code>dL(t) / dh_i(t)</code> which is where the so called <strong><em>backpropagation through time</em></strong> comes into play.</p>
<blockquote>
<h1 id="Backpropagation-through-time-BPTT"><a href="#Backpropagation-through-time-BPTT" class="headerlink" title="Backpropagation through time (BPTT)"></a>Backpropagation through time (BPTT)</h1></blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-113aeedc747a3628.gif?imageMogr2/auto-orient/strip" alt="Back Propagation Through Time"></p>
<p>This variable <code>L(t)</code> allows us to express the following recursion:</p>
<p><img src="http://latex.codecogs.com/png.latex?L(t)%20=%20\begin{cases}%20l(t)%20+%20L(t+1)%20&amp;%20\text{if}%20\,%20t%20%3C%20T%20\\%20l(t)%20&amp;%20\text{if}%20\,%20t%20=%20T%20\end{cases}" alt=""></p>
<p>Hence, given activation <code>h(t)</code> of an LSTM node at time <code>t</code>, we have that:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL(t)}{dh(t)}%20=%20\frac{dl(t)}{dh(t)}%20+%20\frac{dL(t+1)}{dh(t)}" alt=""></p>
<p>Now, we know where the first term on the right hand side <code>dl(t) / dh(t)</code> comes from: it’s simply the elementwise derivative of the loss <code>l(t)</code> with respect to the activations <code>h(t)</code> at time <code>t</code>. The second term <code>dL(t+1) / dh(t)</code> is where the recurrent nature of LSTM’s shows up. It shows that the we need the <em>next</em> node’s derivative information in order to compute the current <em>current</em> node’s derivative information. Since we will ultimately need to compute <code>dL(t) / dh(t)</code> for all <code>t = 1, 2, ... , T</code>, we start by computing</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL(T)}{dh(T)}%20=%20\frac{dl(T)}{dh(T)}" alt=""></p>
<p>and work our way backwards through the network. Hence the term <em>backpropagation through time</em>. With these intuitions in place, we jump into the code.</p>
<blockquote>
<h1 id="Code-Talk-is-cheap-Show-me-the-code"><a href="#Code-Talk-is-cheap-Show-me-the-code" class="headerlink" title="Code (Talk is cheap, Show me the code)"></a>Code (Talk is cheap, Show me the code)</h1></blockquote>
<p>We now present the code that performs the backprop pass through a single node at time <code>1 &lt;= t &lt;= T</code>. The code takes as input:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-98208ee1ecaa495f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>And computes:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-28f4a30188b7dd3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>whose values will need to be propagated backwards in time. The code also adds derivatives to:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-8c47979fe8d86be1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>since recall that we must sum the derivatives from each time step:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL}{dw}%20=%20\sum_{t%20=%201}^{T}%20\sum_{i%20=%201}^{M}%20\frac{dL(t)}{dh_i(t)}\frac{dh_i(t)}{dw}" alt=""></p>
<p>Also, note that we use:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-34424089b87efa6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>where we recall that <code>X_c(t) = [x(t), h(t-1)]</code>. Without any further due, the code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">top_diff_is</span><span class="params">(self, top_diff_h, top_diff_s)</span>:</span></div><div class="line">    <span class="comment"># notice that top_diff_s is carried along the constant error carousel</span></div><div class="line">    ds = self.state.o * top_diff_h + top_diff_s</div><div class="line">    do = self.state.s * top_diff_h</div><div class="line">    di = self.state.g * ds</div><div class="line">    dg = self.state.i * ds</div><div class="line">    df = self.s_prev * ds</div><div class="line"></div><div class="line">    <span class="comment"># diffs w.r.t. vector inside sigma / tanh function</span></div><div class="line">    di_input = (<span class="number">1.</span> - self.state.i) * self.state.i * di</div><div class="line">    df_input = (<span class="number">1.</span> - self.state.f) * self.state.f * df</div><div class="line">    do_input = (<span class="number">1.</span> - self.state.o) * self.state.o * do</div><div class="line">    dg_input = (<span class="number">1.</span> - self.state.g ** <span class="number">2</span>) * dg</div><div class="line"></div><div class="line">    <span class="comment"># diffs w.r.t. inputs</span></div><div class="line">    self.param.wi_diff += np.outer(di_input, self.xc)</div><div class="line">    self.param.wf_diff += np.outer(df_input, self.xc)</div><div class="line">    self.param.wo_diff += np.outer(do_input, self.xc)</div><div class="line">    self.param.wg_diff += np.outer(dg_input, self.xc)</div><div class="line">    self.param.bi_diff += di_input</div><div class="line">    self.param.bf_diff += df_input</div><div class="line">    self.param.bo_diff += do_input</div><div class="line">    self.param.bg_diff += dg_input</div><div class="line"></div><div class="line">    <span class="comment"># compute bottom diff</span></div><div class="line">    dxc = np.zeros_like(self.xc)</div><div class="line">    dxc += np.dot(self.param.wi.T, di_input)</div><div class="line">    dxc += np.dot(self.param.wf.T, df_input)</div><div class="line">    dxc += np.dot(self.param.wo.T, do_input)</div><div class="line">    dxc += np.dot(self.param.wg.T, dg_input)</div><div class="line"></div><div class="line">    <span class="comment"># save bottom diffs</span></div><div class="line">    self.state.bottom_diff_s = ds * self.state.f</div><div class="line">    self.state.bottom_diff_x = dxc[:self.param.x_dim]</div><div class="line">    self.state.bottom_diff_h = dxc[self.param.x_dim:]</div></pre></td></tr></table></figure>
<blockquote>
<h1 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h1></blockquote>
<p>The forward propagation equations show that modifying <code>s(t)</code> affects the loss <code>L(t)</code> by directly changing the values of <code>h(t)</code> as well as <code>h(t+1)</code>. However, modifying <code>s(t)</code> affects <code>L(t+1)</code> only by modifying <code>h(t+1)</code>. Therefore, by the chain rule:</p>
<p><img src="http://latex.codecogs.com/png.latex?\\\frac{dL(t)}{ds_i(t)}%20=%20\frac{dL(t)}{dh_i(t)}%20\frac{dh_i(t)}{ds_i(t)}%20+%20\frac{dL(t)}{dh_i(t+1)}%20\frac{dh_i(t+1)}{ds_i(t)}%20\\\\\\=%20\frac{dL(t)}{dh_i(t)}%20\frac{dh_i(t)}{ds_i(t)}%20+%20\frac{dL(t+1)}{dh_i(t+1)}%20\frac{dh_i(t+1)}{ds_i(t)}%20\\\\\\=%20\frac{dL(t)}{dh_i(t)}%20\frac{dh_i(t)}{ds_i(t)}%20+%20\frac{dL(t+1)}{ds_i(t)}%20\\\\\\%20=%20\frac{dL(t)}{dh_i(t)}%20\frac{dh_i(t)}{ds_i(t)}%20+%20[\texttt{top\_diff\_s}]_i%20\\" alt=""></p>
<p>Since the forward propagation equations state:</p>
<p><img src="http://latex.codecogs.com/png.latex?h(t)%20=%20s(t)%20*%20o(t)" alt=""></p>
<p>we get that:</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{dL(t)}{dh_i(t)}%20*%20\frac{dh_i(t)}{ds_i(t)}%20=%20o_i(t)%20*%20[\texttt{top\_diff\_h}]_i" alt=""></p>
<p>Putting all this together we have:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ds = self.state.o * top_diff_h + top_diff_s</div></pre></td></tr></table></figure>
<p>The rest of the equations should be straightforward to derive, please let me know if anything is unclear.</p>
<hr>
<blockquote>
<h1 id="Test-LSTM-Network"><a href="#Test-LSTM-Network" class="headerlink" title="Test  LSTM Network"></a>Test  LSTM Network</h1></blockquote>
<p>此 <a href="https://github.com/Hzwcode/lstm" target="_blank" rel="external">代码</a> 其是通过自己实现 lstm 网络来逼近一个序列，y_list = [-0.5, 0.2, 0.1, -0.5]，测试结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line">cur iter:  0</div><div class="line">y_pred[0] : 0.041349</div><div class="line">y_pred[1] : 0.069304</div><div class="line">y_pred[2] : 0.116993</div><div class="line">y_pred[3] : 0.165624</div><div class="line">loss:  0.753483886253</div><div class="line">cur iter:  1</div><div class="line">y_pred[0] : -0.223297</div><div class="line">y_pred[1] : -0.323066</div><div class="line">y_pred[2] : -0.394514</div><div class="line">y_pred[3] : -0.433984</div><div class="line">loss:  0.599065083953</div><div class="line">cur iter:  2</div><div class="line">y_pred[0] : -0.140715</div><div class="line">y_pred[1] : -0.181836</div><div class="line">y_pred[2] : -0.219436</div><div class="line">y_pred[3] : -0.238904</div><div class="line">loss:  0.445095565699</div><div class="line">cur iter:  3</div><div class="line">y_pred[0] : -0.138010</div><div class="line">y_pred[1] : -0.166091</div><div class="line">y_pred[2] : -0.203394</div><div class="line">y_pred[3] : -0.233627</div><div class="line">loss:  0.428061605701</div><div class="line">cur iter:  4</div><div class="line">y_pred[0] : -0.139986</div><div class="line">y_pred[1] : -0.157368</div><div class="line">y_pred[2] : -0.195655</div><div class="line">y_pred[3] : -0.237612</div><div class="line">loss:  0.413581711096</div><div class="line">cur iter:  5</div><div class="line">y_pred[0] : -0.144410</div><div class="line">y_pred[1] : -0.151859</div><div class="line">y_pred[2] : -0.191676</div><div class="line">y_pred[3] : -0.246137</div><div class="line">loss:  0.399770442382</div><div class="line">cur iter:  6</div><div class="line">y_pred[0] : -0.150306</div><div class="line">y_pred[1] : -0.147921</div><div class="line">y_pred[2] : -0.189501</div><div class="line">y_pred[3] : -0.257119</div><div class="line">loss:  0.386136380384</div><div class="line">cur iter:  7</div><div class="line">y_pred[0] : -0.157119</div><div class="line">y_pred[1] : -0.144659</div><div class="line">y_pred[2] : -0.188067</div><div class="line">y_pred[3] : -0.269322</div><div class="line">loss:  0.372552465753</div><div class="line">cur iter:  8</div><div class="line">y_pred[0] : -0.164490</div><div class="line">y_pred[1] : -0.141537</div><div class="line">y_pred[2] : -0.186737</div><div class="line">y_pred[3] : -0.281914</div><div class="line">loss:  0.358993892096</div><div class="line">cur iter:  9</div><div class="line">y_pred[0] : -0.172187</div><div class="line">y_pred[1] : -0.138216</div><div class="line">y_pred[2] : -0.185125</div><div class="line">y_pred[3] : -0.294326</div><div class="line">loss:  0.345449256686</div><div class="line">cur iter:  10</div><div class="line">y_pred[0] : -0.180071</div><div class="line">y_pred[1] : -0.134484</div><div class="line">y_pred[2] : -0.183013</div><div class="line">y_pred[3] : -0.306198</div><div class="line">loss:  0.331888922037</div><div class="line"></div><div class="line">……</div><div class="line"></div><div class="line">cur iter:  97</div><div class="line">y_pred[0] : -0.500351</div><div class="line">y_pred[1] : 0.201185</div><div class="line">y_pred[2] : 0.099026</div><div class="line">y_pred[3] : -0.499154</div><div class="line">loss:  3.1926009167e-06</div><div class="line">cur iter:  98</div><div class="line">y_pred[0] : -0.500342</div><div class="line">y_pred[1] : 0.201122</div><div class="line">y_pred[2] : 0.099075</div><div class="line">y_pred[3] : -0.499190</div><div class="line">loss:  2.88684626031e-06</div><div class="line">cur iter:  99</div><div class="line">y_pred[0] : -0.500331</div><div class="line">y_pred[1] : 0.201063</div><div class="line">y_pred[2] : 0.099122</div><div class="line">y_pred[3] : -0.499226</div><div class="line">loss:  2.61076360677e-06</div></pre></td></tr></table></figure>
<p>可以看出迭代100轮，最后Loss在不断收敛，并且逐渐逼近了预期序列：y_list = [-0.5, 0.2, 0.1, -0.5]。</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="http://www.jianshu.com/p/408ab8177a53" target="_blank" rel="external">深度学习 — 反向传播(BP)理论推导 (zhwhong)</a></li>
<li><a href="http://nicodjimenez.github.io/2014/08/08/lstm.html" target="_blank" rel="external">Nico’s Blog：Simple LSTM</a></li>
<li><a href="https://github.com/Hzwcode/lstm" target="_blank" rel="external">Github仓库：https://github.com/Hzwcode/lstm</a></li>
<li><a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank" rel="external">RECURRENT NEURAL NETWORKS TUTORIAL, PART 3 – BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS</a></li>
<li><a href="http://www.jianshu.com/p/c930d61e1f16" target="_blank" rel="external">[福利] 深入理解 RNNs &amp; LSTM 网络学习资料</a></li>
<li><a href="http://www.jianshu.com/p/c7e3f417641c" target="_blank" rel="external">关于简书中如何编辑Latex数学公式</a></li>
</ul>
<hr>
<p>(喜欢的可以点一下红心，转载请注明出处，谢谢！)</p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;参考：&lt;a href=&quot;http://nicodjimenez.github.io/2014/08/08/lstm.html&quot;&gt;Nico’s Blog - Simple LSTM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github代码：&lt;a href=&quot;https://github.com/Hzwcode/lstm&quot;&gt;https://github.com/Hzwcode/lstm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;前面我们介绍过CNN中普通的&lt;a href=&quot;http://www.jianshu.com/p/408ab8177a53&quot;&gt;BP反向传播算法的推导&lt;/a&gt;，但是在RNN（比如&lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot;&gt;LSTM&lt;/a&gt;）中，反向传播被称作&lt;a href=&quot;https://en.wikipedia.org/wiki/Backpropagation_through_time&quot;&gt;BPTT&lt;/a&gt;（Back Propagation Through Time），它是和时间序列有关的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/145616-4ae3ab8b8426cdcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/145616-113aeedc747a3628.gif?imageMogr2/auto-orient/strip&quot; alt=&quot;Back Propagation Through Time&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="https://Hzwcode.github.io/categories/machine-learning/"/>
    
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 — 反向传播(BP)理论推导</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Backpropagation-principle/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Backpropagation-principle/</id>
    <published>2017-02-24T10:23:35.000Z</published>
    <updated>2017-02-24T10:29:56.164Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><ul>
<li><a href="http://www.jianshu.com/p/c7e3f417641c" target="_blank" rel="external">关于简书中如何编辑Latex数学公式</a></li>
<li><a href="http://www.jianshu.com/p/2aca6e8ac7c8" target="_blank" rel="external">[RNN] Simple LSTM代码实现 &amp; BPTT理论推导</a></li>
</ul>
<hr>
<p>【知识预备】： <a href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" target="_blank" rel="external">UFLDL教程 - 反向传导算法</a></p>
<p>首先我们不讲数学，先上图解，看完图不懂再看后面：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-be0f5712599bf47b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<a id="more"></a>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-190148f7a5f6d59a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-9c0e2a3e41e50184.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-67d7988a4783c6a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-6c9b26999076e229.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-25ed873c3fd53595.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-2af819d45509d1e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-40c7e1c9c6f8cd66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-73012c1bbefe6fd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-d95cd8caa246cfd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-7b7e599bf97627ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-ef5d956b6c35c904.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-e801483bf206b984.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-74eacee144d4ac4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-7ad6f7e9368f4c91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-6cb99673d9ba0fa3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-7420efdf411bbf82.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-ce90b252f0901bc6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-d830f54f90ba8f24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-84cef5edf507cd73.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<h1 id="“BP”-Math-Principle"><a href="#“BP”-Math-Principle" class="headerlink" title="“BP” Math Principle"></a>“BP” Math Principle</h1><p>======================================================================<br><strong>Example</strong>：下面看一个简单的三层神经网络模型，一层输入层，一层隐藏层，一层输出层。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-4a6d84a2e3f81c87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>注：定义输入分别为x1, x2（对应图中的i1，i2），期望输出为y1，y2，假设logistic函数采用sigmoid函数:</p>
<p><img src="http://latex.codecogs.com/png.latex?y%20=%20f(x)=sigmoid(x)%20=\frac{1}{1%20+%20e^{-x}}" alt=""></p>
<p>易知：<br><img src="http://latex.codecogs.com/png.latex?f%27(x)%20=%20f(x)%20*%20(1%20-%20f(x))" alt=""></p>
<p>下面开始正式分析(纯手打！！！)。</p>
<p>======================================================================</p>
<h1 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a><strong>前向传播</strong></h1><p>首先分析神经元h1： </p>
<p><img src="http://latex.codecogs.com/png.latex?input_{(h1)}%20=%20w1%20*%20x1%20+%20w2%20*%20x2%20+%20b1" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?output_{(h1)}%20=%20f(input_{(h1)})%20=%20\frac{1}{1%20+%20e^{-(w1*x1+w2*x2+b1)}}" alt=""></p>
<p>同理可得神经元h2：<br><img src="http://latex.codecogs.com/png.latex?input_{(h2)}%20=%20w3%20*%20x1%20+%20w4%20*%20x2%20+%20b1" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?output_{(h2)}%20=%20f(input_{(h2)})%20=%20\frac{1}{1%20+%20e^{-(w3*x1+w4*x2+b1)}}" alt=""></p>
<p>对输出层神经元重复这个过程，使用隐藏层神经元的输出作为输入。这样就能给出o1，o2的输入输出：<br><img src="http://latex.codecogs.com/png.latex?input_{(o1)}%20=%20w5%20*%20output_{(h1)}%20+%20w6%20*%20output_{(h2)}%20+%20b2" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?output_{(o1)}%20=%20f(input_{(o1)})" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?input_{(o2)}%20=%20w7%20*%20output_{(h1)}%20+%20w8%20*%20output_{(h2)}%20+%20b2" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?output_{(o2)}%20=%20f(input_{(o2)})" alt=""></p>
<p>现在开始统计所有误差，如下：<br><img src="http://latex.codecogs.com/png.latex?J_{total}%20=%20\sum%20\frac{1}{2}(output%20-%20target)^2%20=%20J_{o1}+J_{o2}" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?J_{o1}%20=%20\frac{1}{2}(output(o1)-y1)^2" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?J_{o2}%20=%20\frac{1}{2}(output(o2)-y2)^2" alt=""></p>
<p>======================================================================</p>
<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a><strong>反向传播</strong></h1><h2 id="【输出层】"><a href="#【输出层】" class="headerlink" title="【输出层】"></a><strong>【输出层】</strong></h2><p>对于w5，想知道其改变对总误差有多少影响，于是求Jtotal对w5的偏导数，如下：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w5}=\frac{\partial%20J_{total}}{\partial%20output_{(o1)}}*\frac{\partial%20output_{(o1)}}{\partial%20input_{(o1)}}*\frac{\partial%20input_{(o1)}}{\partial%20w5}" alt=""></p>
<p>分别求每一项：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20output_{(o1)}}=\frac{\partial%20J_{o1}}{\partial%20output_{(o1)}}=output_{(o1)}-y_1" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20output_{(o1)}}{\partial%20input_{(o1)}}%20=%20f%27(input_{(o1)})=output_{(o1)}*(1%20-%20output_{(o1)})" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20input_{(o1)}}{\partial%20w5}=\frac{\partial%20(w5%20*%20output_{(h1)}%20+%20w6%20*%20output_{(h2)}%20+%20b2)}{\partial%20w5}=output_{(h1)}" alt=""></p>
<p>于是有Jtotal对w5的偏导数：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w5}=(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*output_{(h1)}" alt=""></p>
<p>据此更新权重w5，有：<br><img src="http://latex.codecogs.com/png.latex?w5^+%20=%20w5%20-%20\eta*\frac{\partial%20J_{total}}{\partial%20w5}" alt=""></p>
<p>同理可以更新参数w6，w7，w8。<br>在有新权重导入隐藏层神经元（即，当继续下面的反向传播算法时，使用原始权重，而不是更新的权重）之后，执行神经网络中的实际更新。</p>
<h2 id="【隐藏层】"><a href="#【隐藏层】" class="headerlink" title="【隐藏层】"></a><strong>【隐藏层】</strong></h2><p><img src="http://upload-images.jianshu.io/upload_images/145616-4f4ed88c60ee15e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>对于w1，想知道其改变对总误差有多少影响，于是求Jtotal对w1的偏导数，如下：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w1}=\frac{\partial%20J_{total}}{\partial%20output_{(h1)}}*\frac{\partial%20output_{(h1)}}{\partial%20input_{(h1)}}*\frac{\partial%20input_{(h1)}}{\partial%20w1}" alt=""></p>
<p>分别求每一项：</p>
<hr>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20output_{(h1)}}=\frac{\partial%20J_{o1}}{\partial%20output_{(h1)}}+\frac{\partial%20J_{o2}}{\partial%20output_{(h1)}}" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{o1}}{\partial%20output_{(h1)}}=\frac{\partial%20J_{o1}}{\partial%20output_{(o1)}}*\frac{\partial%20output_{(o1)}}{\partial%20input_{(o1)}}*\frac{\partial%20input_{(o1)}}{\partial%20output_{(h1)}}" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?=(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*w5" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{o2}}{\partial%20output_{(h1)}}=\frac{\partial%20J_{o2}}{\partial%20output_{(o2)}}*\frac{\partial%20output_{(o2)}}{\partial%20input_{(o2)}}*\frac{\partial%20input_{(o2)}}{\partial%20output_{(h1)}}" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?=(output_{(o2)}-y2)*[output_{(o2)}*(1%20-%20output_{(o2)})]*w7" alt=""></p>
<hr>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20output_{(h1)}}{\partial%20input_{(h1)}}%20=%20f%27(input_{(h1)})=output_{(h1)}*(1%20-%20output_{(h1)})" alt=""></p>
<hr>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20input_{(h1)}}{\partial%20w1}=\frac{\partial%20(w1%20*%20x1%20+%20w2%20*%20x2%20+%20b1)}{\partial%20w1}=x1" alt=""></p>
<p>于是有Jtotal对w1的偏导数：</p>
<p><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w1}=\{(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*w5" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?+%20(output_{(o2)}-y2)*[output_{(o2)}*(1%20-%20output_{(o2)})]*w7\}*" alt=""></p>
<p><img src="http://latex.codecogs.com/png.latex?[output_{(h1)}*(1%20-%20output_{(h1)})]*x1" alt=""></p>
<p>据此更新w1，有：</p>
<p><img src="http://latex.codecogs.com/png.latex?w1^+%20=%20w1%20-%20\eta*\frac{\partial%20J_{total}}{\partial%20w1}" alt=""></p>
<p>同理可以更新参数w2，w3，w4。</p>
<p>======================================================================</p>
<h1 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a><strong>应用实例</strong></h1><p>假设对于上述简单三层网络模型，按如下方式初始化权重和偏置：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-c8c0d034ff7a0c4f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>根据上述推导的公式：<br>由</p>
<p><img src="http://latex.codecogs.com/png.latex?input_{(h1)}%20=%20w1%20*%20x1%20+%20w2%20*%20x2%20+%20b1" alt=""></p>
<p>得到：<br>input(h1) = 0.15 * 0.05 + 0.20 * 0.10 + 0.35 = 0.3775<br>output(h1) = f(input(h1)) = 1 / (1 + e^(-input(h1))) = 1 / (1 + e^-0.3775) = 0.593269992</p>
<p>同样得到：<br>input(h2) = 0.25 * 0.05 + 0.30 * 0.10 + 0.35 = 0.3925<br>output(h2) = f(input(h2)) = 1 / (1 + e^(-input(h2))) = 1 / (1 + e^-0.3925) = 0.596884378</p>
<p>对输出层神经元重复这个过程，使用隐藏层神经元的输出作为输入。这样就能给出o1的输出：<br>input(o1) = w5 * output(h1) + w6 * (output(h2)) + b2 = 0.40 * 0.593269992 + 0.45 * 0.596884378 + 0.60 = 1.105905967<br>output(o1) = f(input(o1)) = 1 / (1 + e^-1.105905967) = 0.75136507</p>
<p>同理output(o2) = 0.772928465</p>
<p>开始统计所有误差，求代价函数：<br>Jo1 = 1/2 * (0.75136507 - 0.01)^2 = 0.298371109<br>Jo2 = 1/2 * (0.772928465 - 0.99)^2 = 0.023560026</p>
<p><strong>综合所述</strong>，可以得到总误差为：Jtotal = Jo1 + Jo2 = 0.321931135</p>
<p>然后反向传播，根据公式<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w5}=(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*output_{(h1)}" alt=""></p>
<p>求出 Jtotal对w5的偏导数为:<br>a = (0.75136507 - 0.01)*0.75136507*(1-0.75136507)*0.593269992 = 0.082167041</p>
<p>为了减少误差，然后从当前的权重减去这个值（可选择乘以一个学习率，比如设置为0.5），得：<br>w5+ = w5 - eta * a = 0.40 - 0.5 * 0.082167041 = 0.35891648</p>
<p>同理可以求出：<br>w6+ = 0.408666186<br>w7+ = 0.511301270<br>w8+ = 0.561370121</p>
<p>对于隐藏层，更新w1，求Jtotal对w1的偏导数：<br><img src="http://latex.codecogs.com/png.latex?\frac{\partial%20J_{total}}{\partial%20w1}=\{(output_{(o1)}-y1)*[output_{(o1)}*(1%20-%20output_{(o1)})]*w5" alt=""><br><img src="http://latex.codecogs.com/png.latex?+%20(output_{(o2)}-y2)*[output_{(o2)}*(1%20-%20output_{(o2)})]*w7\}*" alt=""><br><img src="http://latex.codecogs.com/png.latex?[output_{(h1)}*(1%20-%20output_{(h1)})]*x1" alt=""></p>
<p>偏导数为：<br>b = (tmp1 + tmp2) * tmp3</p>
<p>tmp1 = (0.75136507 - 0.01) * [0.75136507 * (1 - 0.75136507)] * 0.40 = 0.74136507 * 0.186815602 * 0.40 = 0.055399425<br>tmp2 = -0.019049119<br>tmp3 = 0.593269992 * (1 - 0.593269992) * 0.05 = 0.012065035</p>
<p>于是b = 0.000438568</p>
<p>更新权重w1为：<br>w1+ = w1 - eta * b = 0.15 - 0.5 * 0.000438568 = 0.149780716</p>
<p>同样可以求得：<br>w2+ = 0.19956143<br>w3+ = 0.24975114<br>w4+ = 0.29950229</p>
<p>最后，更新了所有的权重！ 当最初前馈传播时输入为0.05和0.1，网络上的误差是0.298371109。 在第一轮反向传播之后，总误差现在下降到0.291027924。 它可能看起来不太多，但是在重复此过程10,000次之后。例如，错误倾斜到0.000035085。<br>在这一点上，当前馈输入为0.05和0.1时，两个输出神经元产生0.015912196（相对于目标为0.01）和0.984065734（相对于目标为0.99），已经很接近了O(∩_∩)O~~</p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul>
<li><a href="https://zhuanlan.zhihu.com/p/23270674" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/23270674</a></li>
<li><a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="external">Principles of training multi-layer neural network using backpropagation</a></li>
<li><a href="http://www.jianshu.com/p/2aca6e8ac7c8" target="_blank" rel="external">[RNN] Simple LSTM代码实现 &amp; BPTT理论推导</a></li>
<li><a href="http://www.jianshu.com/p/c7e3f417641c" target="_blank" rel="external">简书中如何编辑Latex数学公式</a></li>
</ul>
<hr>
<p>(转载请注明出处： <a href="http://www.jianshu.com/p/408ab8177a53" target="_blank" rel="external">深度学习 — 反向传播(BP)理论推导(zhwhong)</a>)</p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.jianshu.com/p/c7e3f417641c&quot;&gt;关于简书中如何编辑Latex数学公式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.jianshu.com/p/2aca6e8ac7c8&quot;&gt;[RNN] Simple LSTM代码实现 &amp;amp; BPTT理论推导&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;【知识预备】： &lt;a href=&quot;http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95&quot;&gt;UFLDL教程 - 反向传导算法&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先我们不讲数学，先上图解，看完图不懂再看后面：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/145616-be0f5712599bf47b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="https://Hzwcode.github.io/categories/machine-learning/"/>
    
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>[Detection] CNN 之 &quot;物体检测&quot; 篇</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Detection-CNN/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Detection-CNN/</id>
    <published>2017-02-24T10:20:50.000Z</published>
    <updated>2017-02-24T10:28:50.432Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><h1 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h1><ul>
<li><a href="http://arxiv.org/abs/1311.2524" target="_blank" rel="external">RCNN</a> </li>
<li><a href="http://arxiv.org/abs/1504.08083" target="_blank" rel="external">Fast RCNN</a> </li>
<li><a href="http://arxiv.org/abs/1506.01497" target="_blank" rel="external">Faster RCNN</a> </li>
<li><a href="http://arxiv.org/pdf/1605.06409v1.pdf" target="_blank" rel="external">R-FCN</a></li>
<li><a href="http://arxiv.org/abs/1506.02640" target="_blank" rel="external">YOLO</a> </li>
<li><a href="http://www.cs.unc.edu/~wliu/papers/ssd.pdf" target="_blank" rel="external">SSD</a> </li>
<li>NMS</li>
<li>xywh VS xyxy</li>
</ul>
<hr>
<a id="more"></a>
<h1 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h1><p><a href="http://arxiv.org/abs/1311.2524" target="_blank" rel="external">Rich feature hierarchies for accurate object detection and semantic segmentation</a> </p>
<p>早期，使用窗口扫描进行物体识别，计算量大。 RCNN去掉窗口扫描，用聚类方式，对图像进行分割分组，得到多个侯选框的层次组。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-f4c5c9a89c842dcb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<ul>
<li>原始图片通过Selective Search提取候选框，约有2k个</li>
<li>侯选框缩放成固定大小</li>
<li>经过CNN</li>
<li>经两个全连接后，分类</li>
</ul>
<blockquote>
<p>拓展阅读：<a href="http://blog.csdn.net/hjimce/article/details/50187029" target="_blank" rel="external">基于R-CNN的物体检测-CVPR 2014</a></p>
</blockquote>
<h1 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h1><p><a href="http://arxiv.org/abs/1504.08083" target="_blank" rel="external">Fast R-CNN</a> </p>
<p>RCNN中有CNN重复计算，Fast RCNN则去掉重复计算，并微调选框位置。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-1d610559358abecf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<ul>
<li>整图经过CNN，得到特征图</li>
<li>提取域候选框</li>
<li>把候选框投影到特征图上，Pooling采样成固定大小</li>
<li>经两个全连接后，分类与微调选框位置</li>
</ul>
<h1 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h1><p><a href="http://arxiv.org/abs/1506.01497" target="_blank" rel="external">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a> </p>
<p>提取候选框运行在CPU上，耗时2s，效率低下。<br>Faster RCNN使用CNN来预测候选框。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-8b71602ad793eed9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<ul>
<li>整图经过CNN，得到特征图</li>
<li>经过核为 3×3×256 的卷积，每个点上预测k个anchor box是否是物体，并微调anchor box的位置</li>
<li>提取出物体框后，采用Fast RCNN同样的方式，进行分类</li>
<li>选框与分类共用一个CNN网络</li>
</ul>
<p>anchor box的设置应比较好的覆盖到不同大小区域，如下图:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-833ff24cf66a7fd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>一张1000×600的图片，大概可以得到20k个anchor box(60×40×9)。</p>
<h1 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a>R-FCN</h1><p><a href="http://arxiv.org/pdf/1605.06409v1.pdf" target="_blank" rel="external">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a> </p>
<blockquote>
<p>论文翻译详见：<a href="http://www.jianshu.com/p/db1b74770e52" target="_blank" rel="external">[译] 基于R-FCN的物体检测 (zhwhong)</a></p>
</blockquote>
<p>RCNN系列(RCNN、Fast RCNN、Faster RCNN)中，网络由两个子CNN构成。在图片分类中，只需一个CNN，效率非常高。所以物体检测是不是也可以只用一个CNN？ </p>
<p>图片分类需要兼容形变，而物体检测需要利用形变，如何平衡？ </p>
<p>R-FCN利用在CNN的最后进行位置相关的特征pooling来解决以上两个问题。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-8eb1556488b4fdc2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>经普通CNN后，做有 k^2(C+1) 个 channel 的卷积，生成位置相关的特征(position-sensitive score maps)。</p>
<p>C 表示分类数，加 1 表示背景，k 表示后续要pooling 的大小，所以生成 k^2 倍的channel，以应对后面的空间pooling。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-0bcb1e46be5e24c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>普通CNN后，还有一个RPN(Region Proposal Network)，生成候选框。</p>
<p>假设一个候选框大小为 w×h，将它投影在位置相关的特征上，并采用average-pooling的方式生成一个 k×k×k^2(C+1) 的块(与Fast RCNN一样)，再采用空间相关的pooling(k×k平面上每一个点取channel上对应的部分数据)，生成 k×k×(C+1)的块，最后再做average-pooling生成 C+1 的块，最后做softmax生成分类概率。</p>
<p>类似的，RPN也可以采用空间pooling的结构，生成一个channel为 4k^2的特征层。</p>
<p>空间pooling的具体操作可以参考下面。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-4411b2baa05764f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>训练与SSD相似，训练时拿来做lost计算的点取一个常数，如128。 除去正点，剩下的所有使用概率最高的负点。</p>
<h1 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h1><p><a href="http://arxiv.org/abs/1506.02640" target="_blank" rel="external">You Only Look Once: Unified, Real-Time Object Detection</a> </p>
<p>Faster RCNN需要对20k个anchor box进行判断是否是物体，然后再进行物体识别，分成了两步。 YOLO则把物体框的选择与识别进行了结合，一步输出，即变成”You Only Look Once”。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-881c58173e5fab4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<ul>
<li>把原始图片缩放成448×448大小</li>
<li>运行单个CNN</li>
<li>计算物体中心是否落入单元格、物体的位置、物体的类别</li>
</ul>
<p>模型如下:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-148936c1f19644a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<ul>
<li>把缩放成统一大小的图片分割成S×S的单元格</li>
<li>每个单元格输出B个矩形框(冗余设计)，包含框的位置信息(x, y, w, h)与物体的Confidence</li>
<li>每个单元格再输出C个类别的条件概率P(Class∣Object)</li>
<li>最终输出层应有S×S×(B∗5+C)个单元</li>
<li>x, y 是每个单元格的相对位置</li>
<li>w, h 是整图的相对大小</li>
</ul>
<p>Conficence定义如下:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-772c5abc28591971.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>在原论文中，S = 7，B = 2，C = 20，所以输出的单元数为7×7×30。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-a8d08b9a46de7f9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>代价函数：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-7f2b9e54c3730d5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>其中 <code>λ_coord=5</code>，<code>λ_noobj=0.5</code>。<br>一般，w与 h 不是在 [0,1]上的均匀分布，偏小，所以开方。</p>
<p><strong>注: 开方的解释是我自己的估计，可能不对。</strong></p>
<h1 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h1><p><a href="http://www.cs.unc.edu/~wliu/papers/ssd.pdf" target="_blank" rel="external">SSD: Single Shot MultiBox Detector</a> </p>
<p>YOLO在 7×7 的框架下识别物体，遇到大量小物体时，难以处理。<br>SSD则在不同层级的feature map下进行识别，能够覆盖更多范围。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-f481d203b810dba3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>假设在 m 层 feature map 上进行识别，则第 k 层的基本比例为</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-e1ea76b11f39c48a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>比如 s_min=0.2，s_max=0.95，表示整张图片识别物体所占比最小 0.2，最大 0.95。</p>
<p>在基本比例上，再取多个长宽比，令 a={1, 2, 3, 1/2, 1/3}，长宽分别为</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-d79012d45d6f57a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>Match策略上，取ground truth与以上生成的格子重叠率大于0.5的。</p>
<h1 id="SSD-vs-YOLO"><a href="#SSD-vs-YOLO" class="headerlink" title="SSD vs YOLO"></a>SSD vs YOLO</h1><p><img src="http://upload-images.jianshu.io/upload_images/145616-70f2fd38db66b76e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>位置采用Smooth L1 Regression，分类采用Softmax。<br>代价函数为：</p>
<p><img src="http://latex.codecogs.com/png.latex?L%20=%20L_{conf}(x,%20c)%20+%20\alpha%20\cdot%20L_{loc}(c,%20l,%20g))" alt=""></p>
<p>x  表示类别输出，c 表示目标分类，l 表示位置输出，g 表示目标位置, α是比例常数，可取1。<br>训练过程中负点远多于正点，所以只取负点中，概率最大的几个，数量与正点成 3:1 。</p>
<h1 id="NMS"><a href="#NMS" class="headerlink" title="NMS"></a>NMS</h1><p>以上方法，同一物体可能有多个预测值。<br>可用NMS(Non-maximum suppression，非极大值抑制)来去重。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-ba89e4a3fde65974.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>如上图所示，一共有6个识别为人的框，每一个框有一个置信率。<br>现在需要消除多余的:</p>
<ul>
<li>按置信率排序: 0.95, 0.9, 0.9, 0.8, 0.7, 0.7</li>
<li>取最大0.95的框为一个物体框</li>
<li>剩余5个框中，去掉与0.95框重叠率大于0.6(可以另行设置)，则保留0.9, 0.8, 0.7三个框</li>
<li>重复上面的步骤，直到没有框了，0.9为一个框</li>
<li>选出来的为: 0.95, 0.9</li>
</ul>
<p>两个矩形的重叠率计算方式如下:</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-59ba4b17d2cc2538.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="xywh-VS-xyxy"><a href="#xywh-VS-xyxy" class="headerlink" title="xywh VS xyxy"></a>xywh VS xyxy</h1><p>系列论文中，位置都用 (x,y,w,h)来表示，没有用左上角、右下角 (x,y,x,y) 来表示。<br>初衷是当 (w,h)正确时，(x,y) 一点错，会导致整个框就不准了。<br>在初步的实际实验中，(x,y,x,y) 效果要差一些。</p>
<p>背后的逻辑，物体位置用 (x,y,w,h) 来学习比较容易。<br>(x,y) 只需要位置相关的加权就能计算出来；<br>(w,h) 就更简单了，直接特征值相加即可。</p>
<hr>
<ul>
<li>原文链接：<a href="http://www.cosmosshadow.com/ml/%E5%BA%94%E7%94%A8/2015/12/07/%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B.html" target="_blank" rel="external">Detection</a></li>
<li>参考：<a href="http://www.jianshu.com/p/db1b74770e52" target="_blank" rel="external">[译] 基于R-FCN的物体检测 (zhwhong)</a></li>
</ul>
<p>(转载请注明出处：<a href="http://www.jianshu.com/p/067f6a989d31" target="_blank" rel="external">[Detection] CNN 之 “物体检测” 篇 (zhwhong)</a>)</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Index&quot;&gt;&lt;a href=&quot;#Index&quot; class=&quot;headerlink&quot; title=&quot;Index&quot;&gt;&lt;/a&gt;Index&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1311.2524&quot;&gt;RCNN&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1504.08083&quot;&gt;Fast RCNN&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1506.01497&quot;&gt;Faster RCNN&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1605.06409v1.pdf&quot;&gt;R-FCN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1506.02640&quot;&gt;YOLO&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.unc.edu/~wliu/papers/ssd.pdf&quot;&gt;SSD&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;NMS&lt;/li&gt;
&lt;li&gt;xywh VS xyxy&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="machine learning" scheme="https://Hzwcode.github.io/categories/machine-learning/"/>
    
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/tags/Machine-Learning/"/>
    
      <category term="Knowledge" scheme="https://Hzwcode.github.io/tags/Knowledge/"/>
    
  </entry>
  
  <entry>
    <title>[斯坦福CS231n课程整理] Convolutional Neural Networks for Visual Recognition(附翻译，作业)</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Convolutional-Neural-Networks-for-Visual-Recognition-CS231n/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Convolutional-Neural-Networks-for-Visual-Recognition-CS231n/</id>
    <published>2017-02-24T09:27:52.000Z</published>
    <updated>2017-02-24T10:13:29.414Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><blockquote>
<h1 id="CS231n课程：面向视觉识别的卷积神经网络"><a href="#CS231n课程：面向视觉识别的卷积神经网络" class="headerlink" title="CS231n课程：面向视觉识别的卷积神经网络"></a>CS231n课程：面向视觉识别的卷积神经网络</h1></blockquote>
<ul>
<li>课程官网：<a href="http://cs231n.stanford.edu/" target="_blank" rel="external">CS231n: Convolutional Neural Networks for Visual Recognition</a></li>
<li>Github：<a href="https://github.com/cs231n/cs231n.github.io" target="_blank" rel="external">https://github.com/cs231n/cs231n.github.io</a> | <a href="http://cs231n.github.io/" target="_blank" rel="external">http://cs231n.github.io/</a></li>
<li>教学安排及大纲：<a href="http://vision.stanford.edu/teaching/cs231n/syllabus.html" target="_blank" rel="external">Schedule and Syllabus</a></li>
<li>课程视频：Youtube上查看<a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw" target="_blank" rel="external">Andrej Karpathy</a>创建的<a href="https://link.zhihu.com/?target=https%3A//www.youtube.com/playlist%3Flist%3DPLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC" target="_blank" rel="external">播放列表</a>，或者<a href="http://study.163.com/course/introduction/1003223001.htm#/courseDetail" target="_blank" rel="external">网易云课堂</a></li>
<li>课程pdf及视频下载：<a href="https://pan.baidu.com/s/1eRHH4L8" target="_blank" rel="external">百度网盘下载</a>，密码是4efx</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-a0eeadfcd667b7bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<a id="more"></a>
<blockquote>
<h1 id="授课-Stanford-Vision-Lab"><a href="#授课-Stanford-Vision-Lab" class="headerlink" title="授课 (Stanford Vision Lab)"></a>授课 (<a href="http://vision.stanford.edu/index.html" target="_blank" rel="external">Stanford Vision Lab</a>)</h1></blockquote>
<ul>
<li><a href="http://vision.stanford.edu/feifeili/" target="_blank" rel="external">Fei-Fei Li</a> (Associate Professor, Stanford University)</li>
<li><a href="http://cs.stanford.edu/people/karpathy/" target="_blank" rel="external">Andrej Karpathy</a> | <a href="https://github.com/karpathy" target="_blank" rel="external">Github</a> | <a href="http://karpathy.github.io/" target="_blank" rel="external">Blog</a> | <a href="https://twitter.com/karpathy" target="_blank" rel="external">Twitter</a></li>
<li><a href="http://cs.stanford.edu/people/jcjohns/" target="_blank" rel="external">Justin Johnson</a> | <a href="https://github.com/jcjohnson" target="_blank" rel="external">Github</a></li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-df9eb2f6ea9512fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Course Instructors and Teaching Assistants"></p>
<blockquote>
<h1 id="课程原文-amp-作业-amp-中文翻译笔记"><a href="#课程原文-amp-作业-amp-中文翻译笔记" class="headerlink" title="课程原文 &amp; 作业 &amp; 中文翻译笔记"></a>课程原文 &amp; 作业 &amp; 中文翻译笔记</h1><ul>
<li>知乎专栏：<a href="https://zhuanlan.zhihu.com/intelligentunit" target="_blank" rel="external"><strong>智能单元</strong></a></li>
<li>作者：<a href="https://www.zhihu.com/people/du-ke/answers" target="_blank" rel="external"><strong>杜客</strong></a> (在此对作者表示特别感谢！)</li>
</ul>
</blockquote>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-3b415a85af702e04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="翻译得到Karpathy许可"></p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit" target="_blank" rel="external">贺完结！CS231n官方笔记授权翻译总集篇发布</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20870307?refer=intelligentunit" target="_blank" rel="external">获得授权翻译斯坦福CS231n课程笔记系列</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：Python Numpy教程</a> | <a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：图像分类笔记（上）</a> | <a href="http://cs231n.github.io/classification/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20900216?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：图像分类笔记（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：线性分类笔记（上）</a> | <a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：线性分类笔记（中）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：线性分类笔记（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21354230?refer=intelligentunit" target="_blank" rel="external">知友智靖远关于CS231n课程字幕翻译的倡议 </a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：最优化笔记（上）</a> | <a href="http://cs231n.github.io/optimization-1/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：最优化笔记（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：反向传播笔记 </a> | <a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21441838?refer=intelligentunit" target="_blank" rel="external">斯坦福CS231n课程作业 # 1 简介 </a> | <a href="http://cs231n.github.io/assignments2016/assignment1/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 1（上）</a> | <a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 1（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 2 </a> | <a href="http://cs231n.github.io/neural-networks-2/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 3（上）</a> | <a href="http://cs231n.github.io/neural-networks-3/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：神经网络笔记 3（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21941485?refer=intelligentunit" target="_blank" rel="external">斯坦福CS231n课程作业 # 2 简介 </a> | <a href="http://cs231n.github.io/assignments2016/assignment2/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" target="_blank" rel="external">CS231n课程笔记翻译：卷积神经网络笔记 </a> | <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" target="_blank" rel="external">斯坦福CS231n课程作业 # 3 简介 </a> | <a href="http://cs231n.github.io/assignments2016/assignment3/" target="_blank" rel="external">课程原文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22282421?refer=intelligentunit" target="_blank" rel="external">Andrej Karpathy的回信和Quora活动邀请</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22232836?refer=intelligentunit" target="_blank" rel="external">知行合一码作业，深度学习真入门 </a></li>
</ul>
<p><strong>【附录 - Assignment】：</strong></p>
<ul>
<li>[简书] <a href="http://www.jianshu.com/p/004c99623104" target="_blank" rel="external">CS231n (winter 2016) : Assignment1</a></li>
<li>[简书] <a href="http://www.jianshu.com/p/9c4396653324" target="_blank" rel="external">CS231n (winter 2016) : Assignment2</a></li>
<li>[简书] <a href="http://www.jianshu.com/p/e46b1aa48886" target="_blank" rel="external">CS231n (winter 2016) : Assignment3（更新中）</a></li>
<li>[Github] CS231n作业<a href="https://github.com/MyHumbleSelf/cs231n" target="_blank" rel="external"> 参考1</a> | <a href="https://github.com/dengfy/cs231n" target="_blank" rel="external">参考2</a> ……</li>
</ul>
<hr>
<p>(再次感谢<a href="https://zhuanlan.zhihu.com/intelligentunit" target="_blank" rel="external">智能单元-知乎专栏</a>，以及知乎作者<a href="https://www.zhihu.com/people/du-ke/answers" target="_blank" rel="external">@杜客</a>和相关朋友<a href="https://www.zhihu.com/people/584f06e4ed2edc6007e4793179e7cdc1" target="_blank" rel="external">@ShiqingFan</a>，<a href="https://www.zhihu.com/people/hmonkey" target="_blank" rel="external">@猴子</a>，<a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" target="_blank" rel="external">@堃堃</a>，<a href="https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481" target="_blank" rel="external">@李艺颖</a>等为CS231n课程翻译工作做出的贡献，辛苦了！)</p>
<hr>
<p><strong>其他课程整理：</strong></p>
<ul>
<li><a href="http://www.jianshu.com/p/062d2bbbef93" target="_blank" rel="external">[斯坦福CS224d课程整理] Natural Language Processing with Deep Learning</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/0a6ef31ff77a" target="_blank" rel="external">[斯坦福CS229课程整理] Machine Learning Autumn 2016</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/c68d0df13e0b" target="_blank" rel="external">[coursera 机器学习课程] Machine Learning by Andrew Ng</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;h1 id=&quot;CS231n课程：面向视觉识别的卷积神经网络&quot;&gt;&lt;a href=&quot;#CS231n课程：面向视觉识别的卷积神经网络&quot; class=&quot;headerlink&quot; title=&quot;CS231n课程：面向视觉识别的卷积神经网络&quot;&gt;&lt;/a&gt;CS231n课程：面向视觉识别的卷积神经网络&lt;/h1&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;课程官网：&lt;a href=&quot;http://cs231n.stanford.edu/&quot;&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Github：&lt;a href=&quot;https://github.com/cs231n/cs231n.github.io&quot;&gt;https://github.com/cs231n/cs231n.github.io&lt;/a&gt; | &lt;a href=&quot;http://cs231n.github.io/&quot;&gt;http://cs231n.github.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;教学安排及大纲：&lt;a href=&quot;http://vision.stanford.edu/teaching/cs231n/syllabus.html&quot;&gt;Schedule and Syllabus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;课程视频：Youtube上查看&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw&quot;&gt;Andrej Karpathy&lt;/a&gt;创建的&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//www.youtube.com/playlist%3Flist%3DPLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&quot;&gt;播放列表&lt;/a&gt;，或者&lt;a href=&quot;http://study.163.com/course/introduction/1003223001.htm#/courseDetail&quot;&gt;网易云课堂&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;课程pdf及视频下载：&lt;a href=&quot;https://pan.baidu.com/s/1eRHH4L8&quot;&gt;百度网盘下载&lt;/a&gt;，密码是4efx&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/145616-a0eeadfcd667b7bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="https://Hzwcode.github.io/categories/machine-learning/"/>
    
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/tags/Machine-Learning/"/>
    
      <category term="Knowledge" scheme="https://Hzwcode.github.io/tags/Knowledge/"/>
    
  </entry>
  
  <entry>
    <title>[Linux] Ubuntu下超好看扁平主题 : Flatabulous</title>
    <link href="https://Hzwcode.github.io/2017/02/24/Linux-Ubuntu_Flatabulous/"/>
    <id>https://Hzwcode.github.io/2017/02/24/Linux-Ubuntu_Flatabulous/</id>
    <published>2017-02-24T08:12:16.000Z</published>
    <updated>2017-02-24T09:30:50.030Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><img src="http://upload-images.jianshu.io/upload_images/145616-909d61913233d890.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Flatabulous"></p>
<a id="more"></a>
<p>使用ubuntu的小伙伴们，不知道你们对ubuntu自带主题有什么看法，反正我个人不太喜欢，个人比较喜欢扁平化的风格。<br>下面给大家推荐一个我长期使用的扁平化风格的主题－<a href="https://github.com/anmoljagetia/Flatabulous" target="_blank" rel="external">Flatabulous</a> 。<br>先看一下我的桌面(个人比较偏向单色调，不要在意这些细节啦)：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-1564d71f915f7cef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="My Desktop"></p>
<p>那么<strong>Flatabulous</strong>到底是什么呢？<br>　　“This is a Flat theme for Ubuntu and other debian based Linux Systems. This is based on the Ultra-Flat theme. Special thanks to @steftrikia and Satyajit Sahoo for the original work.”<br>哈哈，不卖关子了，它其实就是一个超级好看的扁平化Ubuntu主题。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-e639fe182b0b743b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>下面就开始说说怎么安装它吧~</p>
<h1 id="安装"><a href="#安装" class="headerlink" title="[ 安装 ]"></a>[ 安装 ]</h1><h2 id="Step-1-安装-Unity-Tweak-Tool"><a href="#Step-1-安装-Unity-Tweak-Tool" class="headerlink" title="Step 1　安装 Unity Tweak Tool"></a>Step 1　安装 Unity Tweak Tool</h2><p>要安装这个主题，首先要安装<a href="https://launchpad.net/unity-tweak-tool" target="_blank" rel="external">Unity Tweak Tool</a>或者<a href="https://github.com/tualatrix/ubuntu-tweak" target="_blank" rel="external">Ubuntu Tweak Tool</a>。<br>安装Unity Tweak Tool可以很简单地执行如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install unity-tweak-tool</div></pre></td></tr></table></figure></p>
<p>安装Ubuntu Tweak Tool可以使用如下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo add-apt-repository ppa:tualatrix/ppa  </div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install ubuntu-tweak</div></pre></td></tr></table></figure></p>
<p>或者跑到它们的网站下载.deb文件(推荐)，打开Ubuntu软件中心安装或者使用命令<code>dpkg -i</code>(推荐)安装。</p>
<p>注：If you are on Ubuntu 16.04 or higher, run the commands below to install Ubuntu Tweak:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ wget -q -O - http://archive.getdeb.net/getdeb-archive.key | sudo apt-key add -</div><div class="line">$ sudo sh -c &apos;echo &quot;deb http://archive.getdeb.net/ubuntu xenial-getdeb apps&quot; &gt;&gt; /etc/apt/sources.list.d/getdeb.list&apos;</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install ubuntu-tweak</div></pre></td></tr></table></figure>
<p>安装完毕后，我们可以就搜到Ubuntu Tweak这款软件了，如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-c073230df8a73b8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="Step-2-安装Flatabulous主题"><a href="#Step-2-安装Flatabulous主题" class="headerlink" title="Step 2　安装Flatabulous主题"></a>Step 2　安装Flatabulous主题</h2><h3 id="方式1：Using-the-deb-file-for-Debian-Ubuntu-and-derivatives-Recommended"><a href="#方式1：Using-the-deb-file-for-Debian-Ubuntu-and-derivatives-Recommended" class="headerlink" title="方式1：Using the .deb file for Debian, Ubuntu and derivatives (Recommended)"></a>方式1：Using the .deb file for Debian, Ubuntu and derivatives (Recommended)</h3><p>下载.deb文件，点击<a href="https://github.com/anmoljagetia/Flatabulous/releases" target="_blank" rel="external">这里</a>，下载后，打开Ubuntu软件中心或者使用命令<code>dpkg -i</code>（推荐）安装。</p>
<h3 id="方式2：Using-the-noobslab-PPA"><a href="#方式2：Using-the-noobslab-PPA" class="headerlink" title="方式2：Using the noobslab PPA"></a>方式2：Using the noobslab PPA</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo add-apt-repository ppa:noobslab/themes</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install flatabulous-theme</div></pre></td></tr></table></figure>
<h3 id="方式3：下载Flatabulous源码"><a href="#方式3：下载Flatabulous源码" class="headerlink" title="方式3：下载Flatabulous源码"></a>方式3：下载Flatabulous源码</h3><p>下载主题源码，点击<a href="https://github.com/anmoljagetia/Flatabulous/archive/master.zip" target="_blank" rel="external">这里</a>，或者使用git克隆下来，Github仓库地址： <a href="https://github.com/anmoljagetia/Flatabulous" target="_blank" rel="external">https://github.com/anmoljagetia/Flatabulous</a><br>如果下载的是zip文件，先将其解压，然后移动到/usr/share/themes/下。如果是git clone下来的，直接执行下如下命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo mv Flatabulous /usr/share/themes/</div></pre></td></tr></table></figure>
<h2 id="Step-3-Tweak配置"><a href="#Step-3-Tweak配置" class="headerlink" title="Step 3　Tweak配置"></a>Step 3　Tweak配置</h2><p>我们打开Ubuntu Tweak，选择<strong>调整-&gt;主题</strong>，如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-c937f438f034d8bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>然后，配置GTK主题和窗口主题，选择Flatabulous，如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-1e90b53fc9d14f68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>你们可以模仿我的配置，不过此时还有一个问题，就是你发现图标主题没有<code>Ultra-Flat</code>选项，这个<code>icon</code>需要额外下载，原生的<code>Tweak</code>里面并没有。<br>对于图标，我使用的是ultra-flat-icons主题。有蓝色（推荐），橙色和薄荷绿颜色可用。要安装它，你可以运行下面的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo add-apt-repository ppa:noobslab/icons</div><div class="line">$ sudo apt-get update</div><div class="line">$ sudo apt-get install ultra-flat-icons</div></pre></td></tr></table></figure></p>
<p>或者你也可以运行<code>sudo apt-get install ultra-flat-icons-orange</code>或者 <code>sudo apt-get install ultra-flat-icons-green</code>。<br>根据你自己喜欢的颜色选择，我推荐的是扁平图标，但是你也可以看看<strong>Numix</strong>和<strong>Flattr</strong>。</p>
<p>图标安装好后，再打开Ubuntu Tweak，选择 <code>调整-&gt;主题</code>，选择图标主题为<code>Ultra-Flat</code>。</p>
<p>安装完以后，只需要在theme进行相应的配置，然后换一个自己喜欢的桌面壁纸，我们就能看到超级好看的ubuntu啦。如果不行，重启计算机，应该就可以了。重启之后你的计算机看起来差不多是这样的：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-a9408b3132214304.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="扁平化图标"></p>
<h1 id="部分效果图截图"><a href="#部分效果图截图" class="headerlink" title="[ 部分效果图截图 ]"></a>[ 部分效果图截图 ]</h1><h2 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h2><p><img src="http://upload-images.jianshu.io/upload_images/145616-63d8986b44433f99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="Theme-with-Sublime-Text-3-and-JavaScript-Code"><a href="#Theme-with-Sublime-Text-3-and-JavaScript-Code" class="headerlink" title="Theme with Sublime Text 3 and JavaScript Code"></a>Theme with Sublime Text 3 and JavaScript Code</h2><p><img src="http://upload-images.jianshu.io/upload_images/145616-ac6eca94b5b50a2e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="系统设置"><a href="#系统设置" class="headerlink" title="系统设置"></a>系统设置</h2><p><img src="http://upload-images.jianshu.io/upload_images/145616-b0e714419dcd6433.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="Posters"><a href="#Posters" class="headerlink" title="Posters"></a>Posters</h2><p><img src="http://upload-images.jianshu.io/upload_images/145616-9c99a2b56e70c30f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-0e6a006a57adb9d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h2 id="Terminal"><a href="#Terminal" class="headerlink" title="Terminal"></a>Terminal</h2><p><img src="http://upload-images.jianshu.io/upload_images/145616-421d8c2880c84c62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="[ Reference ]"></a>[ Reference ]</h1><ul>
<li><a href="http://www.xulukun.cn/flatabulous-ubuntu.html" target="_blank" rel="external">Flatabulous：超级好看的Ubuntu 扁平主题</a></li>
<li><a href="https://github.com/anmoljagetia/Flatabulous" target="_blank" rel="external">Github -&gt; Flatabulous</a></li>
</ul>
<p>(转载请注明原作者及出处, 谢谢！)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/145616-909d61913233d890.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;Flatabulous&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://Hzwcode.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://Hzwcode.github.io/tags/Linux/"/>
    
      <category term="ubuntu" scheme="https://Hzwcode.github.io/tags/ubuntu/"/>
    
      <category term="theme" scheme="https://Hzwcode.github.io/tags/theme/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Materials</title>
    <link href="https://Hzwcode.github.io/2017/02/23/Machine-Learning-Materials/"/>
    <id>https://Hzwcode.github.io/2017/02/23/Machine-Learning-Materials/</id>
    <published>2017-02-23T08:57:14.000Z</published>
    <updated>2017-02-24T09:32:12.839Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p><img src="http://upload-images.jianshu.io/upload_images/145616-02a9d0afdcfb7047.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<hr>
<blockquote>
<h1 id="Awesome系列"><a href="#Awesome系列" class="headerlink" title="Awesome系列　"></a>Awesome系列　</h1></blockquote>
<ul>
<li><a href="https://github.com/josephmisiti/awesome-machine-learning" target="_blank" rel="external"><strong>Awesome Machine Learning</strong></a></li>
<li><a href="https://github.com/ChristosChristofidis/awesome-deep-learning" target="_blank" rel="external"><strong>Awesome Deep Learning</strong></a></li>
<li><a href="https://github.com/jtoy/awesome-tensorflow" target="_blank" rel="external"><strong>Awesome TensorFlow</strong></a><a id="more"></a></li>
<li><a href="https://github.com/TensorFlowKR/awesome_tensorflow_implementations" target="_blank" rel="external">Awesome TensorFlow Implementations</a></li>
<li><a href="https://github.com/carpedm20/awesome-torch" target="_blank" rel="external">Awesome Torch</a></li>
<li><a href="https://github.com/jbhuang0604/awesome-computer-vision" target="_blank" rel="external">Awesome Computer Vision</a></li>
<li><a href="https://github.com/kjw0612/awesome-deep-vision" target="_blank" rel="external">Awesome Deep Vision</a></li>
<li><a href="https://github.com/kjw0612/awesome-rnn" target="_blank" rel="external">Awesome RNN</a></li>
<li><a href="https://github.com/keonkim/awesome-nlp" target="_blank" rel="external">Awesome NLP</a></li>
<li><a href="https://github.com/owainlewis/awesome-artificial-intelligence" target="_blank" rel="external">Awesome AI</a></li>
<li><a href="https://github.com/terryum/awesome-deep-learning-papers" target="_blank" rel="external">Awesome Deep Learning Papers</a></li>
<li><a href="https://github.com/MaxwellRebo/awesome-2vec" target="_blank" rel="external">Awesome 2vec</a></li>
</ul>
<blockquote>
<h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1></blockquote>
<ul>
<li>[Book] <a href="http://neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="external"><strong>Neural Networks and Deep Learning</strong></a> 中文翻译(不完整): <a href="https://www.gitbook.com/book/hit-scir/neural-networks-and-deep-learning-zh_cn/details" target="_blank" rel="external">神经网络与深度学习</a> 第五章中文翻译: <a href="http://www.jianshu.com/p/917f71b06499" target="_blank" rel="external">[译] 第五章 深度神经网络为何很难训练</a></li>
<li>[Book] <a href="http://www.deeplearningbook.org/" target="_blank" rel="external">Deep Learning - MIT Press</a></li>
<li>[Book] <a href="http://www.springer.com/gb/book/9780387310732" target="_blank" rel="external">Pattern Recognition and Machine Learning</a> (Bishop) | <a href="https://book.douban.com/subject/2061116/" target="_blank" rel="external">豆瓣</a> | <a href="http://nbviewer.jupyter.org/github/lijin-THU/notes-machine-learning/blob/master/ReadMe.ipynb" target="_blank" rel="external">PRML &amp; DL笔记</a> | <a href="https://www.gitbook.com/book/mqshen/prml/details" target="_blank" rel="external">GitBook</a></li>
<li>[Course] <a href="https://cn.udacity.com/course/deep-learning--ud730/" target="_blank" rel="external"><strong>Deep Learning - Udacity</strong></a></li>
<li>[Course] <a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="external"><strong>Machine Learning by Andrew Ng - Coursera</strong></a> | <a href="http://www.jianshu.com/p/c68d0df13e0b" target="_blank" rel="external"><strong>课程资料整理</strong></a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li>[Course] <a href="http://cs231n.stanford.edu/" target="_blank" rel="external"><strong>Convolutional Neural Networks for Visual Recognition(CS231n)</strong></a> | <a href="http://www.jianshu.com/p/182baeb82c71" target="_blank" rel="external"><strong>课程资料整理</strong></a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li>[Course] <a href="http://cs224d.stanford.edu/" target="_blank" rel="external">Deep Learning for Natural Language Processing(CS224d)</a> | <a href="http://www.jianshu.com/p/062d2bbbef93" target="_blank" rel="external">课程资料整理</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li>[View] <a href="https://github.com/aymericdamien/TopDeepLearning" target="_blank" rel="external">Top Deep Learning Projects on Github</a></li>
<li>[View] <a href="https://github.com/andrewt3000/DL4NLP/blob/master/README.md" target="_blank" rel="external">Deep Learning for NLP resources</a></li>
<li>[View] <a href="http://www.jianshu.com/p/6752a8845d01" target="_blank" rel="external">资源 | 深度学习资料大全：从基础到各种网络模型</a></li>
<li>[View] <a href="http://www.jianshu.com/nb/8413272" target="_blank" rel="external">Paper | DL相关论文中文翻译</a></li>
<li>[View] <a href="http://www.jianshu.com/p/80bd4d4c2992" target="_blank" rel="external">深度学习新星：GAN的基本原理、应用和走向</a></li>
<li>[View] <a href="http://www.jianshu.com/p/c20917a91472" target="_blank" rel="external">推荐 | 九本不容错过的深度学习和神经网络书籍</a></li>
<li>[View] <a href="https://github.com/memect/hao" target="_blank" rel="external">Github好东西传送门</a> –&gt; <a href="https://github.com/memect/hao/blob/master/awesome/deep-learning-introduction.md" target="_blank" rel="external">深度学习入门与综述资料</a></li>
</ul>
<blockquote>
<h1 id="Frameworks"><a href="#Frameworks" class="headerlink" title="Frameworks"></a>Frameworks</h1></blockquote>
<ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">TensorFlow (by google)</a></li>
<li><a href="https://github.com/dmlc/mxnet" target="_blank" rel="external">MXNet</a></li>
<li><a href="http://torch.ch/" target="_blank" rel="external">Torch (by Facebook)</a></li>
<li>[Caffe (by UC Berkley)(<a href="http://caffe.berkeleyvision.org/" target="_blank" rel="external">http://caffe.berkeleyvision.org/</a>)</li>
<li>[Deeplearning4j(<a href="http://deeplearning4j.org/" target="_blank" rel="external">http://deeplearning4j.org</a>)</li>
<li>Brainstorm</li>
<li>Theano、Chainer、Marvin、Neon、ConvNetJS</li>
</ul>
<blockquote>
<h1 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h1></blockquote>
<ul>
<li>官方文档</li>
<li><a href="https://www.tensorflow.org/tutorials" target="_blank" rel="external">TensorFlow Tutorial</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/" target="_blank" rel="external">TensorFlow 官方文档中文版</a></li>
<li><a href="http://download.tensorflow.org/paper/whitepaper2015.pdf" target="_blank" rel="external">TensorFlow Whitepaper</a></li>
<li><a href="http://www.jianshu.com/p/65dc64e4c81f" target="_blank" rel="external">[译] TensorFlow白皮书</a></li>
<li>[API] <a href="https://www.tensorflow.org/versions/r0.8/api_docs/index.html" target="_blank" rel="external">API Document</a></li>
</ul>
<blockquote>
<h1 id="入门教程"><a href="#入门教程" class="headerlink" title="入门教程"></a>入门教程</h1></blockquote>
<ul>
<li>[教程] <a href="http://learningtensorflow.com/index.html" target="_blank" rel="external">Learning TensorFlow</a></li>
<li><a href="https://github.com/nlintz/TensorFlow-Tutorials" target="_blank" rel="external">TensorFlow-Tutorials @ github</a> (推荐)</li>
<li><a href="https://github.com/jtoy/awesome-tensorflow" target="_blank" rel="external">Awesome-TensorFlow</a> (推荐)</li>
<li><a href="https://github.com/aymericdamien/TensorFlow-Examples" target="_blank" rel="external">TensorFlow-Examples @ github</a></li>
<li><a href="https://github.com/pkmital/tensorflow_tutorials" target="_blank" rel="external">tensorflow_tutorials @ github</a></li>
</ul>
<blockquote>
<h1 id="分布式教程"><a href="#分布式教程" class="headerlink" title="分布式教程"></a>分布式教程</h1></blockquote>
<ul>
<li><a href="https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html#distributed-tensorflow" target="_blank" rel="external">Distributed TensorFlow官方文档</a></li>
<li><a href="https://github.com/ischlag/distributed-tensorflow-example" target="_blank" rel="external">distributed-tensorflow-example @ github</a> (推荐)</li>
<li><a href="https://github.com/ashitani/DistributedTensorFlowSample" target="_blank" rel="external">DistributedTensorFlowSample @ github</a></li>
<li><a href="http://parameterserver.org/" target="_blank" rel="external">Parameter Server</a></li>
</ul>
<blockquote>
<h1 id="Paper-Model"><a href="#Paper-Model" class="headerlink" title="Paper (Model)"></a>Paper (Model)</h1></blockquote>
<h2 id="CNN-Nets"><a href="#CNN-Nets" class="headerlink" title="CNN Nets"></a>CNN Nets</h2><ul>
<li><a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="external">LeNet</a></li>
<li><a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf" target="_blank" rel="external">AlexNet</a></li>
<li><a href="https://arxiv.org/abs/1312.6229v4" target="_blank" rel="external">OverFeat</a></li>
<li><a href="https://arxiv.org/abs/1312.4400v3" target="_blank" rel="external">NIN</a></li>
<li><a href="http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf" target="_blank" rel="external">GoogLeNet</a></li>
<li><a href="https://arxiv.org/abs/1409.4842v1" target="_blank" rel="external">Inception-V1</a></li>
<li><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">Inception-V2</a></li>
<li><a href="http://arxiv.org/abs/1512.00567" target="_blank" rel="external">Inception-V3</a></li>
<li><a href="https://arxiv.org/abs/1602.07261" target="_blank" rel="external">Inception-V4</a></li>
<li><a href="http://arxiv.org/abs/1602.07261" target="_blank" rel="external">Inception-ResNet-v2</a></li>
<li><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet 50</a></li>
<li><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet 101</a></li>
<li><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">ResNet 152</a></li>
<li><a href="http://arxiv.org/abs/1409.1556.pdf" target="_blank" rel="external">VGG 16</a></li>
<li><a href="http://arxiv.org/abs/1409.1556.pdf" target="_blank" rel="external">VGG 19</a></li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/145616-131a561dcbe74aba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p>
<p>(注：图片来自 <a href="https://github.com/tensorflow/models/tree/master/slim#Pretrained" target="_blank" rel="external">Github : TensorFlow-Slim image classification library</a>)</p>
<p>额外参考：</p>
<ul>
<li><a href="http://www.jianshu.com/p/6d441e208547" target="_blank" rel="external">[ILSVRC] 基于OverFeat的图像分类、定位、检测</a></li>
<li><a href="http://www.jianshu.com/p/7975f179ec49" target="_blank" rel="external">[卷积神经网络-进化史] 从LeNet到AlexNet</a></li>
<li><a href="http://www.jianshu.com/p/fe428f0b32c1" target="_blank" rel="external">[透析] 卷积神经网络CNN究竟是怎样一步一步工作的？</a></li>
<li><a href="http://www.jianshu.com/p/ba51f8c6e348" target="_blank" rel="external">GoogLenet中，1X1卷积核到底有什么作用呢？</a></li>
<li><a href="http://www.jianshu.com/p/408ab8177a53" target="_blank" rel="external">深度学习 — 反向传播(BP)理论推导</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22464594?refer=hsmyy" target="_blank" rel="external">无痛的机器学习第一季目录 - 知乎</a></li>
</ul>
<h2 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h2><ul>
<li><a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="external">R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="external">Fast R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1506.01497v3" target="_blank" rel="external">Faster R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="external">FCN</a></li>
<li><a href="https://arxiv.org/abs/1605.06409v2" target="_blank" rel="external">R-FCN</a></li>
<li><a href="https://arxiv.org/abs/1506.02640v5" target="_blank" rel="external">YOLO</a></li>
<li><a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="external">SSD</a></li>
</ul>
<p>额外参考：</p>
<ul>
<li><a href="http://www.jianshu.com/p/067f6a989d31" target="_blank" rel="external">[Detection] CNN 之 “物体检测” 篇</a></li>
<li><a href="http://www.jianshu.com/p/7e52daaba512" target="_blank" rel="external">计算机视觉中 RNN 应用于目标检测</a></li>
<li><a href="http://www.jianshu.com/p/4ce0aba4e3c2" target="_blank" rel="external">Machine Learning 硬件投入调研</a></li>
</ul>
<h2 id="RNN-amp-LSTM"><a href="#RNN-amp-LSTM" class="headerlink" title="RNN &amp; LSTM"></a>RNN &amp; LSTM</h2><ul>
<li><a href="http://www.jianshu.com/p/c930d61e1f16" target="_blank" rel="external">[福利] 深入理解 RNNs &amp; LSTM 网络学习资料</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/2aca6e8ac7c8" target="_blank" rel="external">[RNN] Simple LSTM代码实现 &amp; BPTT理论推导</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/7e52daaba512" target="_blank" rel="external">计算机视觉中 RNN 应用于目标检测</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li>[推荐] <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external"><strong>Understanding LSTM Networks</strong></a> @ <a href="http://colah.github.io/" target="_blank" rel="external">colah</a> | <a href="http://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="external"><strong>理解LSTM网络</strong></a>[简书] @ <a href="http://www.jianshu.com/u/696dc6c6f01c" target="_blank" rel="external">Not_GOD</a></li>
<li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external">The Unreasonable Effectiveness of Recurrent Neural Networks</a> @ <a href="http://cs.stanford.edu/people/karpathy/" target="_blank" rel="external">Andrej Karpathy</a></li>
<li><a href="http://deeplearning.net/tutorial/lstm.html" target="_blank" rel="external">LSTM Networks for Sentiment Analysis</a> (theano官网LSTM教程+代码)</li>
<li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">Recurrent Neural Networks Tutorial</a> @ <a href="http://www.wildml.com/" target="_blank" rel="external">WILDML</a></li>
<li><a href="http://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/" target="_blank" rel="external">Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)</a> @ <a href="https://twitter.com/iamtrask" target="_blank" rel="external">iamtrask</a></li>
</ul>
<hr>
<blockquote>
<h1 id="Stanford-机器学习课程整理"><a href="#Stanford-机器学习课程整理" class="headerlink" title="Stanford 机器学习课程整理"></a>Stanford 机器学习课程整理</h1></blockquote>
<ul>
<li><a href="http://www.jianshu.com/p/c68d0df13e0b" target="_blank" rel="external">[coursera 机器学习课程] Machine Learning by Andrew Ng</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/182baeb82c71" target="_blank" rel="external">[斯坦福CS231n课程整理] Convolutional Neural Networks for Visual Recognition（附翻译，下载）</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/062d2bbbef93" target="_blank" rel="external">[斯坦福CS224d课程整理] Natural Language Processing with Deep Learning</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
<li><a href="http://www.jianshu.com/p/0a6ef31ff77a" target="_blank" rel="external">[斯坦福CS229课程整理] Machine Learning Autumn 2016</a> @ <a href="http://www.jianshu.com/u/38cd2a8c425e" target="_blank" rel="external">zhwhong</a></li>
</ul>
<hr>
<p>( 个人整理，未经允许禁止转载，授权转载请注明作者出处：<a href="http://www.jianshu.com/p/e0238db24973" target="_blank" rel="external">Machine Learning 学习资料 (zhwhong)</a> ，谢谢！)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://upload-images.jianshu.io/upload_images/145616-02a9d0afdcfb7047.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;h1 id=&quot;Awesome系列&quot;&gt;&lt;a href=&quot;#Awesome系列&quot; class=&quot;headerlink&quot; title=&quot;Awesome系列　&quot;&gt;&lt;/a&gt;Awesome系列　&lt;/h1&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/josephmisiti/awesome-machine-learning&quot;&gt;&lt;strong&gt;Awesome Machine Learning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ChristosChristofidis/awesome-deep-learning&quot;&gt;&lt;strong&gt;Awesome Deep Learning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/jtoy/awesome-tensorflow&quot;&gt;&lt;strong&gt;Awesome TensorFlow&lt;/strong&gt;&lt;/a&gt;
    
    </summary>
    
      <category term="machine learning" scheme="https://Hzwcode.github.io/categories/machine-learning/"/>
    
    
      <category term="Machine Learning" scheme="https://Hzwcode.github.io/tags/Machine-Learning/"/>
    
      <category term="Knowledge" scheme="https://Hzwcode.github.io/tags/Knowledge/"/>
    
  </entry>
  
  <entry>
    <title>Welcome to Hexo</title>
    <link href="https://Hzwcode.github.io/2017/02/23/hello-world/"/>
    <id>https://Hzwcode.github.io/2017/02/23/hello-world/</id>
    <published>2017-02-23T06:00:00.000Z</published>
    <updated>2017-02-24T08:17:40.871Z</updated>
    
    <content type="html"><![CDATA[<script src="/assets/js/APlayer.min.js"> </script><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<a id="more"></a>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#Create-a-new-post&quot; class=&quot;headerlink&quot; title=&quot;Create a new post&quot;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;$ hexo new &lt;span class=&quot;string&quot;&gt;&quot;My New Post&quot;&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/writing.html&quot;&gt;Writing&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="tool" scheme="https://Hzwcode.github.io/categories/tool/"/>
    
    
      <category term="hexo" scheme="https://Hzwcode.github.io/tags/hexo/"/>
    
      <category term="github" scheme="https://Hzwcode.github.io/tags/github/"/>
    
  </entry>
  
</feed>
